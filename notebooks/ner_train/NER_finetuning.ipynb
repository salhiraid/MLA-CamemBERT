{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salhi\\.conda\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import datasets_l \n",
    "import numpy as np \n",
    "from transformers import BertTokenizerFast \n",
    "from transformers import DataCollatorForTokenClassification \n",
    "from transformers import AutoModelForTokenClassification \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CamembertModel, CamembertTokenizer, AdamW, get_cosine_schedule_with_warmup \n",
    "from transformers import CamembertModel, CamembertTokenizer, CamembertConfig\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "\n",
    "ner_dataset = datasets_l.load_dataset(\"conll2003\",\n",
    "                                trust_remote_code=True) \n",
    "ner_dataset\n",
    "\n",
    "from datasets_l import load_dataset\n",
    "ds = load_dataset(\"unimelb-nlp/wikiann\", \"fr\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salhi\\.conda\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: torch.Size([16, 128])\n",
      "Attention Mask: torch.Size([16, 128])\n",
      "Labels: torch.Size([16, 128])\n",
      "tensor([-100,    5,    5,    6,    6,    6,    6,    6,    6,    6,    6, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100])\n",
      "tensor([  101,  2862,  2063,  4078, 16569,  4078,  2632, 10374,  1011,  7803,\n",
      "         2015,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "['[CLS]', 'list', '##e', 'des', 'communes', 'des', 'al', '##pes', '-', 'maritime', '##s', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salhi\\AppData\\Local\\Temp\\ipykernel_1296\\2603194139.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(tokenized_data['labels'][0], dtype=torch.long),\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=128, label_all_tokens=True):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label_all_tokens = label_all_tokens\n",
    "\n",
    "    def tokenize_and_align_labels(self, tokens, ner_tags):\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            tokens,\n",
    "            truncation=True,\n",
    "            is_split_into_words=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        labels = []\n",
    "        for i, label in enumerate(ner_tags):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)  # Special tokens\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(label[word_idx] if self.label_all_tokens else -100)\n",
    "                previous_word_idx = word_idx\n",
    "            labels.append(label_ids)\n",
    "\n",
    "        # Convert all outputs to tensors\n",
    "        tokenized_inputs = {key: torch.tensor(val, dtype=torch.long) for key, val in tokenized_inputs.items()}\n",
    "        tokenized_inputs[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        return tokenized_inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        tokenized_data = self.tokenize_and_align_labels(\n",
    "            [data['tokens']], [data['ner_tags']]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokenized_data['input_ids'].squeeze(),\n",
    "            'attention_mask': tokenized_data['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(tokenized_data['labels'][0], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"unimelb-nlp/wikiann\", \"fr\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")#AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Prepare datasets\n",
    "train_data = NERDataset(dataset['train'], tokenizer)\n",
    "val_data = NERDataset(dataset['validation'], tokenizer)\n",
    "test_data = NERDataset(dataset['test'], tokenizer)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "# Example usage\n",
    "for batch in train_loader:\n",
    "    print(\"Input IDs:\", batch['input_ids'].shape)\n",
    "    print(\"Attention Mask:\", batch['attention_mask'].shape)\n",
    "    print(\"Labels:\", batch['labels'].shape)\n",
    "    print(batch['labels'][0])\n",
    "    print(batch['input_ids'][0])\n",
    "    print(tokenizer.convert_ids_to_tokens(batch['input_ids'][0]))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamemBERTBaseModel(nn.Module):\n",
    "    def __init__(self, model_path: str, trainable: bool = False):\n",
    "        super(CamemBERTBaseModel, self).__init__()\n",
    "        self.base_model = CamembertModel.from_pretrained(model_path)\n",
    "        self.trainable = trainable\n",
    "        self.config = CamembertConfig()\n",
    "\n",
    "        if not trainable:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.base_model.eval()\n",
    "        else:\n",
    "            self.base_model.train()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "    def get_hidden_size(self) -> int:\n",
    "        return self.base_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerFinetuningModel(nn.Module):\n",
    "    def __init__(self, model_path: str, num_labels: int = 9, trainable: bool = True):\n",
    "        super(NerFinetuningModel, self).__init__()\n",
    "        self.base_model = CamemBERTBaseModel(model_path, trainable=trainable)\n",
    "        self.hidden_size = self.base_model.get_hidden_size()\n",
    "        self.ner_head = nn.Linear(self.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor = None):\n",
    "        hidden_states = self.base_model(input_ids, attention_mask)\n",
    "        logits = self.ner_head(hidden_states)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"logits\": logits, \"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, device, lr=5e-5, num_labels=9, save_dir=\"./models\"):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    num_training_steps = num_epochs * len(train_loader)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        correct, total = 0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask, labels)\n",
    "            logits = outputs[\"logits\"]\n",
    "            loss = outputs[\"loss\"]\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            all_preds.extend(preds.view(-1).tolist())\n",
    "            all_labels.extend(labels.view(-1).tolist())\n",
    "            correct += (preds.view(-1) == labels.view(-1)).sum().item()\n",
    "            total += labels.view(-1).numel()\n",
    "\n",
    "        # Compute metrics\n",
    "        valid_preds = [p for p, l in zip(all_preds, all_labels) if l != -100]\n",
    "        valid_labels = [l for l in all_labels if l != -100]\n",
    "        train_f1 = f1_score(valid_labels, valid_preds, average=\"weighted\", labels=list(range(num_labels)))\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct, total = 0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask, labels)\n",
    "                logits = outputs[\"logits\"]\n",
    "                loss = outputs[\"loss\"]\n",
    "\n",
    "                # Accumulate loss\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # Compute accuracy\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                all_preds.extend(preds.view(-1).tolist())\n",
    "                all_labels.extend(labels.view(-1).tolist())\n",
    "                correct += (preds.view(-1) == labels.view(-1)).sum().item()\n",
    "                total += labels.view(-1).numel()\n",
    "\n",
    "        # Compute metrics\n",
    "        valid_preds = [p for p, l in zip(all_preds, all_labels) if l != -100]\n",
    "        valid_labels = [l for l in all_labels if l != -100]\n",
    "        val_f1 = f1_score(valid_labels, valid_preds, average=\"weighted\", labels=list(range(num_labels)))\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Save model\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), f\"{save_dir}/ner_model_epoch_{epoch + 1}.pth\")\n",
    "        print(f\"Model saved to {save_dir}/ner_model_epoch_{epoch + 1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a997069fa7e48d3bc79d4560d2cf2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salhi\\.conda\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\salhi\\.cache\\huggingface\\hub\\models--camembert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = NERDataset(dataset['train'], tokenizer)\n",
    "val_data = NERDataset(dataset['validation'], tokenizer)\n",
    "test_data = NERDataset(dataset['test'], tokenizer)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "\n",
    "    # Initialize model\n",
    "model_path = \"camembert-base\"\n",
    "num_labels = len(dataset[\"train\"].features[\"ner_tags\"].feature.names)\n",
    "model = NerFinetuningModel(model_path, num_labels=num_labels, trainable=True)\n",
    "\n",
    "    # Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_model(model, train_loader, val_loader, num_epochs=5, device=device, save_dir=\"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_model(model, data_loader, device, num_labels):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset.\n",
    "    :param model: Trained NER model.\n",
    "    :param data_loader: DataLoader for the evaluation dataset.\n",
    "    :param device: Device (CPU or GPU) to run the evaluation.\n",
    "    :param num_labels: Number of NER labels.\n",
    "    :return: Dictionary containing evaluation loss, accuracy, and F1-score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask, labels)\n",
    "            logits = outputs[\"logits\"]\n",
    "            loss = outputs[\"loss\"]\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Predictions\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            all_preds.extend(preds.view(-1).tolist())\n",
    "            all_labels.extend(labels.view(-1).tolist())\n",
    "\n",
    "            # Compute accuracy\n",
    "            correct += (preds.view(-1) == labels.view(-1)).sum().item()\n",
    "            total += labels.view(-1).numel()\n",
    "\n",
    "    # Compute metrics\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Filter out padding tokens (-100) for F1-score\n",
    "    valid_preds = [p for p, l in zip(all_preds, all_labels) if l != -100]\n",
    "    valid_labels = [l for l in all_labels if l != -100]\n",
    "\n",
    "    f1 = f1_score(valid_labels, valid_preds, average=\"weighted\", labels=list(range(num_labels)))\n",
    "\n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_metrics = evaluate_model(model, test_loader, device, num_labels)\n",
    "print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Test F1-Score: {test_metrics['f1_score']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

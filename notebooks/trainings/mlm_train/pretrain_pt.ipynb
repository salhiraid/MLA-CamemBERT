{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle   \n",
    "from datasets import load_from_disk\n",
    "from transformers import CamembertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Napster\\\\Desktop\\\\M2_ISI\\\\MLA\\\\CamemBERT\\\\MLA-CamemBERT'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(os.path.abspath(\"../../../\"))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenized_oscar_dataset(Dataset):\n",
    "    def __init__(self, raw_tokenised_data):\n",
    "        \"\"\"\n",
    "        Initialiser le dataset avec les données sous forme de dictionnaire.\n",
    "        \"\"\"\n",
    "        self.raw_data = raw_tokenised_data  # Le dictionnaire avec 'input_ids', 'attention_mask', et 'labels'\n",
    "        self.data = {\n",
    "            \"texts\": self.raw_data['text'],\n",
    "            \"input_ids\": self.raw_data['input_ids'],\n",
    "            \"attention_mask\": self.raw_data['attention_mask'],\n",
    "            \"labels\": self.raw_data['labels']\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Retourne le nombre d'exemples dans le dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retourne un exemple individuel sous forme de dictionnaire.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.data['input_ids'][idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.data['attention_mask'][idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.data['labels'][idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    \n",
    "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
    "    \"\"\"Prepare masked tokens for MLM.\"\"\"\n",
    "    labels = inputs.clone()\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # Replace 80% of masked tokens with [MASK]\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # Replace 10% of masked tokens with random words\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def preprocess_function(texts, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Préparer les données pour l'entraînement MLM.\n",
    "    :param texts: Liste de textes.\n",
    "    :param tokenizer: Tokenizer de CamemBERT.\n",
    "    :return: Dictionnaire avec 'input_ids', 'attention_mask', et 'labels'.\n",
    "    \"\"\"\n",
    "    # Tokeniser les textes\n",
    "    tokenized = tokenizer(texts, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    # Appliquer le masquage\n",
    "    masked_inputs, labels = mask_tokens(input_ids, tokenizer)\n",
    "\n",
    "    return { \n",
    "        \"text\": list(texts),\n",
    "        \"input_ids\": masked_inputs.tolist(),\n",
    "        \"attention_mask\": attention_mask.tolist(),\n",
    "        \"labels\": labels.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the downlaoded Oscar :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path =\"data/oscar.Arrow\"\n",
    "mini_oscar_dataset = load_from_disk(dataset_path)\n",
    "train_texts, val_texts = train_test_split(mini_oscar_dataset['text'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess_function(train_texts, tokenizer)\n",
    "val_data = preprocess_function(val_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_tokenized_data(data, filename):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "save_tokenized_data(train_data, \"data/tokenized_data/tokenized_train_data.pkl\")\n",
    "save_tokenized_data(val_data, \"data/tokenized_data/tokenized_val_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenized_data(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "train_data = load_tokenized_data(\"data/tokenized_data/tokenized_train_data.pkl\")\n",
    "val_data = load_tokenized_data(\"data/tokenized_data/tokenized_val_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les datasets\n",
    "train_dataset = Tokenized_oscar_dataset(train_data)\n",
    "val_dataset = Tokenized_oscar_dataset(val_data)\n",
    "\n",
    "# Créer les DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8192, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester la donnée :\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['attention_mask'].shape)\n",
    "print(batch['labels'].shape)\n",
    "\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}, Validation samples: {len(val_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        print(f\"Initializing {module.__class__.__name__} with Xavier Uniform\")\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        print(f\"Initializing {module.__class__.__name__} with Constant Weights\")\n",
    "        nn.init.constant_(module.bias, 0)\n",
    "        nn.init.constant_(module.weight, 1)\n",
    "    if hasattr(module, 'bias') and module.bias is not None:\n",
    "        print(f\"Initializing Bias for {module.__class__.__name__}\")\n",
    "        nn.init.constant_(module.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, loss, save_dir):\n",
    "    \"\"\"\n",
    "    Sauvegarder l'état actuel du modèle, optimiseur et perte.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "    save_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Checkpoint saved at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. data :\n",
    "batch_size = 4096\n",
    "train_data = load_tokenized_data(\"data/tokenized_data/tokenized_train_data.pkl\")\n",
    "val_data = load_tokenized_data(\"data/tokenized_data/tokenized_val_data.pkl\")\n",
    "\n",
    "#  1.1. Créer les datasets\n",
    "train_dataset = Tokenized_oscar_dataset(train_data)\n",
    "val_dataset = Tokenized_oscar_dataset(val_data)\n",
    "\n",
    "#  1.2. Créer les DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 2. model :\n",
    "from transformers import CamembertForMaskedLM\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1  # 10 %\n",
    "warmup_steps = 10\n",
    "\n",
    "save_interval = 40  # Save the model every 40 epochs \n",
    "eval_interval = 40  # Evaluate the model every 40 epochs\n",
    "num_epochs = 420  # Total epochs\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=learning_rate)\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "# Dossier de sauvegarde\n",
    "save_dir = \"models/mlm_training/model_checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "############################## Training Loop ################################\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Mettre le modèle en mode entraînement\n",
    "    train_loss = 0.0  # Accumulateur pour la perte par époque\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # once in a while evaluate our validation loss : (only for one step, next we'll consider more steps validation)\n",
    "    if epoch % 40 ==0 or epoch == num_epochs - 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for batch_index, batch in enumerate(val_loader):\n",
    "                # 1. Prepare val data :\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                # 2. Forward pass :\n",
    "                # optimizer.zero_grad() not needed since torch.no_grad is used \n",
    "                logits = model(input_ids, attention_mask=attention_mask)['logits']\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            print(f\"Epoch {epoch + 1 }, Validation Loss: {avg_val_loss:.4f}\")\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "    # Update the parameters of the model :\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\") as pbar:\n",
    "        for batch_index, batch in enumerate(train_loader):\n",
    "            # 1. Prepare trin data :\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # 2. Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)\n",
    "\n",
    "            # 3. Backward pass et optimisation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Suivi de la progression\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "            pbar.update(1)\n",
    "        \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0 # time difference in seconds\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}, Epoch time: {dt:.2f} s\")\n",
    "\n",
    "    # Sauvegarder la perte moyenne de l'époque\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "    # Sauvegarder tous les 40 epochs\n",
    "    if (epoch + 1) % save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "        save_checkpoint(epoch + 1, model, optimizer, avg_train_loss, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

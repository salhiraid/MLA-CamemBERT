{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from packaging import version\n",
    "from transformers.utils import logging\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CamembertConfig:\n",
    "    \"\"\"\n",
    "    This is the configuration class to store the configuration of a [`CamembertModel`] or a [`TFCamembertModel`]. \n",
    "    It defines the model architecture and is used to instantiate a Camembert model with specified arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size: int = 32005\n",
    "    hidden_size: int = 768\n",
    "    num_hidden_layers: int = 12\n",
    "    num_attention_heads: int = 12\n",
    "    intermediate_size: int = 3072\n",
    "    hidden_act: str = \"gelu\"\n",
    "    hidden_dropout_prob: float = 0.1\n",
    "    attention_probs_dropout_prob: float = 0.1\n",
    "    max_position_embeddings: int = 514\n",
    "    type_vocab_size: int = 1\n",
    "    initializer_range: float = 0.02\n",
    "    layer_norm_eps: float = 1e-05\n",
    "    pad_token_id: int = 1\n",
    "    bos_token_id: int = 0\n",
    "    eos_token_id: int = 2\n",
    "    position_embedding_type: str = \"absolute\"\n",
    "    use_cache: bool = True\n",
    "    head_type: str = \"MLM\"\n",
    "    classifier_dropout: float = None\n",
    "\n",
    "\n",
    "class CamembertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.position_ids = torch.arange(config.max_position_embeddings).unsqueeze(0)  # Shape (1, max_position_embeddings)\n",
    "        self.token_type_ids = torch.zeros_like(self.position_ids, dtype=torch.long)  # Shape (1, max_position_embeddings)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        input_shape = input_ids.size()\n",
    "        batch_size, seq_length = input_shape\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length].to(input_ids.device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = self.token_type_ids[:, :seq_length].expand(batch_size, seq_length).to(input_ids.device)\n",
    "\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        token_type_embeds = self.token_type_embeddings(token_type_ids)\n",
    "        embeddings = inputs_embeds + position_embeds + token_type_embeds\n",
    "\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "class CamembertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        batch_size, seq_length, hidden_size = x.size()\n",
    "        x = x.view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size)\n",
    "        return x.permute(0, 2, 1, 3)  # [batch, num_heads, seq_len, head_size]\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores /= math.sqrt(self.attention_head_size)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, seq_len]\n",
    "            attention_scores += attention_mask\n",
    "\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        context_layer = context_layer.view(hidden_states.size(0), -1, self.all_head_size)\n",
    "\n",
    "        return context_layer\n",
    "    \n",
    "\n",
    "class CamembertMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CamembertBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = CamembertSelfAttention(config)  # Multi-Head Attention\n",
    "        self.ln_1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)  # LayerNorm après l'attention\n",
    "        self.mlp = CamembertMLP(config)  # Feed Forward Network (MLP)\n",
    "        self.ln_2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)  # LayerNorm après le MLP\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Multi-Head Attention avec résiduel\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        hidden_states = hidden_states + attention_output  # Résiduel\n",
    "        hidden_states = self.ln_1(hidden_states)  # Normalisation\n",
    "\n",
    "        # Feed Forward Network (MLP) avec résiduel\n",
    "        mlp_output = self.mlp(hidden_states)\n",
    "        hidden_states = hidden_states + mlp_output  # Résiduel\n",
    "        hidden_states = self.ln_2(hidden_states)  # Normalisation\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Camembert(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = CamembertEmbeddings(config)\n",
    "        self.encoder = nn.ModuleList([CamembertBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        hidden_states = self.embeddings(input_ids)\n",
    "        for block in self.encoder:\n",
    "            hidden_states = block(hidden_states, attention_mask)\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "model = Camembert(CamembertConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle   \n",
    "from datasets import load_from_disk\n",
    "from transformers import CamembertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Napster\\\\Desktop\\\\M2_ISI\\\\MLA\\\\CamemBERT\\\\MLA-CamemBERT'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(os.path.abspath(\"../../../\"))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenized_oscar_dataset(Dataset):\n",
    "    def __init__(self, raw_tokenised_data):\n",
    "        \"\"\"\n",
    "        Initialiser le dataset avec les données sous forme de dictionnaire.\n",
    "        \"\"\"\n",
    "        self.raw_data = raw_tokenised_data  # Le dictionnaire avec 'input_ids', 'attention_mask', et 'labels'\n",
    "        self.data = {\n",
    "            \"texts\": self.raw_data['text'],\n",
    "            \"input_ids\": self.raw_data['input_ids'],\n",
    "            \"attention_mask\": self.raw_data['attention_mask'],\n",
    "            \"labels\": self.raw_data['labels']\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Retourne le nombre d'exemples dans le dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retourne un exemple individuel sous forme de dictionnaire.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.data['input_ids'][idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.data['attention_mask'][idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.data['labels'][idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    \n",
    "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
    "    \"\"\"Prepare masked tokens for MLM.\"\"\"\n",
    "    labels = inputs.clone()\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # Replace 80% of masked tokens with [MASK]\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # Replace 10% of masked tokens with random words\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def preprocess_function(texts, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Préparer les données pour l'entraînement MLM.\n",
    "    :param texts: Liste de textes.\n",
    "    :param tokenizer: Tokenizer de CamemBERT.\n",
    "    :return: Dictionnaire avec 'input_ids', 'attention_mask', et 'labels'.\n",
    "    \"\"\"\n",
    "    # Tokeniser les textes\n",
    "    tokenized = tokenizer(texts, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    # Appliquer le masquage\n",
    "    masked_inputs, labels = mask_tokens(input_ids, tokenizer)\n",
    "\n",
    "    return { \n",
    "        \"text\": list(texts),\n",
    "        \"input_ids\": masked_inputs.tolist(),\n",
    "        \"attention_mask\": attention_mask.tolist(),\n",
    "        \"labels\": labels.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the downlaoded Oscar :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path =\"data/oscar.Arrow\"\n",
    "mini_oscar_dataset = load_from_disk(dataset_path)\n",
    "train_texts, val_texts = train_test_split(mini_oscar_dataset['text'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess_function(train_texts, tokenizer)\n",
    "val_data = preprocess_function(val_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_tokenized_data(data, filename):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "save_tokenized_data(train_data, \"data/tokenized_data/tokenized_train_data.pkl\")\n",
    "save_tokenized_data(val_data, \"data/tokenized_data/tokenized_val_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenized_data(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "train_data = load_tokenized_data(\"data/tokenized_data/tokenized_train_data.pkl\")\n",
    "val_data = load_tokenized_data(\"data/tokenized_data/tokenized_val_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les datasets\n",
    "train_dataset = Tokenized_oscar_dataset(train_data)\n",
    "val_dataset = Tokenized_oscar_dataset(val_data)\n",
    "\n",
    "# Créer les DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8192, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "Training samples: 8, Validation samples: 2\n"
     ]
    }
   ],
   "source": [
    "# tester la donnée :\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['attention_mask'].shape)\n",
    "print(batch['labels'].shape)\n",
    "\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}, Validation samples: {len(val_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        # print(f\"Initializing {module.__class__.__name__} with Xavier Uniform\")\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        # print(f\"Initializing {module.__class__.__name__} with Constant Weights\")\n",
    "        nn.init.constant_(module.bias, 0)\n",
    "        nn.init.constant_(module.weight, 1)\n",
    "    if hasattr(module, 'bias') and module.bias is not None:\n",
    "        # print(f\"Initializing Bias for {module.__class__.__name__}\")\n",
    "        nn.init.constant_(module.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, loss, save_dir):\n",
    "    \"\"\"\n",
    "    Sauvegarder l'état actuel du modèle, optimiseur et perte.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss,\n",
    "    }\n",
    "    save_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Checkpoint saved at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. data :\n",
    "batch_size = 4096\n",
    "train_data = load_tokenized_data(\"data/tokenized_data/tokenized_train_data.pkl\")\n",
    "val_data = load_tokenized_data(\"data/tokenized_data/tokenized_val_data.pkl\")\n",
    "\n",
    "train_dataset = Tokenized_oscar_dataset(train_data)\n",
    "val_dataset = Tokenized_oscar_dataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Model :  \n",
    "from transformers import CamembertForMaskedLM\n",
    "hf_model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n",
    "# hf_model.apply(init_weights)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1  # 10 %\n",
    "warmup_steps = 10\n",
    "\n",
    "save_interval = 40  # Save the model every 40 epochs \n",
    "eval_interval = 40  # Evaluate the model every 40 epochs\n",
    "num_epochs = 420  # Total epochs\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=learning_rate)\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "# Dossier de sauvegarde\n",
    "save_dir = \"models/mlm_training/model_checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "############################## Training Loop ################################\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Mettre le modèle en mode entraînement\n",
    "    train_loss = 0.0  # Accumulateur pour la perte par époque\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # once in a while evaluate our validation loss : (only for one step, next we'll consider more steps validation)\n",
    "    if epoch % 40 ==0 or epoch == num_epochs - 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for batch_index, batch in enumerate(val_loader):\n",
    "                # 1. Prepare val data :\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                # 2. Forward pass :\n",
    "                # optimizer.zero_grad() not needed since torch.no_grad is used \n",
    "                logits = model(input_ids, attention_mask=attention_mask)['logits']\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            print(f\"Epoch {epoch + 1 }, Validation Loss: {avg_val_loss:.4f}\")\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "    # Update the parameters of the model :\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\") as pbar:\n",
    "        for batch_index, batch in enumerate(train_loader):\n",
    "            # 1. Prepare trin data :\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # 2. Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)\n",
    "\n",
    "            # 3. Backward pass et optimisation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Suivi de la progression\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "            pbar.update(1)\n",
    "        \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0 # time difference in seconds\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}, Epoch time: {dt:.2f} s\")\n",
    "\n",
    "    # Sauvegarder la perte moyenne de l'époque\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "    # Sauvegarder tous les 40 epochs\n",
    "    if (epoch + 1) % save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "        save_checkpoint(epoch + 1, model, optimizer, avg_train_loss, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. data :\n",
    "batch_size = 2\n",
    "train_data = load_tokenized_data(\"data/tokenized_data/tokenized_train_data.pkl\")\n",
    "val_data = load_tokenized_data(\"data/tokenized_data/tokenized_val_data.pkl\")\n",
    "\n",
    "train_dataset = Tokenized_oscar_dataset(train_data)\n",
    "val_dataset = Tokenized_oscar_dataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test our model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Model : \n",
    "model_from_scratch = Camembert(CamembertConfig()) \n",
    "model = model_from_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: torch.Size([2, 512])\n",
      "Attention Mask: torch.Size([2, 512])\n",
      "Logits Shape: torch.Size([2, 512, 32005])\n",
      "torch.Size([2, 512])\n",
      "Predicted Tokens: ['Roland Roland Roland Roland Roland FORMATION règlementation intéressantes Rolandrine Roland Roland FORMATIONrine règlementation Roland Roland es règlementation Roland Roland Roland Roland Roland Roland Roland règlementation Roland règlementation Eventrine Roland Roland Roland rêvé intéressantes Rolandrine FORMATION FORMATION Roland Roland Roland Roland règlementation Roland Roland Roland Roland FORMATION FORMATION Roland Roland règlementation Roland physiques Roland FORMATION Roland bd Roland Roland Roland Roland Roland Roland Roland Roland Roland Roland Rolandrine Roland rêvé Roland FORMATION Roland intéressantes règlementation Roland règlementation FORMATION règlementation Roland Roland Roland Roland Roland règlementation Roland FORMATION es FORMATION Rolandrine Rolandrine FORMATION règlementationrine Roland Roland Roland Roland FORMATION Roland Roland FORMATION Roland Roland Roland Roland Roland Roland Roland Roland Roland Roland Roland Roland Event Roland FORMATION Roland Rolandrine Roland Roland Roland règlementation Roland Rolandrine Rolandrine Roland Roland immobil Rolandrine Roland Roland règlementationenberg Roland FORMATION FORMATION règlementation Roland Roland Roland Roland intéressantes Roland règlementation Roland règlementation Rolandrine Roland Roland Rolandrine règlementation Roland Roland Roland gravement Rolandrine Roland FORMATION Roland Roland Roland FORMATION Roland FORMATIONrinerine FORMATION Roland Roland Roland Rolandrine Roland Roland intéressantesrinerine Rolandrine FORMATION Rolandrine Roland Roland FORMATION Roland Rolandarabi Roland Roland Roland intéressantes règlementation Roland Roland intéressantes FORMATION FORMATIONrine Roland Roland Roland Roland Roland Roland Roland Roland Roland immobil Roland Roland Rolandrine Roland Roland FORMATION Roland Roland Roland Rolandrine intéressantesIran Rolandrine Roland Roland Roland RolandIran Roland Roland coude Rolandrine intéressantes Roland Rolandrine enveloppant Rolandlocataire FORMATION rêvé Rolandrine Rolandrine Roland Roland intéressantes Roland Roland Rolandrine Roland Rolandrine Roland FORMATION Rolandrine Roland Roland Roland Roland Roland Roland FORMATION FORMATIONrine Roland Rolandrine intéressantes Roland Roland Roland FORMATION Roland Roland Roland Rolandrinerine FORMATIONfichier Roland règlementation Roland Roland intéressantes Roland Roland Roland Roland Roland règlementation Roland FORMATION Roland Roland Roland intéressantes règlementation Roland Roland Roland Roland Roland Roland Roland FORMATION Roland Roland FORMATION Roland FORMATIONrine Roland Roland règlementationrine Roland intéressantes Roland Rolandrinerine Roland Roland enveloppant Rolandfichier FORMATION Roland règlementation Roland Roland Roland Roland FORMATION Roland Roland Roland Roland Roland Roland Roland Roland Roland conspiration Roland Roland FORMATION conspiration Roland intéressantes Roland FORMATION Roland Rolandrine intéressantes règlementation Roland Roland Roland FORMATION FORMATION rêvé Roland Roland Roland Roland intéressantes Roland gestion FORMATION Roland Roland Roland intéressantes Roland Roland rêvérine Roland Roland FORMATION FORMATION Roland Roland Roland Rolandlocataire Roland Roland Roland Roland Roland Roland Roland Roland Roland Roland Roland Roland Rolandrine Rolandrine Roland Roland Roland Roland FORMATION Roland Roland Rolandrine Roland intéressantes Roland intéressantesrine Rolandrine conspiration Roland Roland Roland Roland Roland Roland Roland rêvé Roland Roland Roland Roland Roland Roland Roland conspiration FORMATION Rolandrine règlementation Roland Roland Roland Roland Rolandrine Roland règlementation règlementation Roland intéressantes Roland Roland FORMATIONrinerine FORMATION Roland rêvé Roland Roland règlementation Roland FORMATION FORMATION Roland Roland Roland rêvé Rolandrine FORMATION conspiration Roland Roland Roland FORMATION Rolandrine FORMATION FORMATION Roland règlementation Roland FORMATION Roland Roland Rolandrine', 'Roland gestion Roland Roland Roland gestion FORMATION Roland Roland FORMATION Roland FORMATIONfichierrine Roland FORMATION Roland rêvé Roland Roland Roland é Roland FORMATION rêvé Roland règlementation Rolandrine conspiration RolandIran Roland Roland Roland Roland règlementationrine Roland Roland conspiration Roland FORMATION Roland règlementation FORMATION FORMATION RolandrineIran FORMATION Roland FORMATIONrine FORMATION Rolandrine FORMATION FORMATION règlementation conspiration Roland Roland Roland FORMATION FORMATION Roland Roland Roland Rolandrine FORMATION Roland Roland règlementation Roland Roland Roland Roland Rolandrinerine conspiration immobil Roland Roland Roland Roland règlementation Roland Roland Roland Roland Rolandrine Rolandrinerine règlementationrine règlementation Roland Roland Rolandap Roland Roland Rolandrine FORMATION Roland Roland Rolandrine Roland Roland intéressantes FORMATION Roland Roland Event FORMATIONrine Roland Roland es Roland Roland Roland règlementation intéressantes Roland Roland RolandIranrine FORMATION Roland Rolandrine Roland Roland FORMATIONenbergrine FORMATION FORMATION immobil Roland Roland Roland Roland FORMATION Rolandrine Rolandrine Rolandrinerine Roland Roland Roland Roland Roland Roland Roland Roland Rolandrine Roland Roland Roland Roland Roland Roland Roland intéressantesrinerine règlementation Roland FORMATION Roland Roland composés Roland Rolandrine Roland Vand Rolandrine Roland Roland rêvé Roland Roland Roland FORMATION FORMATION Roland Roland Roland Roland conspiration règlementation Roland Roland esrine FORMATIONrine Roland FORMATION Roland conspiration Roland règlementation Roland Rolandrine Roland RolandIran Roland FORMATIONADA FORMATION FORMATION Roland Roland Roland Roland RolandIranIran Roland Roland Roland Roland Roland Roland règlementation Rolandrine FORMATION règlementation Roland intéressantes Roland Roland Rolandrinerine Roland Roland Roland Roland conspiration Rolandrine FORMATION Roland DIrine FORMATION Roland Roland Roland Roland FORMATION Roland Roland Roland Roland Roland Roland Roland Roland FORMATION Roland Roland FORMATION Rolandrine Roland Roland FORMATION Rolandrine Roland FORMATION Roland Roland Roland Rolandrine RolandADArine FORMATIONrine FORMATION Roland Roland Roland Roland Roland Roland Roland rêvé FORMATION FORMATION Roland Roland Roland règlementation règlementation Roland Roland Roland Roland FORMATION rivage Roland FORMATION quatuor Roland FORMATION gestion Rolandrine Roland FORMATION FORMATION FORMATION Roland Roland Roland Rolandrinerine Roland Roland Roland Roland Roland Roland Roland règlementation Roland Roland RolandADArine Roland Rolandrine Rolandrine Roland Roland Roland Rolandrine Roland Roland FORMATION FORMATION Roland Roland FORMATIONenberg Roland Roland Roland FORMATIONIran Rolandenberg Roland Roland Roland règlementationrine Roland Roland Roland Roland Roland Roland Roland Rolandugg Roland Roland RolandADA Roland Roland conspiration Rolandrine Roland Roland Roland Roland RolandADA Roland Roland Roland Roland Roland règlementation Roland Roland immobilrine Rolandtor Rolandrine Roland Roland fosse Roland Roland Rolandrine quatuor Roland Rolandrine règlementationrine Roland rivage intéressantes Roland FORMATIONIranrine Roland Roland Roland Rolandrine Roland règlementation gestion FORMATION Roland Roland FORMATION Roland Roland monarchierine FORMATION Roland Roland Rolandrine Roland Vand Roland immobil Roland Roland FORMATION Roland Roland FORMATION Rolandenberg Roland Roland Rolandrine Roland Roland Roland Roland rivage Roland FORMATION Roland Roland Roland FORMATION FORMATION Roland Roland Roland Roland Roland Roland FORMATION Rolandrine FORMATION Roland FORMATION conspiration Roland FORMATION Rolandrine règlementation Roland']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Napster\\AppData\\Local\\Temp\\ipykernel_30228\\1814485245.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(torch.tensor(predicted_tokens).shape)\n"
     ]
    }
   ],
   "source": [
    "batch  = next(iter(train_loader))\n",
    "\n",
    "input_ids = batch['input_ids']  # Les ID des tokens\n",
    "attention_mask = batch[\"attention_mask\"]  # Masque d'attention\n",
    "\n",
    "print(\"Input IDs:\", input_ids.shape)\n",
    "print(\"Attention Mask:\", attention_mask.shape)\n",
    "\n",
    "# Étape 3 : Passer les données dans le modèle\n",
    "logits = model(input_ids, attention_mask)\n",
    "\n",
    "\n",
    "\n",
    "# Étape 4 : Interpréter les résultats\n",
    "print(\"Logits Shape:\", logits.shape)  # Devrait être (batch_size, seq_length, vocab_size)\n",
    "\n",
    "# Étape 5 : Décoder les prédictions pour voir les tokens les plus probables\n",
    "predicted_tokens = torch.argmax(logits, dim=-1)\n",
    "print(torch.tensor(predicted_tokens).shape)\n",
    "decoded_tokens = [tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_tokens]\n",
    "\n",
    "print(\"Predicted Tokens:\", decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train our own model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 10.5864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 4/4 [00:16<00:00,  4.03s/it, loss=9.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 10.1843, Epoch time: 17.36 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 4/4 [00:18<00:00,  4.52s/it, loss=8.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 9.1851, Epoch time: 18.08 s\n",
      "Epoch 3, Validation Loss: 9.2178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 4/4 [00:16<00:00,  4.08s/it, loss=8.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 8.7238, Epoch time: 17.69 s\n",
      "Checkpoint saved at models/mlm_training/model_checkpoints\\checkpoint_epoch_3.pth\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1  # 10 %\n",
    "warmup_steps = 10\n",
    "\n",
    "save_interval = 40  # Save the model every 40 epochs \n",
    "eval_interval = 40  # Evaluate the model every 40 epochs\n",
    "num_epochs = 3 # 420  # Total epochs\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=learning_rate)\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "# Dossier de sauvegarde\n",
    "save_dir = \"models/mlm_training/model_checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "############################## Training Loop ################################\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Mettre le modèle en mode entraînement\n",
    "    train_loss = 0.0  # Accumulateur pour la perte par époque\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # once in a while evaluate our validation loss : (only for one step, next we'll consider more steps validation)\n",
    "    if epoch % 40 ==0 or epoch == num_epochs - 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for batch_index, batch in enumerate(val_loader):\n",
    "                # 1. Prepare val data :\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                # 2. Forward pass :\n",
    "                # optimizer.zero_grad() not needed since torch.no_grad is used \n",
    "                logits = model(input_ids, attention_mask=attention_mask)\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            print(f\"Epoch {epoch + 1 }, Validation Loss: {avg_val_loss:.4f}\")\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "    # Update the parameters of the model :\n",
    "    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\") as pbar:\n",
    "        for batch_index, batch in enumerate(train_loader):\n",
    "            # 1. Prepare trin data :\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # 2. Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)\n",
    "\n",
    "            # 3. Backward pass et optimisation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Suivi de la progression\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "            pbar.update(1)\n",
    "        \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0 # time difference in seconds\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}, Epoch time: {dt:.2f} s\")\n",
    "\n",
    "    # Sauvegarder la perte moyenne de l'époque\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "    # Sauvegarder tous les 40 epochs\n",
    "    if (epoch + 1) % save_interval == 0 or (epoch + 1) == num_epochs:\n",
    "        save_checkpoint(epoch + 1, model, optimizer, avg_train_loss, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: torch.Size([2, 512])\n",
      "Attention Mask: torch.Size([2, 512])\n",
      "Logits Shape: torch.Size([2, 512, 32005])\n",
      "torch.Size([2, 512])\n",
      "ne pouvez pas directement coller images. Envoyez-les depuis votre ordinateur ou insérez- depuis une URL.\n",
      "Predicted Tokens: ['Vous ne pouvez pas directement coller les images. Envoyez-les depuis votre ordinateur ou insérez- les depuis une URL.', \"La troisième prestation est une revue ponctuelle ou continue de la qualité du codage. Cette activité est complémentaire à l’audit du codage, mais elle ne le remplace pas: l’audit annuel non obligatoire, est réalisé par un organisme certifié mandaté pour vérifier que le travail est exempt d’erreurs, «erreur comprise » , compris strictement dans le sens de «fraudes». En re ant la qualité », nous nous rons une analyse détaillée des dossiers en vue d' identifier s »il y a ddes erreurs ou des oublis justifiant, le cas échéant, une refacturation. L'optimisation du codage: 3 étapes: < SERVICES ANALYSE DU POTENTIEL Dans cette première étape, les experts de la qualité analysent de manière très pointue le codage en cours pour effectuer une révision en temps réel et, si nécessaire, refacturer les dossiers envoyés aux assureurs. REFACTURATION DETAILLEE La deuxième étape consiste à analyser les dossiers des années précédentes et d’examiner s’ils ont été codés correctement : Dans le cas où ils conti ‘ eux des oublis, nous proposons aux directions de faire facturerer les dossiers aux assurances. Il est en effet able de refacturer une prestation jusqu’à cinq ans après la facturation. Pour les dossiers passés contenant des erreurs, Swisscoding peut, selon les préférences de l’établissement faire procéder elle-même au recodage du cas ou alors soutenir les équipes internes chargées de les recoder. Enfin, cet accompagnement peut inclure toute la suite négociée avec les assureurs pour que le travail se passe au mieux. ACCOMPAGNEMENT CONTINU La troisième étape suit la révision des dossiers. Elle consiste à coacher l’équipe de codage de l’ établissement après avoir identifié les informations qui nous auraient échappé. Ce coaching peut se faire soit sous forme d’un accompagnement continu, soit par une révision mensuelle du codage, ou encore par des interventions sur place pour identifier avec l ‘ les dernières informations à notre disposition. De cette manière, les codificateur internes de l’établissement sont en mesure d’identifier les potentiels d’ erreurs erreurs et sont équipés pour les « au plus juste !\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Napster\\AppData\\Local\\Temp\\ipykernel_30228\\917335190.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(torch.tensor(predicted_tokens).shape)\n"
     ]
    }
   ],
   "source": [
    "model = hf_model\n",
    "\n",
    "batch  = next(iter(train_loader))\n",
    "\n",
    "input_ids = batch['input_ids']  # Les ID des tokens\n",
    "attention_mask = batch[\"attention_mask\"]  # Masque d'attention\n",
    "\n",
    "print(\"Input IDs:\", input_ids.shape)\n",
    "print(\"Attention Mask:\", attention_mask.shape)\n",
    "\n",
    "# Étape 3 : Passer les données dans le modèle\n",
    "logits = model(input_ids, attention_mask).logits\n",
    "\n",
    "\n",
    "\n",
    "# Étape 4 : Interpréter les résultats\n",
    "print(\"Logits Shape:\", logits.shape)  # Devrait être (batch_size, seq_length, vocab_size)\n",
    "\n",
    "# Étape 5 : Décoder les prédictions pour voir les tokens les plus probables\n",
    "predicted_tokens = torch.argmax(logits, dim=-1)\n",
    "print(torch.tensor(predicted_tokens).shape)\n",
    "decoded_tokens = [tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_tokens]\n",
    "\n",
    "\n",
    "print(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "print(\"Predicted Tokens:\", decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

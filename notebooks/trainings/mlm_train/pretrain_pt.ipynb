{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle   \n",
    "from datasets import load_from_disk\n",
    "from transformers import CamembertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenized_oscar_dataset(Dataset):\n",
    "    def __init__(self, raw_tokenised_data):\n",
    "        \"\"\"\n",
    "        Initialiser le dataset avec les données sous forme de dictionnaire.\n",
    "        \"\"\"\n",
    "        self.raw_data = raw_tokenised_data  # Le dictionnaire avec 'input_ids', 'attention_mask', et 'labels'\n",
    "        self.data = {\n",
    "            \"texts\": self.raw_data['text'],\n",
    "            \"input_ids\": self.raw_data['input_ids'],\n",
    "            \"attention_mask\": self.raw_data['attention_mask'],\n",
    "            \"labels\": self.raw_data['labels']\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Retourne le nombre d'exemples dans le dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retourne un exemple individuel sous forme de dictionnaire.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.data['input_ids'][idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.data['attention_mask'][idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.data['labels'][idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    \n",
    "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
    "    \"\"\"Prepare masked tokens for MLM.\"\"\"\n",
    "    labels = inputs.clone()\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # Replace 80% of masked tokens with [MASK]\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # Replace 10% of masked tokens with random words\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def preprocess_function(texts, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Préparer les données pour l'entraînement MLM.\n",
    "    :param texts: Liste de textes.\n",
    "    :param tokenizer: Tokenizer de CamemBERT.\n",
    "    :return: Dictionnaire avec 'input_ids', 'attention_mask', et 'labels'.\n",
    "    \"\"\"\n",
    "    # Tokeniser les textes\n",
    "    tokenized = tokenizer(texts, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "    # Appliquer le masquage\n",
    "    masked_inputs, labels = mask_tokens(input_ids, tokenizer)\n",
    "\n",
    "    return { \n",
    "        \"text\": list(texts),\n",
    "        \"input_ids\": masked_inputs.tolist(),\n",
    "        \"attention_mask\": attention_mask.tolist(),\n",
    "        \"labels\": labels.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the downlaoded Oscar :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path =\"data/oscar.Arrow\"\n",
    "mini_oscar_dataset = load_from_disk(dataset_path)\n",
    "train_texts, val_texts = train_test_split(mini_oscar_dataset['text'], test_size=0.2, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertConfig, CamembertForMaskedLM, AdamW, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling, CamembertTokenizer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed617b32e60645b2b4084451d716aa73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import CamembertTokenizer\n",
    "\n",
    "# Load the dataset from a local directory\n",
    "dataset_path = \"/home/amine/CamemBERT/data/CamemBERT/data/mini_oscar_1.2/mini_dataset.arrow\"  # Replace with the path to your saved dataset\n",
    "hf_dataset = load_from_disk(dataset_path)\n",
    "\n",
    "# Load the Camembert tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Define masking function\n",
    "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
    "    \"\"\"Prepare masked tokens for MLM.\"\"\"\n",
    "    labels = inputs.clone()\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # Replace 80% of masked tokens with [MASK]\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # Replace 10% of masked tokens with random words\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "# Tokenization and masking\n",
    "def preprocess_data(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids, labels = mask_tokens(tokenized[\"input_ids\"], tokenizer)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],  # Include attention mask\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "# Preprocess the dataset\n",
    "tokenized_dataset = hf_dataset.map(preprocess_data, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorForLanguageModeling\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define a data collator\u001b[39;00m\n\u001b[1;32m      4\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(\n\u001b[0;32m----> 5\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m,\n\u001b[1;32m      6\u001b[0m     mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Enable MLM\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     mlm_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m,  \u001b[38;5;66;03m# Masking probability\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define a data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,  # Enable MLM\n",
    "    mlm_probability=0.15,  # Masking probability\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertForMaskedLM\n",
    "\n",
    "# Load CamembertForMaskedLM\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./camembert-mlm\",  # Directory to save checkpoints\n",
    "    overwrite_output_dir=True,  # Overwrite previous outputs\n",
    "    evaluation_strategy=\"steps\",  # Evaluate every `eval_steps`\n",
    "    save_strategy=\"steps\",  # Save checkpoint every `save_steps`\n",
    "    per_device_train_batch_size=8,  # Batch size per GPU\n",
    "    gradient_accumulation_steps=64,  # Effective batch size = 8 * 64 = 512\n",
    "    learning_rate=1e-4,  # Learning rate from Camembert paper\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    warmup_steps=10000,  # Warmup steps\n",
    "    max_steps=1000000,  # Total training steps\n",
    "    logging_dir=\"./logs\",  # Directory for logs\n",
    "    logging_steps=500,  # Log every 500 steps\n",
    "    save_steps=10000,  # Save every 10,000 steps\n",
    "    eval_steps=10000,  # Evaluate every 10,000 steps\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    num_train_epochs=5,  # Train for 5 epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define a data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,  # Enable MLM\n",
    "    mlm_probability=0.15,  # Masking probability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,  # Ensures proper tokenization and batching\n",
    "    data_collator=data_collator,  # Handles attention_mask and MLM masking\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory /mini_oscar not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     10\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mini_oscar\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your dataset path\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m hf_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m oscar_dataset \u001b[38;5;241m=\u001b[39m OscarDataset(hf_dataset, tokenizer)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# === Convert Dataset into Hugging Face Format === #\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Hugging Face Trainer expects datasets in dictionary format with labels included\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/.venv/lib/python3.10/site-packages/datasets/load.py:2207\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2205\u001b[0m fs, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m url_to_fs(dataset_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[1;32m   2206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m-> 2207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(\n\u001b[1;32m   2209\u001b[0m     posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[1;32m   2210\u001b[0m ):\n\u001b[1;32m   2211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory /mini_oscar not found"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import CamembertForMaskedLM, CamembertTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_from_disk\n",
    "from dataset import OscarDataset\n",
    "\n",
    "# === Initialize Tokenizer and Dataset === #\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/mini_oscar\"  # Replace with your dataset path\n",
    "hf_dataset = load_from_disk(dataset_path)\n",
    "oscar_dataset = OscarDataset(hf_dataset, tokenizer)\n",
    "# === Convert Dataset into Hugging Face Format === #\n",
    "# Hugging Face Trainer expects datasets in dictionary format with labels included\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([item[\"masked_input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
    "        \"labels\": torch.stack([item[\"labels\"] for item in batch]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salhi\\.conda\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CamembertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Model initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "CamembertForMaskedLM                                         --\n",
       "├─CamembertModel: 1-1                                        --\n",
       "│    └─CamembertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                                   23,440,896\n",
       "│    │    └─Embedding: 3-2                                   393,216\n",
       "│    │    └─Embedding: 3-3                                   1,536\n",
       "│    │    └─LayerNorm: 3-4                                   1,536\n",
       "│    │    └─Dropout: 3-5                                     --\n",
       "│    └─CamembertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                                  85,054,464\n",
       "├─CamembertLMHead: 1-2                                       --\n",
       "│    └─Linear: 2-3                                           590,592\n",
       "│    └─LayerNorm: 2-4                                        1,536\n",
       "│    └─Linear: 2-5                                           23,471,418\n",
       "=====================================================================================\n",
       "Total params: 132,955,194\n",
       "Trainable params: 132,955,194\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # model summary\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CamembertForMaskedLM, CamembertTokenizer, TrainingArguments, Trainer , CamembertConfig\n",
    "from datasets import load_from_disk\n",
    "from dataset import OscarDataset\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "# === Model === #\n",
    "# config = CamembertConfig(\n",
    "#     vocab_size=tokenizer.vocab_size,  # Adjust to match your tokenizer's vocab size\n",
    "#     hidden_size=768,                 # Hidden size (RoBERTa_BASE)\n",
    "#     num_hidden_layers=12,            # Number of transformer layers\n",
    "#     num_attention_heads=12,          # Number of attention heads\n",
    "#     intermediate_size=3072,          # FFN inner hidden size\n",
    "#     hidden_dropout_prob=0.1,         # Dropout probability\n",
    "#     attention_probs_dropout_prob=0.1, # Attention dropout probability\n",
    "#     max_position_embeddings=514,     # Maximum sequence length + special tokens\n",
    "#     type_vocab_size=1,               # No token type embeddings\n",
    "#     initializer_range=0.02           # Standard deviation for weight initialization\n",
    "# )\n",
    "\n",
    "config = CamembertConfig()\n",
    "print(config)\n",
    "# Initialize a randomly weighted CamembertForMaskedLM model\n",
    "model = CamembertForMaskedLM(config) \n",
    "# model.to(\"cuda\")\n",
    "\n",
    "print(\"Model initialized\")\n",
    "\n",
    "summary(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training Arguments === #\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./camembert_mlm\",  # Directory to save the model and checkpoints\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # 3 epochs as specified\n",
    "    per_device_train_batch_size=8,  # Batch size\n",
    "    gradient_accumulation_steps=16,  # Effective batch size = 8 * 16\n",
    "    learning_rate=6e-4,  # Peak learning rate for base\n",
    "    weight_decay=0.01,  # Weight decay\n",
    "    max_steps=500000,  # Train for 500k steps\n",
    "    warmup_steps=24000,  # 24k warmup steps for base\n",
    "    save_steps=5000,  # Save model every 5000 steps\n",
    "    logging_steps=500,  # Log training loss every 500 steps\n",
    "    save_total_limit=2,  # Keep only the 2 most recent checkpoints\n",
    "    lr_scheduler_type=\"linear\",  # Linear learning rate decay\n",
    "    evaluation_strategy=\"no\",  # No validation dataset (can be added if needed)\n",
    "    fp16=True,  # Mixed precision training for faster performance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Trainer === #\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=oscar_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# === Train === #\n",
    "trainer.train()\n",
    "\n",
    "# === Save the Final Model === #\n",
    "trainer.save_model(\"./camembert_mlm\")\n",
    "tokenizer.save_pretrained(\"./camembert_mlm\")\n",
    "print(\"Training complete. Model saved to ./camembert_mlm.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

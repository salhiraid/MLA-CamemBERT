{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np \n",
    "from transformers import BertTokenizerFast \n",
    "from transformers import DataCollatorForTokenClassification \n",
    "from transformers import AutoModelForTokenClassification \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CamembertModel, CamembertTokenizer, AdamW, get_cosine_schedule_with_warmup \n",
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "class CamembertConfig:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 32005\n",
    "        self.hidden_size = 768\n",
    "        self.num_hidden_layers = 12\n",
    "        self.num_attention_heads = 12\n",
    "        self.intermediate_size = 3072\n",
    "        self.hidden_act = \"gelu\"\n",
    "        self.hidden_dropout_prob = 0.1\n",
    "        self.attention_probs_dropout_prob = 0.1\n",
    "        self.max_position_embeddings = 514\n",
    "        self.type_vocab_size = 1\n",
    "        self.initializer_range = 0.02\n",
    "        self.layer_norm_eps = 1e-5\n",
    "        self.pad_token_id = 1\n",
    "        self.head_type = \"MLM\"\n",
    "class CamembertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        token_type_embeds = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + position_embeds + token_type_embeds\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        # Debug prints\n",
    "        # print(f\"Embeddings NaN: {torch.isnan(embeddings).any()}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "class CamembertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.dropout = nn.Dropout(0.2)  # Increased dropout rate\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        return x.view(new_x_shape).permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        # Debug query, key, value\n",
    "        # print(f\"Query NaN: {torch.isnan(query_layer).any()}\")\n",
    "        # print(f\"Key NaN: {torch.isnan(key_layer).any()}\")\n",
    "        # print(f\"Value NaN: {torch.isnan(value_layer).any()}\")\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores /= math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # Clamp scores to prevent overflow\n",
    "        attention_scores = torch.clamp(attention_scores, min=-1e9, max=1e9)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1) + 1e-9\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Debug attention scores and probabilities\n",
    "        # print(f\"Attention Scores NaN Before Clamp: {torch.isnan(attention_scores).any()}\")\n",
    "        # print(f\"Attention Probs NaN: {torch.isnan(attention_probs).any()}\")\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        context_layer = context_layer.view(context_layer.size(0), -1, self.all_head_size)\n",
    "\n",
    "        # Debug context layer\n",
    "        # print(f\"Context Layer NaN: {torch.isnan(context_layer).any()}\")\n",
    "\n",
    "        return context_layer\n",
    "\n",
    "\n",
    "\n",
    "class CamembertFeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.activation = F.gelu if config.hidden_act == \"gelu\" else nn.ReLU()\n",
    "        self.dense_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(0.2)  # Increased dropout rate\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        intermediate_output = self.activation(self.dense_1(hidden_states))\n",
    "        intermediate_output = torch.clamp(intermediate_output, min=-1e9, max=1e9)\n",
    "\n",
    "        output = self.dense_2(intermediate_output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.LayerNorm(output + hidden_states)\n",
    "\n",
    "        # Debug intermediate and final outputs\n",
    "        # print(f\"Intermediate Output NaN: {torch.isnan(intermediate_output).any()}\")\n",
    "        # print(f\"Final Output NaN: {torch.isnan(output).any()}\")\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CamembertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = CamembertSelfAttention(config)\n",
    "        self.attention_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.feed_forward = CamembertFeedForward(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        hidden_states = self.attention_norm(hidden_states + attention_output)\n",
    "        return self.feed_forward(hidden_states)\n",
    "\n",
    "class CamembertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([CamembertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "\n",
    "            # Debug prints for each layer\n",
    "            # print(f\"Layer {i} Hidden States NaN: {torch.isnan(hidden_states).any()}\")\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "class CamembertLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = F.gelu(self.dense(hidden_states))\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        logits = self.decoder(hidden_states)\n",
    "\n",
    "        # Debug prints\n",
    "        # print(f\"Logits NaN: {torch.isnan(logits).any()}\")\n",
    "\n",
    "        return logits\n",
    "        \n",
    "class Camembert_emb_en(nn.Module) : \n",
    "    def __init__(self, config):\n",
    "        super(Camembert_emb_en , self).__init__()\n",
    "        self.embeddings = CamembertEmbeddings(config)\n",
    "        self.encoder = CamembertEncoder(config)\n",
    "        self.config = config \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded_input = self.embeddings(input_ids)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = (1.0 - attention_mask) * -float('inf')\n",
    "\n",
    "        encoder_output = self.encoder(embedded_input, attention_mask)\n",
    "        return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamemBERTBaseModel(nn.Module):\n",
    "    def __init__(self, model_path: str, trainable: bool = False):\n",
    "        super(CamemBERTBaseModel, self).__init__()\n",
    "        self.base_model = CamembertModel.from_pretrained(model_path)\n",
    "        self.trainable = trainable\n",
    "        self.config = CamembertConfig()\n",
    "\n",
    "        if not trainable:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.base_model.eval()\n",
    "        else:\n",
    "            self.base_model.train()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "    def get_hidden_size(self) -> int:\n",
    "        return self.base_model.config.hidden_size\n",
    "\n",
    "class CamemBERTBaseModelv2(nn.Module):\n",
    "    def __init__(self, model_path: str, config=None, trainable: bool = False):\n",
    "        super(CamemBERTBaseModelv2, self).__init__()\n",
    "        self.base_model = Camembert_emb_en(config)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "        # Filter out head-related keys\n",
    "        #filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"head.\")}\n",
    "        \n",
    "        # Load the filtered state_dict\n",
    "        self.base_model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"Model loaded from {}\".format(model_path))\n",
    "\n",
    "        self.trainable = trainable\n",
    "        self.config = config\n",
    "\n",
    "        # Set trainable state\n",
    "        if not trainable:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.base_model.eval()\n",
    "        else:\n",
    "            self.base_model.train()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        # Get structured output from base_model\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def get_hidden_size(self) -> int:\n",
    "        return self.base_model.config.hidden_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerFinetuningModel(nn.Module):\n",
    "    def __init__(self, model_path: str, num_labels: int = 9, trainable: bool = True , our_model = False , config = None):\n",
    "        super(NerFinetuningModel, self).__init__()\n",
    "        if our_model : \n",
    "            self.base_model = CamemBERTBaseModelv2(model_path, trainable=trainable , config=config)\n",
    "        else :\n",
    "            self.base_model = CamemBERTBaseModel(model_path, trainable=trainable)\n",
    "        self.hidden_size = self.base_model.get_hidden_size()\n",
    "        self.ner_head = nn.Linear(self.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor = None):\n",
    "        hidden_states = self.base_model(input_ids, attention_mask)\n",
    "    \n",
    "        logits = self.ner_head(hidden_states)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return {\"logits\": logits, \"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=128, label_all_tokens=True):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label_all_tokens = label_all_tokens\n",
    "\n",
    "    def tokenize_and_align_labels(self, tokens, ner_tags):\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            tokens,\n",
    "            truncation=True,\n",
    "            is_split_into_words=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        labels = []\n",
    "        for i, label in enumerate(ner_tags):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)  \n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(label[word_idx] if self.label_all_tokens else -100)\n",
    "                previous_word_idx = word_idx\n",
    "            labels.append(label_ids)\n",
    "\n",
    "        # Convert all outputs to tensors\n",
    "        tokenized_inputs = {key: torch.tensor(val, dtype=torch.long) for key, val in tokenized_inputs.items()}\n",
    "        tokenized_inputs[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        return tokenized_inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        tokenized_data = self.tokenize_and_align_labels(\n",
    "            [data['tokens']], [data['ner_tags']]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokenized_data['input_ids'].squeeze(),\n",
    "            'attention_mask': tokenized_data['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(tokenized_data['labels'][0], dtype=torch.long),\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, device, lr=5e-5, num_labels=9, save_dir=\"./models\"):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    num_training_steps = num_epochs * len(train_loader)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        correct, total = 0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask, labels)\n",
    "            logits = outputs[\"logits\"]\n",
    "            loss = outputs[\"loss\"]\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            all_preds.extend(preds.view(-1).tolist())\n",
    "            all_labels.extend(labels.view(-1).tolist())\n",
    "            correct += (preds.view(-1) == labels.view(-1)).sum().item()\n",
    "            total += labels.view(-1).numel()\n",
    "\n",
    "        # Compute metrics\n",
    "        valid_preds = [p for p, l in zip(all_preds, all_labels) if l != -100]\n",
    "        valid_labels = [l for l in all_labels if l != -100]\n",
    "        train_f1 = f1_score(valid_labels, valid_preds, average=\"weighted\", labels=list(range(num_labels)))\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct, total = 0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask, labels)\n",
    "                logits = outputs[\"logits\"]\n",
    "                loss = outputs[\"loss\"]\n",
    "\n",
    "                # Accumulate loss\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # Compute accuracy\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                all_preds.extend(preds.view(-1).tolist())\n",
    "                all_labels.extend(labels.view(-1).tolist())\n",
    "                correct += (preds.view(-1) == labels.view(-1)).sum().item()\n",
    "                total += labels.view(-1).numel()\n",
    "\n",
    "        # Compute metrics\n",
    "        valid_preds = [p for p, l in zip(all_preds, all_labels) if l != -100]\n",
    "        valid_labels = [l for l in all_labels if l != -100]\n",
    "        val_f1 = f1_score(valid_labels, valid_preds, average=\"weighted\", labels=list(range(num_labels)))\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Save model\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), f\"{save_dir}/ner_model_epoch_{epoch + 1}.pth\")\n",
    "        print(f\"Model saved to {save_dir}/ner_model_epoch_{epoch + 1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_765426/3044594322.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from /home/amine/Noureddine/MLA-CamemBERT/notebooks/trainings/models/Pretraining/model_checkpoints/checkpoint_epoch_9.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_765426/1560678306.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(tokenized_data['labels'][0], dtype=torch.long),\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipykernel_765426/1560678306.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(tokenized_data['labels'][0], dtype=torch.long),\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 1.6680, Train Accuracy: 0.0402, Train F1: 0.2705\n",
      "Val Loss: 1.6727, Val Accuracy: 0.0397, Val F1: 0.2666\n",
      "Model saved to ./models/NER_finetune_from_our_pretrained_model/ner_model_epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_765426/1560678306.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(tokenized_data['labels'][0], dtype=torch.long),\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipykernel_765426/1560678306.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(tokenized_data['labels'][0], dtype=torch.long),\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 1.6653, Train Accuracy: 0.0402, Train F1: 0.2705\n",
      "Val Loss: 1.6670, Val Accuracy: 0.0397, Val F1: 0.2666\n",
      "Model saved to ./models/NER_finetune_from_our_pretrained_model/ner_model_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_765426/1560678306.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(tokenized_data['labels'][0], dtype=torch.long),\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipykernel_765426/1560678306.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(tokenized_data['labels'][0], dtype=torch.long),\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 1.6639, Train Accuracy: 0.0402, Train F1: 0.2705\n",
      "Val Loss: 1.6669, Val Accuracy: 0.0397, Val F1: 0.2666\n",
      "Model saved to ./models/NER_finetune_from_our_pretrained_model/ner_model_epoch_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_765426/1560678306.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(tokenized_data['labels'][0], dtype=torch.long),\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipykernel_765426/1560678306.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(tokenized_data['labels'][0], dtype=torch.long),\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 1.6628, Train Accuracy: 0.0402, Train F1: 0.2705\n",
      "Val Loss: 1.6657, Val Accuracy: 0.0397, Val F1: 0.2666\n",
      "Model saved to ./models/NER_finetune_from_our_pretrained_model/ner_model_epoch_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_765426/1560678306.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(tokenized_data['labels'][0], dtype=torch.long),\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipykernel_765426/1560678306.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(tokenized_data['labels'][0], dtype=torch.long),\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Train Loss: 1.6619, Train Accuracy: 0.0402, Train F1: 0.2705\n",
      "Val Loss: 1.6655, Val Accuracy: 0.0397, Val F1: 0.2666\n",
      "Model saved to ./models/NER_finetune_from_our_pretrained_model/ner_model_epoch_5.pth\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")#AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "dataset = load_dataset(\"unimelb-nlp/wikiann\", \"fr\", trust_remote_code=True)\n",
    "train_data = NERDataset(dataset['train'], tokenizer)\n",
    "val_data = NERDataset(dataset['validation'], tokenizer)\n",
    "test_data = NERDataset(dataset['test'], tokenizer)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32)\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n",
    "config = CamembertConfig()\n",
    "    # Initialize model\n",
    "model_path = \"/home/amine/Noureddine/MLA-CamemBERT/notebooks/trainings/models/Pretraining/model_checkpoints/checkpoint_epoch_9.pth\"#\"camembert-base\"\n",
    "num_labels = len(dataset[\"train\"].features[\"ner_tags\"].feature.names)\n",
    "model = NerFinetuningModel(model_path, num_labels=num_labels, trainable=True , our_model=True , config=config)\n",
    "\n",
    "    # Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_model(model, train_loader, val_loader, num_epochs=5, device=device, save_dir=\"./models/NER_finetune_from_our_pretrained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

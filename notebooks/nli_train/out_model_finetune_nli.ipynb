{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress unnecessary warnings and set verbosity for Transformers\n",
    "import warnings\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# PyTorch core libraries\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# Transformers and Datasets\n",
    "from transformers import CamembertModel, CamembertTokenizer, CamembertConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# PyTorch Lightning and Metrics\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Visualization and DataFrame utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XNLIDataset(Dataset):\n",
    "    def __init__(self, cache_directory, split=\"train\", language=\"fr\", tokenizer=tokenizer, max_length=64):\n",
    "        \"\"\"\n",
    "        PyTorch-compatible dataset for the XNLI dataset.\n",
    "\n",
    "        Args:\n",
    "            split (str): Data split to load (\"train\", \"test\", or \"validation\").\n",
    "            language (str): Target language for the dataset.\n",
    "            cache_directory (str): Directory to cache the downloaded dataset.\n",
    "            max_length (int): Maximum sequence length for padding/truncation.\n",
    "        \"\"\"\n",
    "        super(XNLIDataset, self).__init__()\n",
    "        self.split = split\n",
    "        self.language = language\n",
    "        self.cache_directory = cache_directory\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Load the data and the tokenizer\n",
    "        self.data = load_dataset(\n",
    "            \"facebook/xnli\",\n",
    "            name=self.language,\n",
    "            cache_dir=self.cache_directory\n",
    "        )[self.split]  # Load the specified data split\n",
    "\n",
    "        self.tokenizer = tokenizer  # CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the size of the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a specific sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            dict: Contains `input_ids`, `attention_mask`, and `label`.\n",
    "        \"\"\"\n",
    "        example = self.data[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            example[\"premise\"],\n",
    "            example[\"hypothesis\"],\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Add the labels\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}  # Remove batch dimension\n",
    "        inputs[\"label\"] = torch.tensor(example[\"label\"], dtype=torch.long)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since facebook/xnli couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'fr' at ..\\..\\..\\data\\xnli\\facebook___xnli\\fr\\0.0.0\\b8dd5d7af51114dbda02c0e3f6133f332186418e (last modified on Wed Dec  4 15:17:37 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise : Et il a dit, maman, je suis à la maison.\n",
      "Hypothesis : Il a dit à sa mère qu'il était rentré.\n",
      "Label : 0 (entailment)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"facebook/xnli\", name='fr', cache_dir=\"../../../data/xnli\")\n",
    "\n",
    "# Display some examples from the dataset\n",
    "print(f\"Premise : {dataset['validation'][2]['premise']}\")\n",
    "print(f\"Hypothesis : {dataset['validation'][2]['hypothesis']}\")\n",
    "print(f\"Label : {dataset['validation'][2]['label']} (entailment)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([256, 64])\n",
      "Token IDs (example):\n",
      "tensor([    5,   139,    51,    33,   227,     7,  2699,     7,    50,   146,\n",
      "           15,    13,   269,     9,     6,     6,    69,    33,   227,    15,\n",
      "           77,   907,    46,    11,    62,   149, 10540,     9,     6,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1]) \n",
      "\n",
      "Decoded text (example):\n",
      "<s> Et il a dit, maman, je suis à la maison.</s></s> Il a dit à sa mère qu'il était rentré.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/xnli\"\n",
    "\n",
    "xnli_train_dataset = XNLIDataset(split=\"train\", language=\"fr\", cache_directory=data_path, max_length=64)\n",
    "xnli_val_dataset = XNLIDataset(split=\"validation\", language=\"fr\", cache_directory=data_path, max_length=64)\n",
    "\n",
    "train_loader = DataLoader(xnli_train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(xnli_val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "print(f\"Batch shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Token IDs (example):\\n{batch['input_ids'][2]} \\n\")\n",
    "decoded_text = tokenizer.decode(batch['input_ids'][2])\n",
    "print(f\"Decoded text (example):\\n{decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamemBERTBaseModel(nn.Module):\n",
    "    def __init__(self, model_path: str, trainable: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the base CamemBERT model.\n",
    "        param model_path: Path to the pre-trained CamemBERT model.\n",
    "        \"\"\"\n",
    "        super(CamemBERTBaseModel, self).__init__()\n",
    "        self.base_model = CamembertModel.from_pretrained(model_path)\n",
    "        self.tranaible = trainable\n",
    "        self.config = CamembertConfig()\n",
    "        \n",
    "        if not trainable:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.base_model.eval()\n",
    "        else :\n",
    "            self.base_model.train()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the base model.\n",
    "        param input_ids: Tensor of token IDs.\n",
    "        param attention_mask: Tensor of attention masks.\n",
    "        return: Last hidden states from the base model.\n",
    "        \"\"\"\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "    def get_hidden_size(self) -> int:\n",
    "        return self.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CamemBERTBaseModel(nn.Module):\n",
    "    def __init__(self, model_path: str, trainable: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the base model from a saved .pth file.\n",
    "        :param model_path: Path to the saved model file (.pth).\n",
    "        :param trainable: Whether the model's parameters are trainable.\n",
    "        \"\"\"\n",
    "        super(CamemBERTBaseModel, self).__init__()\n",
    "        # Charger le modèle sauvegardé\n",
    "        self.state_dict = torch.load(model_path)\n",
    "    \n",
    "        self.base_model = torch.load_state_dict(self.state_dict)\n",
    "        self.trainable = trainable\n",
    "        \n",
    "        # Configurer les paramètres comme non entraînables si spécifié\n",
    "        if not trainable:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.base_model.eval()\n",
    "        else:\n",
    "            self.base_model.train()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the base model.\n",
    "        :param input_ids: Tensor of input features.\n",
    "        :param attention_mask: Tensor of attention masks.\n",
    "        :return: Output of the base model.\n",
    "        \"\"\"\n",
    "        # Le comportement exact dépend du modèle sauvegardé\n",
    "        outputs = self.base_model(input_ids, attention_mask)\n",
    "        return outputs\n",
    "\n",
    "    def get_hidden_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the hidden size of the model.\n",
    "        :return: Hidden size as an integer.\n",
    "        \"\"\"\n",
    "        # Cette méthode doit être adaptée pour retourner la dimension de sortie\n",
    "        # Assurez-vous que votre modèle a une propriété ou un attribut correspondant\n",
    "        return self.base_model.hidden_size if hasattr(self.base_model, \"hidden_size\") else 768\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m CamembertModel(config)\n\u001b[0;32m      5\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNapster\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mM2_ISI\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMLA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCamemBERT\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMLA-CamemBERT\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcheckpoint_epoch_4.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n",
      "File \u001b[1;32mc:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\torch\\serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\torch\\serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[0;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32mc:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\torch\\serialization.py:1812\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1811\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1812\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\torch\\serialization.py:1784\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1779\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1784\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1785\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1786\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1787\u001b[0m )\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1790\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\torch\\serialization.py:601\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 601\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\torch\\serialization.py:539\u001b[0m, in \u001b[0;36m_deserialize\u001b[1;34m(backend_name, obj, location)\u001b[0m\n\u001b[0;32m    537\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m--> 539\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\torch\\serialization.py:508\u001b[0m, in \u001b[0;36m_validate_device\u001b[1;34m(location, backend_name)\u001b[0m\n\u001b[0;32m    506\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    509\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m     )\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    516\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "from src.model import CamembertModel, CamembertConfig\n",
    "config = CamembertConfig()\n",
    "model = CamembertModel(config)\n",
    "\n",
    "model_path = r\"C:\\Users\\Napster\\Desktop\\M2_ISI\\MLA\\CamemBERT\\MLA-CamemBERT\\models\\checkpoint_epoch_4.pth\"\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIFinetuningModel(nn.Module):\n",
    "    def __init__(self, base_model, num_labels: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the NLI fine-tuning model.\n",
    "        :param base_model: Instance of the base model.\n",
    "        :param num_labels: Number of labels for NLI.\n",
    "        \"\"\"\n",
    "        super(NLIFinetuningModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        self.hidden_size = base_model.get_hidden_size()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.nli_head = nn.Linear(self.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Forward pass for NLI fine-tuning.\n",
    "        :param input_ids: Tensor of input features.\n",
    "        :param attention_mask: Tensor of attention masks.\n",
    "        :param labels: Optional tensor of labels (batch_size).\n",
    "        :return: Dictionary containing logits and optionally loss.\n",
    "        \"\"\"\n",
    "        # Get last hidden states from the base model\n",
    "        hidden_states = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the [CLS] token's representation or equivalent\n",
    "        cls_output = hidden_states[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        # Pass through the NLI head\n",
    "        logits = self.nli_head(cls_output)  # Shape: (batch_size, num_labels)\n",
    "\n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return {\"logits\": logits, \"loss\": loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLI(pl.LightningModule):\n",
    "    def __init__(self, model, lr=5e-5):\n",
    "        \"\"\"\n",
    "        NLI model for training with PyTorch Lightning.\n",
    "        :param model: Instance of the fine-tuning model.\n",
    "        :param lr: Learning rate.\n",
    "        \"\"\"\n",
    "        super(NLI, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "        # Accuracy metrics for training and validation\n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "\n",
    "        # Metrics tracked per step\n",
    "        self.train_losses_step = []\n",
    "        self.train_accuracies_step = []\n",
    "        self.val_losses_step = []\n",
    "        self.val_accuracies_step = []\n",
    "\n",
    "        # Metrics tracked per epoch\n",
    "        self.train_losses_epoch = []\n",
    "        self.train_accuracies_epoch = []\n",
    "        self.val_losses_epoch = []\n",
    "        self.val_accuracies_epoch = []\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Forward pass for inference.\n",
    "        :param batch: Input batch containing input IDs, attention masks, and labels.\n",
    "        :return: Model logits.\n",
    "        \"\"\"\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self.model(input_ids, attention_mask, labels)\n",
    "        return outputs[\"logits\"]\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        \"\"\"\n",
    "        Performs a single training step.\n",
    "        :param batch: Input batch containing input IDs, attention masks, and labels.\n",
    "        :param batch_index: Index of the batch.\n",
    "        :return: Training loss.\n",
    "        \"\"\"\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        outputs = self.model(input_ids, attention_mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        # Compute accuracy\n",
    "        preds = torch.argmax(outputs[\"logits\"], dim=1)\n",
    "        acc = self.train_accuracy(preds, labels)\n",
    "\n",
    "        # Store step metrics\n",
    "        self.train_losses_step.append(loss.item())\n",
    "        self.train_accuracies_step.append(acc.item())\n",
    "\n",
    "        # Log metrics for progress bar\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=False)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True, on_step=True, on_epoch=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Computes and stores epoch-level training metrics at the end of each epoch.\n",
    "        \"\"\"\n",
    "        avg_loss = torch.tensor(self.train_losses_step).mean().item()\n",
    "        avg_acc = torch.tensor(self.train_accuracies_step).mean().item()\n",
    "\n",
    "        self.train_losses_epoch.append(avg_loss)\n",
    "        self.train_accuracies_epoch.append(avg_acc)\n",
    "\n",
    "        # Display epoch results\n",
    "        print(f\"[Epoch {self.current_epoch}] Train Loss: {avg_loss:.4f}, Train Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "        # Clear step metrics to prepare for the next epoch\n",
    "        self.train_losses_step.clear()\n",
    "        self.train_accuracies_step.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        \"\"\"\n",
    "        Performs a single validation step.\n",
    "        :param batch: Input batch containing input IDs, attention masks, and labels.\n",
    "        :param batch_index: Index of the batch.\n",
    "        :return: Validation loss.\n",
    "        \"\"\"\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        outputs = self.model(input_ids, attention_mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        # Compute accuracy\n",
    "        preds = torch.argmax(outputs[\"logits\"], dim=1)\n",
    "        acc = self.val_accuracy(preds, labels)\n",
    "\n",
    "        # Store step metrics\n",
    "        self.val_losses_step.append(loss.item())\n",
    "        self.val_accuracies_step.append(acc.item())\n",
    "\n",
    "        # Log metrics for progress bar\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Computes and stores epoch-level validation metrics at the end of each epoch.\n",
    "        \"\"\"\n",
    "        avg_loss = torch.tensor(self.val_losses_step).mean().item()\n",
    "        avg_acc = torch.tensor(self.val_accuracies_step).mean().item()\n",
    "\n",
    "        self.val_losses_epoch.append(avg_loss)\n",
    "        self.val_accuracies_epoch.append(avg_acc)\n",
    "\n",
    "        # Display epoch results\n",
    "        print(f\"[Epoch {self.current_epoch}] Val Loss: {avg_loss:.4f}, Val Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "        # Clear step metrics to prepare for the next epoch\n",
    "        self.val_losses_step.clear()\n",
    "        self.val_accuracies_step.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures the optimizer and learning rate scheduler.\n",
    "        :return: Dictionary containing the optimizer and scheduler configurations.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "        # Dynamically calculate the total number of steps\n",
    "        steps_per_epoch = 1534\n",
    "        total_steps = steps_per_epoch * self.trainer.max_epochs\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.lr,\n",
    "            total_steps=total_steps,\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy=\"linear\",\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Napster\\\\Desktop\\\\M2_ISI\\\\MLA\\\\CamemBERT\\\\MLA-CamemBERT'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.chdir(\"../../../../../data/xnli\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(train_loader)\n",
    "\n",
    "# Define the path to the pre-trained model\n",
    "model_path = \"models/4gb_oscar\"  # Path to the Hugging Face pre-trained model\n",
    "\n",
    "# Step 1: Create the fine-tuning model\n",
    "nli_camembert = NLIFinetuningModel(\n",
    "    base_model=CamemBERTBaseModel(model_path, trainable=True),  # Use a trainable base model\n",
    "    num_labels=3  # Classes: entailment, neutral, contradiction\n",
    ")\n",
    "\n",
    "# Step 2: Configure the PyTorch Lightning module for training\n",
    "nb_epochs = 3\n",
    "nb_steps_per_epoch = len(train_loader)\n",
    "pl_camembert = NLI(\n",
    "    model=nli_camembert\n",
    ")\n",
    "\n",
    "# Step 3: Set up model checkpointing\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",           # Metric to monitor (validation loss)\n",
    "    dirpath=\"checkpoints/\",       # Directory for saving model checkpoints\n",
    "    filename=\"nli-{epoch:02d}-{val_loss:.2f}\",  # Format for checkpoint filenames\n",
    "    save_top_k=2,                 # Save only the top 2 models with the best validation loss\n",
    "    mode=\"min\"                    # Minimize the monitored metric\n",
    ")\n",
    "\n",
    "# Step 4: Set up TensorBoard logging\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"my_logs\",           # Directory for saving logs\n",
    "    name=\"final-experiment\"       # Name for the experiment\n",
    ")\n",
    "\n",
    "# Step 5: Initialize the PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,                 # Number of training epochs\n",
    "    accelerator=\"cpu\",            # Use GPU for training (if available)\n",
    "    devices=1,                    # Number of GPUs to use\n",
    "    callbacks=[checkpoint_callback],  # Add checkpointing callback\n",
    "    logger=logger                 # Use the configured TensorBoard logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1534"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnli_train_dataset = XNLIDataset(split=\"train\", language=\"fr\", cache_directory=data_path, max_length=64)\n",
    "xnli_val_dataset = XNLIDataset(split=\"validation\", language=\"fr\", cache_directory=data_path, max_length=64)\n",
    "\n",
    "train_loader = DataLoader(xnli_train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(xnli_val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(pl_camembert, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

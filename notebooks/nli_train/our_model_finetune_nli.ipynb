{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we focus on fine-tuning our custom model specifically for the Natural Language Inference (NLI) task. This involves training the model to classify logical relationships between pairs of sentences, such as entailment, contradiction, or neutrality. The goal is to evaluate how well our model, after pretraining, can adapt to this downstream task and compare its performance to existing benchmarks or similar models. This process will provide insights into the model's ability to capture semantic relationships and its overall effectiveness in real-world language understanding scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 15:48:03.432088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736347683.448483  756868 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736347683.453449  756868 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-08 15:48:03.472306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Suppress unnecessary warnings and set verbosity for Transformers\n",
    "import warnings\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# PyTorch core libraries\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# Transformers and Datasets\n",
    "from transformers import CamembertModel, CamembertTokenizer, CamembertConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# PyTorch Lightning and Metrics\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Visualization and DataFrame utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XNLIDataset(Dataset):\n",
    "    def __init__(self, cache_directory, split=\"train\", language=\"fr\", tokenizer=tokenizer, max_length=64):\n",
    "        \"\"\"\n",
    "        PyTorch-compatible dataset for the XNLI dataset.\n",
    "\n",
    "        Args:\n",
    "            split (str): Data split to load (\"train\", \"test\", or \"validation\").\n",
    "            language (str): Target language for the dataset.\n",
    "            cache_directory (str): Directory to cache the downloaded dataset.\n",
    "            max_length (int): Maximum sequence length for padding/truncation.\n",
    "        \"\"\"\n",
    "        super(XNLIDataset, self).__init__()\n",
    "        self.split = split\n",
    "        self.language = language\n",
    "        self.cache_directory = cache_directory\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Load the data and the tokenizer\n",
    "        self.data = load_dataset(\n",
    "            \"facebook/xnli\",\n",
    "            name=self.language,\n",
    "            cache_dir=self.cache_directory\n",
    "        )[self.split]  # Load the specified data split\n",
    "\n",
    "        self.tokenizer = tokenizer  # CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the size of the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a specific sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            dict: Contains `input_ids`, `attention_mask`, and `label`.\n",
    "        \"\"\"\n",
    "        example = self.data[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            example[\"premise\"],\n",
    "            example[\"hypothesis\"],\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Add the labels\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}  # Remove batch dimension\n",
    "        inputs[\"label\"] = torch.tensor(example[\"label\"], dtype=torch.long)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise : Et il a dit, maman, je suis à la maison.\n",
      "Hypothesis : Il a dit à sa mère qu'il était rentré.\n",
      "Label : 0 (entailment)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"facebook/xnli\", name='fr', cache_dir=\"Noureddine/MLA-CamemBERT/data/XNLI\")\n",
    "\n",
    "# Display some examples from the dataset\n",
    "print(f\"Premise : {dataset['validation'][2]['premise']}\")\n",
    "print(f\"Hypothesis : {dataset['validation'][2]['hypothesis']}\")\n",
    "print(f\"Label : {dataset['validation'][2]['label']} (entailment)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([256, 64])\n",
      "Token IDs (example):\n",
      "tensor([    5,   139,    51,    33,   227,     7,  2699,     7,    50,   146,\n",
      "           15,    13,   269,     9,     6,     6,    69,    33,   227,    15,\n",
      "           77,   907,    46,    11,    62,   149, 10540,     9,     6,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1]) \n",
      "\n",
      "Decoded text (example):\n",
      "<s> Et il a dit, maman, je suis à la maison.</s></s> Il a dit à sa mère qu'il était rentré.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/xnli\"\n",
    "\n",
    "xnli_train_dataset = XNLIDataset(split=\"train\", language=\"fr\", cache_directory=data_path, max_length=64)\n",
    "xnli_val_dataset = XNLIDataset(split=\"validation\", language=\"fr\", cache_directory=data_path, max_length=64)\n",
    "\n",
    "train_loader = DataLoader(xnli_train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(xnli_val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "print(f\"Batch shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Token IDs (example):\\n{batch['input_ids'][2]} \\n\")\n",
    "decoded_text = tokenizer.decode(batch['input_ids'][2])\n",
    "print(f\"Decoded text (example):\\n{decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class CamembertConfig:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 32005\n",
    "        self.hidden_size = 768\n",
    "        self.num_hidden_layers = 12\n",
    "        self.num_attention_heads = 12\n",
    "        self.intermediate_size = 3072\n",
    "        self.hidden_act = \"gelu\"\n",
    "        self.hidden_dropout_prob = 0.1\n",
    "        self.attention_probs_dropout_prob = 0.1\n",
    "        self.max_position_embeddings = 514\n",
    "        self.type_vocab_size = 1\n",
    "        self.initializer_range = 0.02\n",
    "        self.layer_norm_eps = 1e-5\n",
    "        self.pad_token_id = 1\n",
    "        self.head_type = \"MLM\"\n",
    "\n",
    "class CamembertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        token_type_embeds = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + position_embeds + token_type_embeds\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        # Debug prints\n",
    "        # print(f\"Embeddings NaN: {torch.isnan(embeddings).any()}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "class CamembertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.dropout = nn.Dropout(0.2)  # Increased dropout rate\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        return x.view(new_x_shape).permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        # Debug query, key, value\n",
    "        # print(f\"Query NaN: {torch.isnan(query_layer).any()}\")\n",
    "        # print(f\"Key NaN: {torch.isnan(key_layer).any()}\")\n",
    "        # print(f\"Value NaN: {torch.isnan(value_layer).any()}\")\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores /= math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # Clamp scores to prevent overflow\n",
    "        attention_scores = torch.clamp(attention_scores, min=-1e9, max=1e9)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1) + 1e-9\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Debug attention scores and probabilities\n",
    "        # print(f\"Attention Scores NaN Before Clamp: {torch.isnan(attention_scores).any()}\")\n",
    "        # print(f\"Attention Probs NaN: {torch.isnan(attention_probs).any()}\")\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        context_layer = context_layer.view(context_layer.size(0), -1, self.all_head_size)\n",
    "\n",
    "        # Debug context layer\n",
    "        # print(f\"Context Layer NaN: {torch.isnan(context_layer).any()}\")\n",
    "\n",
    "        return context_layer\n",
    "\n",
    "\n",
    "\n",
    "class CamembertFeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.activation = F.gelu if config.hidden_act == \"gelu\" else nn.ReLU()\n",
    "        self.dense_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(0.2)  # Increased dropout rate\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        intermediate_output = self.activation(self.dense_1(hidden_states))\n",
    "        intermediate_output = torch.clamp(intermediate_output, min=-1e9, max=1e9)\n",
    "\n",
    "        output = self.dense_2(intermediate_output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.LayerNorm(output + hidden_states)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class CamembertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = CamembertSelfAttention(config)\n",
    "        self.attention_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.feed_forward = CamembertFeedForward(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        hidden_states = self.attention_norm(hidden_states + attention_output)\n",
    "        return self.feed_forward(hidden_states)\n",
    "\n",
    "class CamembertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([CamembertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "\n",
    "            # Debug prints for each layer\n",
    "            # print(f\"Layer {i} Hidden States NaN: {torch.isnan(hidden_states).any()}\")\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "class CamembertLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = F.gelu(self.dense(hidden_states))\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        logits = self.decoder(hidden_states)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class CamembertModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = CamembertEmbeddings(config)\n",
    "        self.encoder = CamembertEncoder(config)\n",
    "        self.head = CamembertLMHead(config) if config.head_type == \"MLM\" else None\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded_input = self.embeddings(input_ids)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = (1.0 - attention_mask) * -float('inf')\n",
    "\n",
    "        encoder_output = self.encoder(embedded_input, attention_mask)\n",
    "        return self.head(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camembert_base(nn.Module) : \n",
    "    def __init__(self, embeddings, encoder):\n",
    "        super(Camembert_base , self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.encoder = encoder\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded_input = self.embeddings(input_ids)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = (1.0 - attention_mask) * -float('inf')\n",
    "\n",
    "        encoder_output = self.encoder(embedded_input, attention_mask)\n",
    "        return encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, we attach a classification head to enable the model to perform three-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NLIHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, dropout_prob=0.1):\n",
    "        super(NLIHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, 256)  # Couche fully connected\n",
    "        self.activation = nn.ReLU()  # Activation ReLU\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)  # Dropout pour la régularisation\n",
    "        self.fc2 = nn.Linear(256, num_labels)  # Couche finale pour les labels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # Première projection linéaire\n",
    "        x = self.activation(x)  # Activation\n",
    "        x = self.dropout(x)  # Application du Dropout\n",
    "        x = self.fc2(x)  # Projection finale vers les classes\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = CamembertConfig()\n",
    "loaded_model = CamembertModel(config)\n",
    "\n",
    "model_path = \"notebooks/trainings/models/Pretraining/model_checkpoints/checkpoint_epoch_9.pth\"\n",
    "checkpoint = torch.load(model_path)\n",
    "# Extract only the model's state_dict\n",
    "model_state_dict = checkpoint['model_state_dict']\n",
    "loaded_model.load_state_dict(model_state_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 3  # Exemples pour NLI : entailment, contradiction, neutral\n",
    "hidden_size = config.hidden_size  # Taille de sortie de l'encoder\n",
    "\n",
    "camembert = Camembert_base(loaded_model.embeddings, loaded_model.encoder)\n",
    "camembert.CamembertLMHead = NLIHead(hidden_size, num_labels)\n",
    "camembert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([8, 128, 32005])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generating example data\n",
    "batch_size = 8\n",
    "seq_len = 128\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "attention_mask = torch.ones(batch_size, seq_len)\n",
    "\n",
    "# Forward pass\n",
    "logits = loaded_model(input_ids, attention_mask)\n",
    "print(\"Logits shape:\", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camembert_base(nn.Module):\n",
    "    def __init__(self, embeddings, encoder, head):\n",
    "        super(Camembert_base, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.encoder = encoder\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Embedding layer\n",
    "        embedded_input = self.embeddings(input_ids)\n",
    "\n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = (1.0 - attention_mask) * -float('inf')\n",
    "\n",
    "        # Encoder layer\n",
    "        encoder_output = self.encoder(embedded_input, attention_mask)\n",
    "\n",
    "        # Head for classification\n",
    "        logits = self.head(encoder_output[:, 0, :])  # Utiliser uniquement le token [CLS] pour classification\n",
    "\n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return {\"logits\": logits, \"loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLI(pl.LightningModule):\n",
    "    def __init__(self, model, lr=5e-5):\n",
    "        \"\"\"\n",
    "        NLI model for training with PyTorch Lightning.\n",
    "        :param model: Instance of the fine-tuning model.\n",
    "        :param lr: Learning rate.\n",
    "        \"\"\"\n",
    "        super(NLI, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "        # Accuracy metrics for training and validation\n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "\n",
    "        # Metrics tracked per step\n",
    "        self.train_losses_step = []\n",
    "        self.train_accuracies_step = []\n",
    "        self.val_losses_step = []\n",
    "        self.val_accuracies_step = []\n",
    "\n",
    "        # Metrics tracked per epoch\n",
    "        self.train_losses_epoch = []\n",
    "        self.train_accuracies_epoch = []\n",
    "        self.val_losses_epoch = []\n",
    "        self.val_accuracies_epoch = []\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Forward pass for inference.\n",
    "        :param batch: Input batch containing input IDs, attention masks, and labels.\n",
    "        :return: Model logits.\n",
    "        \"\"\"\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self.model(input_ids, attention_mask, labels)\n",
    "        return outputs[\"logits\"]\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        \"\"\"\n",
    "        Performs a single training step.\n",
    "        :param batch: Input batch containing input IDs, attention masks, and labels.\n",
    "        :param batch_index: Index of the batch.\n",
    "        :return: Training loss.\n",
    "        \"\"\"\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        outputs = self.model(input_ids, attention_mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        # Compute accuracy\n",
    "        preds = torch.argmax(outputs[\"logits\"], dim=1)\n",
    "        acc = self.train_accuracy(preds, labels)\n",
    "\n",
    "        # Store step metrics\n",
    "        self.train_losses_step.append(loss.item())\n",
    "        self.train_accuracies_step.append(acc.item())\n",
    "\n",
    "        # Log metrics for progress bar\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=False)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True, on_step=True, on_epoch=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Computes and stores epoch-level training metrics at the end of each epoch.\n",
    "        \"\"\"\n",
    "        avg_loss = torch.tensor(self.train_losses_step).mean().item()\n",
    "        avg_acc = torch.tensor(self.train_accuracies_step).mean().item()\n",
    "\n",
    "        self.train_losses_epoch.append(avg_loss)\n",
    "        self.train_accuracies_epoch.append(avg_acc)\n",
    "\n",
    "        # Display epoch results\n",
    "        print(f\"[Epoch {self.current_epoch}] Train Loss: {avg_loss:.4f}, Train Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "        # Clear step metrics to prepare for the next epoch\n",
    "        self.train_losses_step.clear()\n",
    "        self.train_accuracies_step.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        \"\"\"\n",
    "        Performs a single validation step.\n",
    "        :param batch: Input batch containing input IDs, attention masks, and labels.\n",
    "        :param batch_index: Index of the batch.\n",
    "        :return: Validation loss.\n",
    "        \"\"\"\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        outputs = self.model(input_ids, attention_mask, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        # Compute accuracy\n",
    "        preds = torch.argmax(outputs[\"logits\"], dim=1)\n",
    "        acc = self.val_accuracy(preds, labels)\n",
    "\n",
    "        # Store step metrics\n",
    "        self.val_losses_step.append(loss.item())\n",
    "        self.val_accuracies_step.append(acc.item())\n",
    "\n",
    "        # Log metrics for progress bar\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Computes and stores epoch-level validation metrics at the end of each epoch.\n",
    "        \"\"\"\n",
    "        avg_loss = torch.tensor(self.val_losses_step).mean().item()\n",
    "        avg_acc = torch.tensor(self.val_accuracies_step).mean().item()\n",
    "\n",
    "        self.val_losses_epoch.append(avg_loss)\n",
    "        self.val_accuracies_epoch.append(avg_acc)\n",
    "\n",
    "        # Display epoch results\n",
    "        print(f\"[Epoch {self.current_epoch}] Val Loss: {avg_loss:.4f}, Val Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "        # Clear step metrics to prepare for the next epoch\n",
    "        self.val_losses_step.clear()\n",
    "        self.val_accuracies_step.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures the optimizer and learning rate scheduler.\n",
    "        :return: Dictionary containing the optimizer and scheduler configurations.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "        # Dynamically calculate the total number of steps\n",
    "        steps_per_epoch = 1534\n",
    "        total_steps = steps_per_epoch * self.trainer.max_epochs\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.lr,\n",
    "            total_steps=total_steps,\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy=\"linear\",\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1534"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data/xnli\"\n",
    "\n",
    "xnli_train_dataset = XNLIDataset(split=\"train\", language=\"fr\", cache_directory=data_path, max_length=64)\n",
    "xnli_val_dataset = XNLIDataset(split=\"validation\", language=\"fr\", cache_directory=data_path, max_length=64)\n",
    "\n",
    "train_loader = DataLoader(xnli_train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(xnli_val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 3  # Exemples pour NLI : entailment, contradiction, neutral\n",
    "hidden_size = config.hidden_size  # Taille de sortie de l'encoder\n",
    "\n",
    "camembert = Camembert_base(loaded_model.embeddings, loaded_model.encoder, NLIHead(hidden_size, num_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "pl_camembert = NLI(model=camembert)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"gpu\",  # Utilise GPU\n",
    "    devices=1,  # Utilise un seul GPU\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            dirpath=\"../checkpoints/\",\n",
    "            filename=\"nli-{epoch:02d}-{val_loss:.2f}\",\n",
    "            save_top_k=2,\n",
    "            mode=\"min\",\n",
    "        )\n",
    "    ],\n",
    "    logger=TensorBoardLogger(\"logs/\", name=\"nli_experiment\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type               | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model          | Camembert_base     | 103 M  | train\n",
      "1 | train_accuracy | MulticlassAccuracy | 0      | train\n",
      "2 | val_accuracy   | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------------\n",
      "103 M     Trainable params\n",
      "0         Non-trainable params\n",
      "103 M     Total params\n",
      "412.568   Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "152       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c3143e15054751b7c5fd2983347ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(pl_camembert, train_loader, val_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

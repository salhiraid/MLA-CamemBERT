{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XNLI Dataset Exploration\n",
    "\n",
    "This notebook demonstrates how to download and explore the XNLI dataset using the `datasets` library from Hugging Face. We will also decode the `input_ids` to see the corresponding tokens.\n",
    "\n",
    "## Install Required Libraries\n",
    "\n",
    "First, we need to install the `datasets` and `transformers` libraries if they are not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 392702\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 5010\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 2490\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "language = 'fr'\n",
    "cache_directory = \"../../data/xnli\"\n",
    "# Download the XNLI dataset for the specified language\n",
    "dataset = load_dataset(\"facebook/xnli\", name=language, cache_dir=cache_directory)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration\n",
    "\n",
    "Let's look at some examples from the dataset to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': \"L' écrémage conceptuel de la crème a deux dimensions fondamentales : le produit et la géographie .\", 'hypothesis': 'Le produit et la géographie sont ce qui fait travailler la crème de la crème .', 'label': 1}\n",
      "{'premise': 'Et il a dit, maman, je suis à la maison.', 'hypothesis': \"Il a appelé sa mère dès que le bus scolaire l'a déposé.\", 'label': 1}\n",
      "{'premise': \"Eh bien, je ne pensais même pas à cela, mais j'étais si frustré, et j'ai fini par lui reparler.\", 'hypothesis': 'Je ne lui ai pas parlé de nouveau', 'label': 2}\n"
     ]
    }
   ],
   "source": [
    "# Display some examples from the dataset\n",
    "print(dataset['train'][0])\n",
    "print(dataset['validation'][0])\n",
    "print(dataset['test'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding `input_ids`\n",
    "\n",
    "Now we will decode the `input_ids` to see the corresponding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([    5, 17526,  2856,   674,  1017, 21598,     9, 22625,     9,   616,\n",
      "         6974,    86,    81,     6])\n",
      "Tokens: ['<s>', '▁This', '▁is', '▁an', '▁ex', 'ample', '.', '▁sentence', '.', '▁h', 'ello', '▁si', 'r', '</s>']\n",
      "Decoded sentence: This is an example. sentence. hello sir\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Example of input_ids\n",
    "inputs = tokenizer(\"This is an example. sentence. \\t hello sir\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "\n",
    "# Decode the input_ids to see the tokens\n",
    "decoded_sentece = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Decoded sentence:\", decoded_sentece)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have downloaded and explored the XNLI dataset, and we have also decoded the `input_ids` to see the corresponding tokens. You can now use this dataset for your natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392702"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   5,   54, 1918,   30, 1549,    6,    6,   54, 1918,   30,    8,  648,\n",
      "         5251,    6,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer\n",
    "\n",
    "# Charger le tokenizer CamemBERT\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Exemple : préparer une prémisse et une hypothèse\n",
    "premise = \"Le ciel est bleu\"\n",
    "hypothesis = \"Le ciel est de couleur bleue\"\n",
    "\n",
    "# Concaténer et tokeniser\n",
    "inputs = tokenizer(\n",
    "    premise, hypothesis,\n",
    "    max_length=128,        # Limite de la séquence (peut être ajustée)\n",
    "    truncation=True,       # Tronquer si la séquence est trop longue\n",
    "    padding=\"max_length\",  # Compléter à la longueur max (pour les batchs)\n",
    "    return_tensors=\"pt\"    # Retourner des tenseurs PyTorch\n",
    ")\n",
    "\n",
    "# Résultat\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁Le',\n",
       " '▁ciel',\n",
       " '▁est',\n",
       " '▁bleu',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '▁Le',\n",
       " '▁ciel',\n",
       " '▁est',\n",
       " '▁de',\n",
       " '▁couleur',\n",
       " '▁bleue',\n",
       " '</s>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s> 6 <s> <mask> </s> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.sep_token, tokenizer.sep_token_id, tokenizer.cls_token, tokenizer.mask_token, tokenizer.sep_token, tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from datasets import load_dataset\n",
    "# from transformers import CamembertTokenizer\n",
    "\n",
    "# class XNLIDataset(Dataset):\n",
    "#     def __init__(self, split=\"train\"):\n",
    "#         super(XNLIDataset, self).__init__()\n",
    "\n",
    "#         self.split = split\n",
    "#         self.language = \"fr\"\n",
    "#         self.cache_directory = \"../data/xnli\"\n",
    "#         self.data = load_dataset(\n",
    "#             \"facebook/xnli\",\n",
    "#             name=self.language,\n",
    "#             cache_dir=self.cache_directory\n",
    "#         )\n",
    "#         self.tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "#         self.max_length = 128\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data[self.split])\n",
    "\n",
    "#     def __getitem__(self, idx) -> dict :\n",
    "#         if self.split == \"train\":\n",
    "\n",
    "#             inputs = self.tokenizer(\n",
    "#                 self.data(self.split)[idx][\"premise\"],\n",
    "#                 self.data(self.split)[idx][\"hypothesis\"],\n",
    "#                 max_length=self.max_length,\n",
    "#                 truncation=True,\n",
    "#                 padding=\"max_length\",\n",
    "#                 return_tensors=\"pt\"\n",
    "#             )\n",
    "#             return inputs\n",
    "            \n",
    "        \n",
    "#         elif self.split == \"test\":\n",
    "#             inputs = self.tokenizer(\n",
    "#                 self.data(self.split)[idx][\"premise\"],\n",
    "#                 self.data(self.split)[idx][\"hypothesis\"],\n",
    "#                 max_length=self.max_length,\n",
    "#                 truncation=True,\n",
    "#                 padding=\"max_length\",\n",
    "#                 return_tensors=\"pt\"\n",
    "#             )\n",
    "#             return inputs\n",
    "#         elif self.split == \"validation\":\n",
    "#             inputs = self.tokenizer(\n",
    "#                 self.data(self.split)[idx][\"premise\"],\n",
    "#                 self.data(self.split)[idx][\"hypothesis\"],\n",
    "#                 max_length=self.max_length,\n",
    "#                 truncation=True,\n",
    "#                 padding=\"max_length\",\n",
    "#                 return_tensors=\"pt\"\n",
    "#             )\n",
    "#             return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import CamembertTokenizer\n",
    "\n",
    "\n",
    "class XNLIDataset(Dataset):\n",
    "    def __init__(self, split=\"train\", language=\"fr\", tokenizer=tokenizer, cache_directory=\"../data/xnli\", max_length=128):\n",
    "        \"\"\"\n",
    "        Dataset PyTorch pour le dataset XNLI.\n",
    "\n",
    "        Args:\n",
    "            split (str): Partition des données (\"train\", \"test\", \"validation\").\n",
    "            language (str): Langue cible.\n",
    "            cache_directory (str): Répertoire pour stocker le dataset téléchargé.\n",
    "            max_length (int): Longueur maximale pour le padding/truncation.\n",
    "        \"\"\"\n",
    "        super(XNLIDataset, self).__init__()\n",
    "        self.split = split\n",
    "        self.language = language\n",
    "        self.cache_directory = cache_directory\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Charger les données et le tokenizer\n",
    "        self.data = load_dataset(\n",
    "            \"facebook/xnli\",\n",
    "            name=self.language,\n",
    "            cache_dir=self.cache_directory\n",
    "        )[self.split]  # Charger uniquement la partition demandée\n",
    "\n",
    "        self.tokenizer = tokenizer #CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Retourne la taille du dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Récupère un échantillon spécifique.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index de l'échantillon.\n",
    "\n",
    "        Returns:\n",
    "            dict: Contient les `input_ids`, `attention_mask` et `label`.\n",
    "        \"\"\"\n",
    "        example = self.data[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            example[\"premise\"],\n",
    "            example[\"hypothesis\"],\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Ajouter les labels\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}  # Enlever la dimension batch\n",
    "        inputs[\"label\"] = torch.tensor(example[\"label\"], dtype=torch.long)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli = XNLIDataset(split=\"train\")\n",
    "\n",
    "data_loader = DataLoader(xnli, batch_size=8, shuffle=True)\n",
    "batch = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([8, 128])\n",
      "attention_mask: torch.Size([8, 128])\n",
      "label: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for key, val in batch.items():\n",
    "    print(f\"{key}: {val.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['SNLI'] = (\n",
    "    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
    "    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
    "\n",
    "data_dir = d2l.download_extract('SNLI')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CamemBERT Base :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertModel, CamembertConfig\n",
    "\n",
    "model_path = \"../../models/4gb_oscar\"\n",
    "camembert = CamembertModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CamembertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "CamembertModel                                               --\n",
       "├─CamembertEmbeddings: 1-1                                   --\n",
       "│    └─Embedding: 2-1                                        24,579,840\n",
       "│    └─Embedding: 2-2                                        394,752\n",
       "│    └─Embedding: 2-3                                        768\n",
       "│    └─LayerNorm: 2-4                                        1,536\n",
       "│    └─Dropout: 2-5                                          --\n",
       "├─CamembertEncoder: 1-2                                      --\n",
       "│    └─ModuleList: 2-6                                       --\n",
       "│    │    └─CamembertLayer: 3-1                              7,087,872\n",
       "│    │    └─CamembertLayer: 3-2                              7,087,872\n",
       "│    │    └─CamembertLayer: 3-3                              7,087,872\n",
       "│    │    └─CamembertLayer: 3-4                              7,087,872\n",
       "│    │    └─CamembertLayer: 3-5                              7,087,872\n",
       "│    │    └─CamembertLayer: 3-6                              7,087,872\n",
       "│    │    └─CamembertLayer: 3-7                              7,087,872\n",
       "│    │    └─CamembertLayer: 3-8                              7,087,872\n",
       "│    │    └─CamembertLayer: 3-9                              7,087,872\n",
       "│    │    └─CamembertLayer: 3-10                             7,087,872\n",
       "│    │    └─CamembertLayer: 3-11                             7,087,872\n",
       "│    │    └─CamembertLayer: 3-12                             7,087,872\n",
       "├─CamembertPooler: 1-3                                       --\n",
       "│    └─Linear: 2-7                                           590,592\n",
       "│    └─Tanh: 2-8                                             --\n",
       "=====================================================================================\n",
       "Total params: 110,621,952\n",
       "Trainable params: 110,621,952\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = CamembertConfig()\n",
    "print(config)\n",
    "from torchinfo import summary\n",
    "summary(camembert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CamembertModel(\n",
      "  (embeddings): CamembertEmbeddings(\n",
      "    (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): CamembertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x CamembertLayer(\n",
      "        (attention): CamembertAttention(\n",
      "          (self): CamembertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): CamembertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): CamembertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): CamembertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): CamembertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(camembert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import CamembertModel, CamembertTokenizer, CamembertConfig\n",
    "from transformers import CamembertTokenizer\n",
    "\n",
    "\n",
    "class CamemBERTBaseModel(nn.Module):\n",
    "    def __init__(self, model_path: str, trainable: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the base CamemBERT model.\n",
    "        :param model_path: Path to the pre-trained CamemBERT model.\n",
    "        \"\"\"\n",
    "        super(CamemBERTBaseModel, self).__init__()\n",
    "        self.base_model = CamembertModel.from_pretrained(model_path)\n",
    "        self.tranaible = trainable\n",
    "        self.config = CamembertConfig()\n",
    "        #self.config = CamembertModel.from_pretrained(model_path).config\n",
    "\n",
    "        if not trainable:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.base_model.eval()\n",
    "        else :\n",
    "            self.base_model.train()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the base model.\n",
    "        :param input_ids: Tensor of token IDs.\n",
    "        :param attention_mask: Tensor of attention masks.\n",
    "        :return: Last hidden states from the base model.\n",
    "        \"\"\"\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "    def get_hidden_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the hidden size of the base model for dynamically attaching heads.\n",
    "        :return: Hidden size of the CamemBERT model.\n",
    "        \"\"\"\n",
    "        return self.config.hidden_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens : ['▁Ceci', '▁est', '<s>', '▁un', '▁exemple', '▁de', '<s>', '▁phrase', '▁pour', '▁démontrer', '▁le', '▁fonctionnement', '▁de', '▁Cam', 'em', 'BERT', '.', '▁Je', '▁suis', '▁content']\n",
      "Token IDs : [5, 2978, 30, 5, 23, 411, 8, 5, 3572, 24, 8310, 16, 1625, 8, 5628, 1868, 20703, 9, 100, 146, 2945, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# exemple pour tester le base-model :\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "base_model = CamemBERTBaseModel(model_path=model_path, trainable=False)\n",
    "\n",
    "sentence = \"Ceci est <s> un exemple de <s> phrase pour démontrer le fonctionnement de CamemBERT. Je suis content\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Afficher les tokens et leurs IDs\n",
    "print(\"Tokens :\", tokens)\n",
    "print(\"Token IDs :\", inputs[\"input_ids\"].squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape : torch.Size([1, 128, 768])\n",
      "Embedding du premier token (CLS) : torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Passage dans le modèle\n",
    "with torch.no_grad():\n",
    "    embeddings = base_model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "# Afficher la forme des embeddings\n",
    "print(\"Embeddings shape :\", embeddings.shape)\n",
    "\n",
    "# Exemple d'accès à un vecteur spécifique (exemple : le premier token)\n",
    "print(\"Embedding du premier token (CLS) :\", embeddings[:, 0, :].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The NLI HEAD : (just for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "\n",
    "class NLIHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_labels: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the NLI head.\n",
    "        :param hidden_size: Hidden size of the base model's output (e.g., 768 for CamemBERT).\n",
    "        :param num_labels: Number of labels for NLI (default: 3 - coherent, neutral, contradictory).\n",
    "        \"\"\"\n",
    "        super(NLIHead, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.classifier = nn.Linear(self.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, cls_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the NLI head.\n",
    "        :param cls_output: Tensor containing the [CLS] token representation (batch_size, hidden_size).\n",
    "        :return: Logits for each class (batch_size, num_labels).\n",
    "        \"\"\"\n",
    "        return self.classifier(cls_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Initialiser le NLI Head\n",
    "hidden_size = base_model.get_hidden_size()  # Par exemple, 768\n",
    "nli_head = NLIHead(hidden_size=hidden_size, num_labels=3)\n",
    "\n",
    "# Exemple de phrase d'entrée\n",
    "premise = \"Ceci est une phrase pour tester la logique.\"\n",
    "hypothesis = \"C'est un test pour valider une hypothèse.\"\n",
    "\n",
    "# Tokenisation\n",
    "inputs = tokenizer(\n",
    "    premise, hypothesis,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passage dans le modèle de base\n",
    "with torch.no_grad():\n",
    "    embeddings = base_model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "# Récupération du vecteur [CLS] (premier token)\n",
    "cls_output = embeddings[:, 0, :]  # (batch_size, hidden_size)\n",
    "cls_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4450, -0.0039,  0.1467]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4201, 0.2682, 0.3117]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Passage dans le NLI Head\n",
    "logits = nli_head(cls_output)\n",
    "print(logits)\n",
    "probs = softmax(logits, dim=-1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(probs).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits : tensor([[ 0.4450, -0.0039,  0.1467]], grad_fn=<AddmmBackward0>)\n",
      "Probabilités : tensor([[0.4201, 0.2682, 0.3117]], grad_fn=<SoftmaxBackward0>)\n",
      "Label prédit : cohérent\n"
     ]
    }
   ],
   "source": [
    "# Passage dans le NLI Head\n",
    "logits = nli_head(cls_output)\n",
    "\n",
    "# Appliquer une fonction softmax pour obtenir des probabilités\n",
    "\n",
    "# Résultat\n",
    "labels = [\"cohérent\", \"neutre\", \"contradictoire\"]\n",
    "predicted_label = labels[torch.argmax(probs).item()]\n",
    "\n",
    "# Afficher les résultats\n",
    "print(f\"Logits : {logits}\")\n",
    "print(f\"Probabilités : {probs}\")\n",
    "print(f\"Label prédit : {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modele NLI complet :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = inputs[\"input_ids\"]\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on fait ca car on va vouloir changer le modèle a un moment donné et donc pour ne pas refaire le code depuis le début on fait ça"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIFinetuningModel(nn.Module):\n",
    "    def __init__(self, base_model: CamemBERTBaseModel, num_labels: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the NLI fine-tuning model.\n",
    "        :param base_model: Instance of the base CamemBERT model.\n",
    "        :param num_labels: Number of labels for NLI.\n",
    "        \"\"\"\n",
    "        super(NLIFinetuningModel, self).__init__()\n",
    "        self.base_model = base_model \n",
    "        self.nli_head = NLIHead(base_model.get_hidden_size(), num_labels)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Forward pass for NLI fine-tuning.\n",
    "        :param input_ids: Tensor of token IDs.\n",
    "        :param attention_mask: Tensor of attention masks.\n",
    "        :param labels: Optional tensor of labels (batch_size).\n",
    "        :return: Dictionary containing logits and optionally loss.\n",
    "        \"\"\"\n",
    "        # Get last hidden states from the base model\n",
    "        hidden_states = self.base_model(input_ids=input_ids, attention_mask=attention_mask) # (batch_size, seq_len, hidden_size) -> (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Extract the [CLS] token's representation\n",
    "        cls_output = hidden_states[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        # Pass through the NLI head\n",
    "        logits = self.nli_head(cls_output)  # Shape: (batch_size, num_labels)\n",
    "\n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return {\"logits\": logits, \"loss\": loss}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 0.1743,  0.1455,  0.1853],\n",
      "        [-0.0622, -0.2431,  0.1681]], grad_fn=<AddmmBackward0>)\n",
      "Loss: tensor(0.9959, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle CamemBERT pré-entraîné\n",
    "base_model = CamemBERTBaseModel(model_path=model_path, trainable=True)\n",
    "# Créer une instance de NLIFinetuningModel\n",
    "model = NLIFinetuningModel(base_model=base_model, num_labels=3)\n",
    "\n",
    "# Exemple de données d'entrée (batch_size=2)\n",
    "input_ids = torch.tensor([[1, 23, 45, 2], [1, 67, 89, 2]])  # Token IDs\n",
    "attention_mask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 1]])  # Attention mask\n",
    "labels = torch.tensor([0, 2])  # Labels (coherent=0, contradictory=2)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "print(\"Logits:\", output[\"logits\"])\n",
    "print(\"Loss:\", output[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test with real data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import CamembertTokenizer\n",
    "\n",
    "\n",
    "class XNLIDataset(Dataset):\n",
    "    def __init__(self, split=\"train\", language=\"fr\", tokenizer=tokenizer, cache_directory=\"../data/xnli\", max_length=128):\n",
    "        \"\"\"\n",
    "        Dataset PyTorch pour le dataset XNLI.\n",
    "\n",
    "        Args:\n",
    "            split (str): Partition des données (\"train\", \"test\", \"validation\").\n",
    "            language (str): Langue cible.\n",
    "            cache_directory (str): Répertoire pour stocker le dataset téléchargé.\n",
    "            max_length (int): Longueur maximale pour le padding/truncation.\n",
    "        \"\"\"\n",
    "        super(XNLIDataset, self).__init__()\n",
    "        self.split = split\n",
    "        self.language = language\n",
    "        self.cache_directory = cache_directory\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Charger les données et le tokenizer\n",
    "        self.data = load_dataset(\n",
    "            \"facebook/xnli\",\n",
    "            name=self.language,\n",
    "            cache_dir=self.cache_directory\n",
    "        )[self.split]  # Charger uniquement la partition demandée\n",
    "\n",
    "        self.tokenizer = tokenizer #CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Retourne la taille du dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Récupère un échantillon spécifique.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index de l'échantillon.\n",
    "\n",
    "        Returns:\n",
    "            dict: Contient les `input_ids`, `attention_mask` et `label`.\n",
    "        \"\"\"\n",
    "        example = self.data[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            example[\"premise\"],\n",
    "            example[\"hypothesis\"],\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Ajouter les labels\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}  # Enlever la dimension batch\n",
    "        inputs[\"label\"] = torch.tensor(example[\"label\"], dtype=torch.long)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "cache_directory =\"../data/xnli\"\n",
    "xnli = XNLIDataset(split=\"train\", cache_directory=cache_directory, max_length=256)\n",
    "\n",
    "data_loader = DataLoader(xnli, batch_size=16, shuffle=True)\n",
    "batch = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "tensor([0, 0, 1, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 2, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "print(batch['input_ids'].shape)\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['label'])\n",
    "\n",
    "input_ids, attention_mask, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_model = NLIFinetuningModel(base_model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0578067302703857"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = nli_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "loss = output[\"loss\"]\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 0.2395, -0.1110, -0.1766],\n",
      "        [ 0.3280, -0.0443, -0.1854],\n",
      "        [ 0.2464, -0.0946, -0.1660],\n",
      "        [ 0.3357, -0.0239, -0.0630],\n",
      "        [ 0.3458, -0.0148, -0.1879],\n",
      "        [ 0.2138, -0.0526, -0.1945],\n",
      "        [ 0.2906, -0.0869, -0.1904],\n",
      "        [ 0.2087, -0.0788, -0.2027],\n",
      "        [ 0.2006, -0.1566, -0.1919],\n",
      "        [ 0.2682, -0.0122, -0.2245],\n",
      "        [ 0.3817, -0.0137, -0.2667],\n",
      "        [ 0.2620, -0.1228, -0.1829],\n",
      "        [ 0.2008, -0.1305, -0.2211],\n",
      "        [ 0.2664, -0.0700, -0.1728],\n",
      "        [ 0.4032, -0.1006, -0.1706],\n",
      "        [ 0.2362, -0.1486, -0.1505]], grad_fn=<AddmmBackward0>)\n",
      "Loss: tensor(1.0649, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = nli_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "print(\"Logits:\", output[\"logits\"])\n",
    "print(\"Loss:\", output[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the model :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Prepare data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import CamembertTokenizer\n",
    "\n",
    "# Instancier le tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Charger vos données\n",
    "train_dataset = XNLIDataset(split=\"train\", language=\"fr\", tokenizer=tokenizer, cache_directory=\"../data/xnli\", max_length=128)\n",
    "val_dataset = XNLIDataset(split=\"validation\", language=\"fr\", tokenizer=tokenizer, cache_directory=\"../data/xnli\", max_length=128)\n",
    "\n",
    "# Créer les DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Configure data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle CamemBERT\n",
    "base_model = CamemBERTBaseModel(model_path=\"../../models/4gb_oscar\", trainable=True)\n",
    "\n",
    "# Initialiser le modèle complet avec la tête NLI\n",
    "model = NLIFinetuningModel(base_model=base_model, num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Optimizer and LossFunction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Optimiseur\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Fonction de perte\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Training configuration :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cude\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# def train_for_one_epoch():\n",
    "#     model.train() # activant le dropout et la normalisation par exemple \n",
    "#     running_loss = 0.0\n",
    "#     for batch in train_loader:\n",
    "#         input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"label\"].to(device)\n",
    "#         #optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss, logits = output[\"loss\"], output[\"logits\"]\n",
    "#         # Backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() # loss.item() retourne la valeur de la loss\n",
    "\n",
    "#     return running_loss / len(train_loader) # we devide by the number of batchs to get the average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for batch in tqdm(loader, desc=\"Training\"):\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         labels = batch[\"label\"].to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs[\"loss\"]\n",
    "\n",
    "#         # Backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     return running_loss / len(loader)\n",
    "\n",
    "\n",
    "# def evaluate(model, loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     running_loss = 0.0\n",
    "#     correct_predictions = 0\n",
    "#     total_predictions = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(loader, desc=\"Validation\"):\n",
    "#             input_ids = batch[\"input_ids\"].to(device)\n",
    "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#             labels = batch[\"label\"].to(device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#             loss = outputs[\"loss\"]\n",
    "#             logits = outputs[\"logits\"]\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             # Prédictions\n",
    "#             predictions = torch.argmax(logits, dim=-1)\n",
    "#             correct_predictions += (predictions == labels).sum().item()\n",
    "#             total_predictions += labels.size(0)\n",
    "\n",
    "#     accuracy = correct_predictions / total_predictions\n",
    "#     return running_loss / len(loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_epochs = 2\n",
    "\n",
    "# for epoch in tqdm(range(nb_epochs)):\n",
    "#     print(f\"Epoch {epoch + 1}/{nb_epochs}\")\n",
    "\n",
    "#     # Entraînement\n",
    "#     train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "#     print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "#     # Validation\n",
    "#     val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n",
    "#     print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    num_epochs=2,\n",
    "    log_interval=1000,\n",
    "    scheduler=None,\n",
    "    save_best_model=True,\n",
    "    checkpoint_path=\"best_model.pth\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Fonction professionnelle pour entraîner un modèle avec suivi des métriques et validation périodique.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Le modèle à entraîner.\n",
    "        train_loader (DataLoader): DataLoader pour les données d'entraînement.\n",
    "        val_loader (DataLoader): DataLoader pour les données de validation.\n",
    "        optimizer (Optimizer): Optimiseur à utiliser.\n",
    "        criterion (Loss): Fonction de perte.\n",
    "        device (torch.device): Appareil d'entraînement (CPU ou GPU).\n",
    "        num_epochs (int): Nombre d'époques d'entraînement.\n",
    "        log_interval (int): Fréquence d'affichage des logs pendant l'entraînement.\n",
    "        scheduler (torch.optim.lr_scheduler, optional): Scheduler pour ajuster le taux d'apprentissage.\n",
    "        save_best_model (bool): Sauvegarder le meilleur modèle basé sur la performance de validation.\n",
    "        checkpoint_path (str): Chemin pour sauvegarder le modèle.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contient les historiques de perte et d'exactitude.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_accuracy\": [],\n",
    "    }\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n=== Epoch {epoch + 1}/{num_epochs} ===\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Entraînement\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=\"Training\"), 1):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[\"loss\"]\n",
    "\n",
    "            # Backward pass\n",
    "            # optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights :\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Affichage périodique\n",
    "            if step % log_interval == 0 or step == len(train_loader):\n",
    "                avg_loss = running_loss / step\n",
    "                print(f\"Step {step}/{len(train_loader)} - Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs[\"loss\"]\n",
    "                logits = outputs[\"logits\"]\n",
    "\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "                # Prédictions\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                correct_predictions += (predictions == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "\n",
    "\n",
    "        avg_val_loss = val_running_loss / len(val_loader)\n",
    "        val_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "        history[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Scheduler step (si utilisé)\n",
    "        if scheduler:\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Sauvegarde du meilleur modèle\n",
    "        if save_best_model and avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model saved to {checkpoint_path}\")\n",
    "\n",
    "        # Afficher le temps d'exécution pour l'époque\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f} seconds\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CamembertForMaskedLM, CamembertTokenizer, TrainingArguments, AdamW, Trainer , CamembertConfig\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_from_disk\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OscarDataset\n\u001b[0;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m CamembertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamembert-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# === Model === #\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#config = CamembertConfig(\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#    vocab_size=tokenizer.vocab_size)  # Adjust to match your tokenizer's vocab size\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#     initializer_range=0.02           # Standard deviation for weight initialization\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    " # model summary\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CamembertForMaskedLM, CamembertTokenizer, TrainingArguments, AdamW, Trainer , CamembertConfig\n",
    "from datasets import load_from_disk\n",
    "from dataset import OscarDataset\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "# === Model === #\n",
    "#config = CamembertConfig(\n",
    " #    vocab_size=tokenizer.vocab_size)  # Adjust to match your tokenizer's vocab size\n",
    "#     hidden_size=768,                 # Hidden size (RoBERTa_BASE)\n",
    "#     num_hidden_layers=12,            # Number of transformer layers\n",
    "#     num_attention_heads=12,          # Number of attention heads\n",
    "#     intermediate_size=3072,          # FFN inner hidden size\n",
    "#     hidden_dropout_prob=0.1,         # Dropout probability\n",
    "#     attention_probs_dropout_prob=0.1, # Attention dropout probability\n",
    "#     max_position_embeddings=514,     # Maximum sequence length + special tokens\n",
    "#     type_vocab_size=1,               # No token type embeddings\n",
    "#     initializer_range=0.02           # Standard deviation for weight initialization\n",
    "# )\n",
    "\n",
    "config = CamembertConfig()\n",
    "print(config)\n",
    "# Initialize a randomly weighted CamembertForMaskedLM model\n",
    "model = CamembertForMaskedLM(config) \n",
    "#model.to(\"cuda\")\n",
    "\n",
    "print(\"Model initialized\")\n",
    "print(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "LocalTokenNotFoundError",
     "evalue": "Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocalTokenNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m whoami\n\u001b[1;32m----> 2\u001b[0m user_info \u001b[38;5;241m=\u001b[39m \u001b[43mwhoami\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVous êtes connecté en tant que : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\huggingface_hub\\hf_api.py:1663\u001b[0m, in \u001b[0;36mHfApi.whoami\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m   1649\u001b[0m \u001b[38;5;129m@validate_hf_hub_args\u001b[39m\n\u001b[0;32m   1650\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhoami\u001b[39m(\u001b[38;5;28mself\u001b[39m, token: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;124;03m    Call HF API to know \"whoami\".\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;124;03m            To disable authentication, pass `False`.\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1661\u001b[0m     r \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   1662\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/whoami-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m-> 1663\u001b[0m         headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_hf_headers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# If `token` is provided and not `None`, it will be used by default.\u001b[39;49;00m\n\u001b[0;32m   1665\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Otherwise, the token must be retrieved from cache or env variable.\u001b[39;49;00m\n\u001b[0;32m   1666\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1668\u001b[0m     )\n\u001b[0;32m   1669\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1670\u001b[0m         hf_raise_for_status(r)\n",
      "File \u001b[1;32mc:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\huggingface_hub\\hf_api.py:9484\u001b[0m, in \u001b[0;36mHfApi._build_hf_headers\u001b[1;34m(self, token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[0;32m   9481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   9482\u001b[0m     \u001b[38;5;66;03m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[0;32m   9483\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[1;32m-> 9484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_hf_headers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_write_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_write_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9489\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\huggingface_hub\\utils\\_headers.py:124\u001b[0m, in \u001b[0;36mbuild_hf_headers\u001b[1;34m(token, is_write_action, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mBuild headers dictionary to send in a HF Hub call.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m        If `token=True` but token is not saved locally.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Get auth token to send\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m token_to_send \u001b[38;5;241m=\u001b[39m \u001b[43mget_token_to_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m _validate_token_to_send(token_to_send, is_write_action\u001b[38;5;241m=\u001b[39mis_write_action)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Combine headers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\huggingface_hub\\utils\\_headers.py:158\u001b[0m, in \u001b[0;36mget_token_to_send\u001b[1;34m(token)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cached_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LocalTokenNotFoundError(\n\u001b[0;32m    159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken is required (`token=True`), but no token found. You\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m need to provide a token or be logged in to Hugging Face with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `huggingface-cli login` or `huggingface_hub.login`. See\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/settings/tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         )\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_token\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Case implicit use of the token is forbidden by env variable\u001b[39;00m\n",
      "\u001b[1;31mLocalTokenNotFoundError\u001b[0m: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "user_info = whoami()\n",
    "print(f\"Vous êtes connecté en tant que : {user_info['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    CamembertTokenizer,\n",
    "    CamembertForSequenceClassification,\n",
    "    CamembertForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "class CamemBERTFineTuner:\n",
    "    def __init__(self, model_path, num_labels, task=\"classification\"):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = CamembertTokenizer.from_pretrained(model_path)\n",
    "        self.task = task\n",
    "        \n",
    "        # Charger le modèle adapté à la tâche\n",
    "        if task == \"classification\":  # e.g., NLI\n",
    "            self.model = CamembertForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)\n",
    "        elif task == \"token_classification\":  # e.g., NER\n",
    "            self.model = CamembertForTokenClassification.from_pretrained(model_path, num_labels=num_labels)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported task. Use 'classification' or 'token_classification'.\")\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        \"\"\"Tokenize input data\"\"\"\n",
    "        return self.tokenizer(\n",
    "            examples[\"text\"],  # Assurez-vous que 'text' correspond à votre dataset\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "    \n",
    "    def load_and_preprocess_data(self, dataset_name, split, text_column, label_column):\n",
    "        \"\"\"Charger et préparer les données\"\"\"\n",
    "        dataset = load_dataset(dataset_name, split=split)\n",
    "        dataset = dataset.map(\n",
    "            self.tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[text_column]\n",
    "        )\n",
    "        dataset = dataset.rename_column(label_column, \"labels\")\n",
    "        dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "        return dataset\n",
    "\n",
    "    def train(self, train_dataset, eval_dataset, output_dir, batch_size=8, epochs=3, lr=5e-5):\n",
    "        \"\"\"Fine-tuner le modèle\"\"\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=epochs,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=f\"{output_dir}/logs\",\n",
    "            logging_steps=10,\n",
    "            save_total_limit=2\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "\n",
    "    def predict(self, texts):\n",
    "        \"\"\"Effectuer une prédiction sur de nouveaux textes\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import CamembertModel\n",
    "\n",
    "class CamemBERTBaseModel(nn.Module):\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the base CamemBERT model.\n",
    "        :param model_path: Path to the pre-trained CamemBERT model.\n",
    "        \"\"\"\n",
    "        super(CamemBERTBaseModel, self).__init__()\n",
    "        self.base_model = CamembertModel.from_pretrained(model_path)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the base model.\n",
    "        :param input_ids: Tensor of token IDs.\n",
    "        :param attention_mask: Tensor of attention masks.\n",
    "        :return: Last hidden states from the base model.\n",
    "        \"\"\"\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "    def get_hidden_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the hidden size of the base model for dynamically attaching heads.\n",
    "        :return: Hidden size of the CamemBERT model.\n",
    "        \"\"\"\n",
    "        return self.base_model.config.hidden_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIFinetuningModel(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, hidden_size: int, num_labels: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the NLI fine-tuning model.\n",
    "        :param base_model: Instance of the base CamemBERT model.\n",
    "        :param hidden_size: Hidden size of the base model's output.\n",
    "        :param num_labels: Number of labels for NLI (default: 3).\n",
    "        \"\"\"\n",
    "        super(NLIFinetuningModel, self).__init__()\n",
    "        self.base_model = base_model  # Base CamemBERT model\n",
    "        self.nli_head = NLIHead(hidden_size, num_labels)  # NLI-specific head\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Tensor,\n",
    "        attention_mask: Tensor,\n",
    "        labels: Tensor = None,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Forward pass for NLI fine-tuning.\n",
    "        :param input_ids: Tensor of token IDs.\n",
    "        :param attention_mask: Tensor of attention masks.\n",
    "        :param labels: Optional tensor of labels (batch_size).\n",
    "        :return: Dictionary containing logits and optionally loss.\n",
    "        \"\"\"\n",
    "        # Get last hidden states from the base model\n",
    "        hidden_states = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the [CLS] token's representation\n",
    "        cls_output = hidden_states[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        # Pass through the NLI head\n",
    "        logits = self.nli_head(cls_output)  # Shape: (batch_size, num_labels)\n",
    "\n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return {\"logits\": logits, \"loss\": loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIFinetuningModel(nn.Module):\n",
    "    def __init__(self, base_model: CamemBERTBaseModel, num_labels: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the NLI fine-tuning model.\n",
    "        :param base_model: Instance of the base CamemBERT model.\n",
    "        :param num_labels: Number of labels for NLI.\n",
    "        \"\"\"\n",
    "        super(NLIFinetuningModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.nli_head = NLIHead(base_model.get_hidden_size(), num_labels)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Forward pass for NLI fine-tuning.\n",
    "        :param input_ids: Tensor of token IDs.\n",
    "        :param attention_mask: Tensor of attention masks.\n",
    "        :param labels: Optional tensor of labels (batch_size).\n",
    "        :return: Dictionary containing logits and optionally loss.\n",
    "        \"\"\"\n",
    "        # Get last hidden states from the base model\n",
    "        hidden_states = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the [CLS] token's representation\n",
    "        cls_output = hidden_states[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        # Pass through the NLI head\n",
    "        logits = self.nli_head(cls_output)  # Shape: (batch_size, num_labels)\n",
    "\n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return {\"logits\": logits, \"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NLIDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m hypotheses \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLe ciel est vert.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMarie est au marché.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Contradiction, Neutral\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mNLIDataset\u001b[49m(premises, hypotheses, labels, model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamembert-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Premier exemple\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NLIDataset' is not defined"
     ]
    }
   ],
   "source": [
    "premises = [\"Le ciel est bleu.\", \"Marie mange une pomme.\"]\n",
    "hypotheses = [\"Le ciel est vert.\", \"Marie est au marché.\"]\n",
    "labels = [2, 1]  # Contradiction, Neutral\n",
    "\n",
    "dataset = NLIDataset(premises, hypotheses, labels, model_path=\"camembert-base\")\n",
    "print(dataset[0])  # Premier exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6598ab26-5bc2-4acb-82c6-2f0f9a6b509b",
   "metadata": {},
   "source": [
    "# Downloading the OSCAR Dataset\n",
    "\n",
    "In this notebook, we download the OSCAR dataset to perform the pretraining of the **CamemBERT** language model.\n",
    "\n",
    "## Prerequisites\n",
    "1. Ensure you are logged into **Hugging Face**:\n",
    "   - Run the following command in your terminal to authenticate:\n",
    "     ```bash\n",
    "     huggingface-cli login\n",
    "     ```\n",
    "   - This will prompt you to enter your Hugging Face token. You can generate a token from [Hugging Face Settings](https://huggingface.co/settings/tokens).\n",
    "\n",
    "2. Accept the dataset usage conditions on Hugging Face:\n",
    "   - Visit the [OSCAR dataset page](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201) and click on **\"Request Access\"** if needed.\n",
    "\n",
    "## Steps in this Notebook\n",
    "- Download the OSCAR dataset in the desired language.\n",
    "- Preprocess the dataset for training.\n",
    "\n",
    "\n",
    "> Once you have completed the prerequisites, you can run the code below to proceed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64d75747-5d6d-4618-86ac-cc76e4e2afe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous êtes connecté en tant que : Noureddine-khaous\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "# Vérifiez votre identité sur Hugging Face\n",
    "user_info = whoami()\n",
    "print(f\"Vous êtes connecté en tant que : {user_info['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16e126-f17e-451d-8af0-d542a5a9b274",
   "metadata": {},
   "source": [
    "## Download the oscar dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0597d4d7-8cec-4dd4-91be-1cf46fef62c9",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb2e7cca05c49df8a311e52ec42fae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"oscar-corpus/OSCAR-2201\",\n",
    "                        #use_auth_token=True, # the method doesn't accept the param\n",
    "                        language=\"fr\", \n",
    "                        streaming=False, # download locally\n",
    "                        split=\"train\",\n",
    "                        cache_dir='CamemBERT/data',\n",
    "                        trust_remote_code=True) # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fa28dba-c3aa-4d8a-bba2-17798fdfdb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La taille du dataset est : 52037098\n",
      "Le type de dataset est : <class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(f'La taille du dataset est : {len(dataset)}')\n",
    "print(f'Le type de dataset est : {type(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19112e32-62fe-4dbb-821a-5c0f4f47342c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille totale du dataset en cache : 308.25 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Obtenir le chemin des fichiers cache\n",
    "cache_files = dataset.cache_files\n",
    "\n",
    "# Calculer la taille totale en bytes\n",
    "total_size_bytes = sum(os.path.getsize(file[\"filename\"]) for file in cache_files)\n",
    "\n",
    "# Convertir en gigaoctets\n",
    "total_size_gb = total_size_bytes / (1024 ** 3)\n",
    "print(f\"Taille totale du dataset en cache : {total_size_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b2b6221-aeab-4091-992c-7d9b15db4986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d'exemples : 52037098\n",
      "Colonnes : ['id', 'text', 'meta']\n",
      "\n",
      "{'id': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None), 'meta': {'warc_headers': {'warc-record-id': Value(dtype='string', id=None), 'warc-date': Value(dtype='string', id=None), 'content-type': Value(dtype='string', id=None), 'content-length': Value(dtype='int32', id=None), 'warc-type': Value(dtype='string', id=None), 'warc-identified-content-language': Value(dtype='string', id=None), 'warc-refers-to': Value(dtype='string', id=None), 'warc-target-uri': Value(dtype='string', id=None), 'warc-block-digest': Value(dtype='string', id=None)}, 'identification': {'label': Value(dtype='string', id=None), 'prob': Value(dtype='float32', id=None)}, 'annotations': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'line_identifications': [{'label': Value(dtype='string', id=None), 'prob': Value(dtype='float32', id=None)}]}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nombre total d'exemples : {len(dataset)}\")\n",
    "print(\"Colonnes :\", dataset.column_names)\n",
    "print()\n",
    "#print(\"Premier exemple :\", dataset[0])\n",
    "print(dataset.features)      # Types des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b6782-a73f-477c-b5dc-2ba23c632e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'bon', '##jou', '##r', ',', 'comment', 'all', '##ez', '-', 'vo', '##us', '?', 'au', '##jou', '##rd', \"'\", 'hui', 'il', 'fai', '[SEP]']\n",
      "Input IDs: tensor([[  101, 14753, 23099,  2099,  1010,  7615,  2035,  9351,  1011, 29536,\n",
      "          2271,  1029,  8740, 23099,  4103,  1005, 17504,  6335, 26208,   102]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "example = {\"text\": \"Bonjour, comment allez-vous ? .\"}\n",
    "\n",
    "tokenized_example = tokenizer(\n",
    "    example[\"text\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=20,  # Limite arbitraire pour l'exemple\n",
    "    return_tensors=\"pt\"  # Retourne les tenseurs PyTorch\n",
    ")\n",
    "\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(tokenized_example[\"input_ids\"][0]))\n",
    "print(\"Input IDs:\", tokenized_example[\"input_ids\"])\n",
    "print(\"Attention Mask:\", tokenized_example[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7215ded",
   "metadata": {},
   "source": [
    "## Preparing Data for Training CamemBERT\n",
    "\n",
    "### Context\n",
    "We aim to train CamemBERT using the French OSCAR dataset. As mentioned in the original CamemBERT paper, reducing the training dataset size (e.g., from 138GB to 4GB) still provides meaningful results, although with a slight performance drop.\n",
    "\n",
    "In this project, we downloaded **308GB** of French text from OSCAR. To make the training manageable and comparable, we limit the dataset size to **4GB**.\n",
    "\n",
    "### Objective\n",
    "Create a random subset of **4GB** from the **308GB** dataset while minimizing the risk of overfitting. To avoid the model learning biases from specific topics, we shuffle the data and select a representative sample.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "1. **Dataset Download**: We used Hugging Face's `datasets` library to download the French OSCAR dataset.\n",
    "   - Total number of examples: **52,037,098**.\n",
    "   - Total size: **308GB**.\n",
    "\n",
    "2. **Calculating the Ratio**:\n",
    "   To reduce the dataset to **4GB**, we compute the ratio:\n",
    "   $$\n",
    "   \\text{ratio} = \\frac{4}{308} \\approx 0.013\n",
    "   $$\n",
    "   This corresponds to approximately **1.3%** of the dataset, or **676,482 examples** out of the **52 million**.\n",
    "\n",
    "3. **Selection Process**:\n",
    "   - Shuffle the dataset to ensure randomness.\n",
    "   - Select **676,482 examples** to create a dataset of approximately **4GB**.\n",
    "\n",
    "4. **Saving the Subset**:\n",
    "   - The selected data is saved in an efficient format:\n",
    "     - **Arrow**: Optimized for direct use with Hugging Face.\n",
    "     - **Parquet**: Suitable for general-purpose processing.\n",
    "\n",
    "### Pipeline Steps\n",
    "- Shuffle the full dataset.\n",
    "- Randomly select **676,482 examples** corresponding to **4GB**.\n",
    "- Save the subset in a compact format.\n",
    "\n",
    "### Verification\n",
    "After saving, we verify the final dataset size in **GB** to ensure it aligns with the target size of **4GB**.\n",
    "\n",
    "---\n",
    "\n",
    "This reduction ensures faster training while preserving the diversity and quality needed to avoid biases and overfitting. The resulting mini-dataset will be used for pretraining or fine-tuning CamemBERT.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a801130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "user_info = whoami()\n",
    "print(f\"Vous êtes connecté en tant que : {user_info['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b91bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"oscar-corpus/OSCAR-2201\",\n",
    "                        #use_auth_token=True, # the method doesn't accept the param\n",
    "                        language=\"fr\", \n",
    "                        streaming=False, # download locally\n",
    "                        split=\"train\",\n",
    "                        cache_dir='CamemBERT/data',\n",
    "                        trust_remote_code=True) # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6854e3d",
   "metadata": {},
   "source": [
    "## 2. Shuffle the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f98ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "num_examples_needed = 676482  # Nombre d'exemples pour environ 4GB\n",
    "\n",
    "mini_dataset = shuffled_dataset.select(range(num_examples_needed))\n",
    "print(f\"Nombre d'exemples dans le mini-dataset : {len(mini_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eec99ad",
   "metadata": {},
   "source": [
    "Save the mini oscar dataset of 4gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder en Arrow (recommandé pour Hugging Face)\n",
    "mini_dataset.save_to_disk(\"CamemBERT/data/mini_oscar/mini_dataset.arrow\")\n",
    "print(\"Mini-dataset sauvegardé au format Arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad27f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

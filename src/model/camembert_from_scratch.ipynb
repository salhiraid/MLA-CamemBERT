{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForMaskedLM(\n",
       "  (roberta): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): CamembertLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=32005, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CamembertForMaskedLM\n",
    "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from packaging import version\n",
    "from transformers.utils import logging\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamembertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.position_ids = torch.arange(config.max_position_embeddings).unsqueeze(0)  # Shape (1, max_position_embeddings)\n",
    "        self.token_type_ids = torch.zeros_like(self.position_ids, dtype=torch.long)  # Shape (1, max_position_embeddings)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        input_shape = input_ids.size()\n",
    "        batch_size, seq_length = input_shape\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length].to(input_ids.device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = self.token_type_ids[:, :seq_length].expand(batch_size, seq_length).to(input_ids.device)\n",
    "\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        token_type_embeds = self.token_type_embeddings(token_type_ids)\n",
    "        embeddings = inputs_embeds + position_embeds + token_type_embeds\n",
    "\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([4, 20])\n",
      "Output embeddings shape: torch.Size([4, 20, 768])\n",
      "Test passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class CamembertConfig:\n",
    "    \"\"\"\n",
    "    This is the configuration class to store the configuration of a [`CamembertModel`] or a [`TFCamembertModel`]. \n",
    "    It defines the model architecture and is used to instantiate a Camembert model with specified arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size: int = 30522\n",
    "    hidden_size: int = 768\n",
    "    num_hidden_layers: int = 12\n",
    "    num_attention_heads: int = 12\n",
    "    intermediate_size: int = 3072\n",
    "    hidden_act: str = \"gelu\"\n",
    "    hidden_dropout_prob: float = 0.1\n",
    "    attention_probs_dropout_prob: float = 0.1\n",
    "    max_position_embeddings: int = 512\n",
    "    type_vocab_size: int = 2\n",
    "    initializer_range: float = 0.02\n",
    "    layer_norm_eps: float = 1e-12\n",
    "    pad_token_id: int = 1\n",
    "    bos_token_id: int = 0\n",
    "    eos_token_id: int = 2\n",
    "    position_embedding_type: str = \"absolute\"\n",
    "    use_cache: bool = True\n",
    "    head_type: str = \"MLM\"\n",
    "    classifier_dropout: float = None\n",
    "\n",
    "# Instantiate the configuration and embeddings\n",
    "config = CamembertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MockConfig' object has no attribute 'vocab_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mCamembertEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create dummy inputs\u001b[39;00m\n\u001b[0;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mCamembertEmbeddings.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m, config\u001b[38;5;241m.\u001b[39mhidden_size, padding_idx\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mmax_position_embeddings, config\u001b[38;5;241m.\u001b[39mhidden_size, padding_idx\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mtype_vocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MockConfig' object has no attribute 'vocab_size'"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = CamembertEmbeddings(config)\n",
    "\n",
    "# Create dummy inputs\n",
    "batch_size = 4\n",
    "seq_length = 20\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))  # Random token IDs\n",
    "token_type_ids = torch.randint(0, config.type_vocab_size, (batch_size, seq_length))  # Random token type IDs\n",
    "\n",
    "# Forward pass through the embeddings\n",
    "output = embeddings(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "# Display results\n",
    "print(\"Input IDs shape:\", input_ids.shape)\n",
    "print(\"Output embeddings shape:\", output.shape)\n",
    "assert output.shape == (batch_size, seq_length, config.hidden_size), \"Output shape is incorrect!\"\n",
    "\n",
    "print(\"Test passed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input hidden states shape: torch.Size([4, 20, 768])\n",
      "Output context shape: torch.Size([4, 20, 768])\n",
      "\n",
      "Testing without attention mask:\n",
      "Test passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "\n",
    "class CamembertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        batch_size, seq_length, hidden_size = x.size()\n",
    "        x = x.view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size)\n",
    "        return x.permute(0, 2, 1, 3)  # [batch, num_heads, seq_len, head_size]\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores /= math.sqrt(self.attention_head_size)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, seq_len]\n",
    "            attention_scores += attention_mask\n",
    "\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        context_layer = context_layer.view(hidden_states.size(0), -1, self.all_head_size)\n",
    "\n",
    "        return context_layer\n",
    "\n",
    "        \n",
    "# Mock configuration class for testing\n",
    "@dataclass\n",
    "class MockConfig:\n",
    "    hidden_size: int = 768\n",
    "    num_attention_heads: int = 12\n",
    "    attention_probs_dropout_prob: float = 0.1\n",
    "\n",
    "# Create dummy inputs\n",
    "batch_size = 4\n",
    "seq_length = 20\n",
    "\n",
    "# Instantiate configuration and self-attention module\n",
    "config = MockConfig()\n",
    "self_attention = CamembertSelfAttention(config)\n",
    "\n",
    "# Generate random hidden states and an optional attention mask\n",
    "hidden_states = torch.rand(batch_size, seq_length, config.hidden_size)  # Shape: [batch_size, seq_length, hidden_size]\n",
    "attention_mask = torch.ones(batch_size, seq_length)  # Shape: [batch_size, seq_length]\n",
    "attention_mask[:, -5:] = 0  # Mask the last 5 tokens of each sequence\n",
    "\n",
    "# Pass data through the self-attention module\n",
    "output = self_attention(hidden_states, attention_mask)\n",
    "\n",
    "# Print the results\n",
    "print(\"Input hidden states shape:\", hidden_states.shape)\n",
    "print(\"Output context shape:\", output.shape)\n",
    "assert output.shape == hidden_states.shape, \"Output shape mismatch!\"\n",
    "\n",
    "# Additional checks\n",
    "print(\"\\nTesting without attention mask:\")\n",
    "output_no_mask = self_attention(hidden_states)\n",
    "assert output_no_mask.shape == hidden_states.shape, \"Output shape mismatch without attention mask!\"\n",
    "\n",
    "print(\"Test passed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class CamembertConfig:\n",
    "    \"\"\"\n",
    "    This is the configuration class to store the configuration of a [`CamembertModel`] or a [`TFCamembertModel`]. \n",
    "    It defines the model architecture and is used to instantiate a Camembert model with specified arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size: int = 30522\n",
    "    hidden_size: int = 768\n",
    "    num_hidden_layers: int = 12\n",
    "    num_attention_heads: int = 12\n",
    "    intermediate_size: int = 3072\n",
    "    hidden_act: str = \"gelu\"\n",
    "    hidden_dropout_prob: float = 0.1\n",
    "    attention_probs_dropout_prob: float = 0.1\n",
    "    max_position_embeddings: int = 512\n",
    "    type_vocab_size: int = 2\n",
    "    initializer_range: float = 0.02\n",
    "    layer_norm_eps: float = 1e-12\n",
    "    pad_token_id: int = 1\n",
    "    bos_token_id: int = 0\n",
    "    eos_token_id: int = 2\n",
    "    position_embedding_type: str = \"absolute\"\n",
    "    use_cache: bool = True\n",
    "    head_type: str = \"MLM\"\n",
    "    classifier_dropout: float = None\n",
    "\n",
    "# Instantiate the configuration and embeddings\n",
    "config = CamembertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamembertMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 128, 768])\n",
      "Output shape: torch.Size([4, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "mlp = CamembertMLP(config)\n",
    "\n",
    "# Génération de données aléatoires pour le test\n",
    "batch_size = 4\n",
    "seq_length = 128\n",
    "hidden_size = config.hidden_size\n",
    "\n",
    "# Tensor d'entrée aléatoire\n",
    "x = torch.randn(batch_size, seq_length, hidden_size)\n",
    "\n",
    "# Tester la forward pass\n",
    "output = mlp(x)\n",
    "\n",
    "# Afficher les formes des tenseurs\n",
    "print(\"Input shape:\", x.shape)  # (batch_size, seq_length, hidden_size)\n",
    "print(\"Output shape:\", output.shape)  # (batch_size, seq_length, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamembertBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = CamembertSelfAttention(config)  # Multi-Head Attention\n",
    "        self.ln_1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)  # LayerNorm après l'attention\n",
    "        self.mlp = CamembertMLP(config)  # Feed Forward Network (MLP)\n",
    "        self.ln_2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)  # LayerNorm après le MLP\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Multi-Head Attention avec résiduel\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        hidden_states = hidden_states + attention_output  # Résiduel\n",
    "        hidden_states = self.ln_1(hidden_states)  # Normalisation\n",
    "\n",
    "        # Feed Forward Network (MLP) avec résiduel\n",
    "        mlp_output = self.mlp(hidden_states)\n",
    "        hidden_states = hidden_states + mlp_output  # Résiduel\n",
    "        hidden_states = self.ln_2(hidden_states)  # Normalisation\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 128, 768])\n",
      "Output shape: torch.Size([4, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instancier le CamembertBlock\n",
    "block = CamembertBlock(config)\n",
    "\n",
    "# Générer des données aléatoires pour le test\n",
    "batch_size = 4\n",
    "seq_length = 128\n",
    "hidden_size = config.hidden_size\n",
    "\n",
    "hidden_states = torch.randn(batch_size, seq_length, hidden_size)\n",
    "attention_mask = torch.ones(batch_size, seq_length)  # Tous les tokens sont non masqués\n",
    "\n",
    "# Tester le bloc\n",
    "output = block(hidden_states, attention_mask)\n",
    "\n",
    "# Afficher les formes des tenseurs\n",
    "print(\"Input shape:\", hidden_states.shape)  # (batch_size, seq_length, hidden_size)\n",
    "print(\"Output shape:\", output.shape)  # (batch_size, seq_length, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from packaging import version\n",
    "from transformers.utils import logging\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class CamembertConfig:\n",
    "    \"\"\"\n",
    "    This is the configuration class to store the configuration of a [`CamembertModel`] or a [`TFCamembertModel`]. \n",
    "    It defines the model architecture and is used to instantiate a Camembert model with specified arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size: int = 32005\n",
    "    hidden_size: int = 768\n",
    "    num_hidden_layers: int = 12\n",
    "    num_attention_heads: int = 12\n",
    "    intermediate_size: int = 3072\n",
    "    hidden_act: str = \"gelu\"\n",
    "    hidden_dropout_prob: float = 0.1\n",
    "    attention_probs_dropout_prob: float = 0.1\n",
    "    max_position_embeddings: int = 514\n",
    "    type_vocab_size: int = 1\n",
    "    initializer_range: float = 0.02\n",
    "    layer_norm_eps: float = 1e-05\n",
    "    pad_token_id: int = 1\n",
    "    bos_token_id: int = 0\n",
    "    eos_token_id: int = 2\n",
    "    position_embedding_type: str = \"absolute\"\n",
    "    use_cache: bool = True\n",
    "    head_type: str = \"MLM\"\n",
    "    classifier_dropout: float = None\n",
    "\n",
    "\n",
    "\n",
    "class CamembertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.position_ids = torch.arange(config.max_position_embeddings).unsqueeze(0)  # Shape (1, max_position_embeddings)\n",
    "        self.token_type_ids = torch.zeros_like(self.position_ids, dtype=torch.long)  # Shape (1, max_position_embeddings)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        input_shape = input_ids.size()\n",
    "        batch_size, seq_length = input_shape\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length].to(input_ids.device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = self.token_type_ids[:, :seq_length].expand(batch_size, seq_length).to(input_ids.device)\n",
    "\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        token_type_embeds = self.token_type_embeddings(token_type_ids)\n",
    "        embeddings = inputs_embeds + position_embeds + token_type_embeds\n",
    "\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "\n",
    "class CamembertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        batch_size, seq_length, hidden_size = x.size()\n",
    "        x = x.view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size)\n",
    "        return x.permute(0, 2, 1, 3)  # [batch, num_heads, seq_len, head_size]\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores /= math.sqrt(self.attention_head_size)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, seq_len]\n",
    "            attention_scores += attention_mask\n",
    "\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        context_layer = context_layer.view(hidden_states.size(0), -1, self.all_head_size)\n",
    "\n",
    "        return context_layer\n",
    "    \n",
    "\n",
    "class CamembertMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CamembertBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = CamembertSelfAttention(config)  # Multi-Head Attention\n",
    "        self.ln_1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)  # LayerNorm après l'attention\n",
    "        self.mlp = CamembertMLP(config)  # Feed Forward Network (MLP)\n",
    "        self.ln_2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)  # LayerNorm après le MLP\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Multi-Head Attention avec résiduel\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        hidden_states = hidden_states + attention_output  # Résiduel\n",
    "        hidden_states = self.ln_1(hidden_states)  # Normalisation\n",
    "\n",
    "        # Feed Forward Network (MLP) avec résiduel\n",
    "        mlp_output = self.mlp(hidden_states)\n",
    "        hidden_states = hidden_states + mlp_output  # Résiduel\n",
    "        hidden_states = self.ln_2(hidden_states)  # Normalisation\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Camembert(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = CamembertEmbeddings(config)\n",
    "        self.encoder = nn.ModuleList([CamembertBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        hidden_states = self.embeddings(input_ids)\n",
    "        for block in self.encoder:\n",
    "            hidden_states = block(hidden_states, attention_mask)\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "model = Camembert(CamembertConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Camembert(\n",
       "  (embeddings): CamembertEmbeddings(\n",
       "    (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): ModuleList(\n",
       "    (0-11): 12 x CamembertBlock(\n",
       "      (attention): CamembertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): CamembertMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=768, out_features=32005, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle Camembert a 127,557,637 paramètres.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Exemple avec ton modèle Camembert\n",
    "config = CamembertConfig()\n",
    "model = Camembert(config)\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Le modèle Camembert a {total_params:,} paramètres.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

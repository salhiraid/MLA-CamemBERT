{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CamembertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Model initialized\n",
      "CamembertForMaskedLM(\n",
      "  (roberta): CamembertModel(\n",
      "    (embeddings): CamembertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(512, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): CamembertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x CamembertLayer(\n",
      "          (attention): CamembertAttention(\n",
      "            (self): CamembertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CamembertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): CamembertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): CamembertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): CamembertLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "CamembertForMaskedLM                                              --\n",
       "├─CamembertModel: 1-1                                             --\n",
       "│    └─CamembertEmbeddings: 2-1                                   --\n",
       "│    │    └─Embedding: 3-1                                        23,440,896\n",
       "│    │    └─Embedding: 3-2                                        393,216\n",
       "│    │    └─Embedding: 3-3                                        1,536\n",
       "│    │    └─LayerNorm: 3-4                                        1,536\n",
       "│    │    └─Dropout: 3-5                                          --\n",
       "│    └─CamembertEncoder: 2-2                                      --\n",
       "│    │    └─ModuleList: 3-6                                       85,054,464\n",
       "├─CamembertLMHead: 1-2                                            --\n",
       "│    └─Linear: 2-3                                                590,592\n",
       "│    └─LayerNorm: 2-4                                             1,536\n",
       "│    └─Linear: 2-5                                                23,471,418\n",
       "==========================================================================================\n",
       "Total params: 132,955,194\n",
       "Trainable params: 132,955,194\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # model summary\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CamembertForMaskedLM, CamembertTokenizer, TrainingArguments, AdamW, Trainer , CamembertConfig\n",
    "from datasets import load_from_disk\n",
    "from dataset import OscarDataset\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "# === Model === #\n",
    "#config = CamembertConfig(\n",
    " #    vocab_size=tokenizer.vocab_size)  # Adjust to match your tokenizer's vocab size\n",
    "#     hidden_size=768,                 # Hidden size (RoBERTa_BASE)\n",
    "#     num_hidden_layers=12,            # Number of transformer layers\n",
    "#     num_attention_heads=12,          # Number of attention heads\n",
    "#     intermediate_size=3072,          # FFN inner hidden size\n",
    "#     hidden_dropout_prob=0.1,         # Dropout probability\n",
    "#     attention_probs_dropout_prob=0.1, # Attention dropout probability\n",
    "#     max_position_embeddings=514,     # Maximum sequence length + special tokens\n",
    "#     type_vocab_size=1,               # No token type embeddings\n",
    "#     initializer_range=0.02           # Standard deviation for weight initialization\n",
    "# )\n",
    "\n",
    "config = CamembertConfig()\n",
    "print(config)\n",
    "# Initialize a randomly weighted CamembertForMaskedLM model\n",
    "model = CamembertForMaskedLM(config) \n",
    "#model.to(\"cuda\")\n",
    "# device = \"mps\"\n",
    "# print(device)\n",
    "# model.to(device)\n",
    "print(\"Model initialized\")\n",
    "print(model)\n",
    "summary(model)\n",
    "https://github.com/salhiraid/MLA-CamemBERT.git\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CamembertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.position_ids = torch.arange(config.max_position_embeddings).unsqueeze(0)  # Shape (1, max_position_embeddings)\n",
    "        self.token_type_ids = torch.zeros_like(self.position_ids, dtype=torch.long)  # Shape (1, max_position_embeddings)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        input_shape = input_ids.size()\n",
    "        batch_size, seq_length = input_shape\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length].to(input_ids.device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = self.token_type_ids[:, :seq_length].expand(batch_size, seq_length).to(input_ids.device)\n",
    "\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        token_type_embeds = self.token_type_embeddings(token_type_ids)\n",
    "        embeddings = inputs_embeds + position_embeds + token_type_embeds\n",
    "\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "CamembertEmbeddings                      --\n",
       "├─Embedding: 1-1                         23,440,896\n",
       "├─Embedding: 1-2                         393,216\n",
       "├─Embedding: 1-3                         1,536\n",
       "├─LayerNorm: 1-4                         1,536\n",
       "├─Dropout: 1-5                           --\n",
       "=================================================================\n",
       "Total params: 23,837,184\n",
       "Trainable params: 23,837,184\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CamembertConfig\n",
    "class Config:\n",
    "    vocab_size = 30522  # Match this with the pretrained tokenizer's vocab size\n",
    "    hidden_size = 768\n",
    "    max_position_embeddings = 512\n",
    "    type_vocab_size = 2\n",
    "    hidden_dropout_prob = 0.1\n",
    "    layer_norm_eps = 1e-12\n",
    "    pad_token_id = 1\n",
    "\n",
    "\n",
    "# Initialize configuration and embeddings\n",
    "config = CamembertConfig()\n",
    "embeddings = CamembertEmbeddings(config)\n",
    "\n",
    "# Sample inputs\n",
    "input_ids = torch.randint(0, config.vocab_size, (2, 10))  # Batch of 2, sequence length of 10\n",
    "output = embeddings(input_ids)\n",
    "summary(embeddings)  # Output: (batch_size, sequence_length, hidden_size)\n",
    "  # Output: (batch_size, sequence_length, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class CamembertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        batch_size, seq_length, hidden_size = x.size()\n",
    "        x = x.view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size)\n",
    "        return x.permute(0, 2, 1, 3)  # [batch, num_heads, seq_len, head_size]\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores /= math.sqrt(self.attention_head_size)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, seq_len]\n",
    "            attention_scores += attention_mask\n",
    "\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        context_layer = context_layer.view(hidden_states.size(0), -1, self.all_head_size)\n",
    "\n",
    "        return context_layer\n",
    "\n",
    "class CamembertFeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout1 = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.dense_2 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dense_3 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout2 = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x - self.dropout1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_3(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        return self.dropout(x)\n",
    "\n",
    "class CamembertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = CamembertSelfAttention(config)\n",
    "        self.attention_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.feed_forward = CamembertFeedForward(config)\n",
    "        self.feed_forward_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Self-Attention with skip connection\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        attention_output = self.attention_norm(hidden_states + attention_output)  # Skip connection\n",
    "\n",
    "        # Feed-Forward with skip connection\n",
    "        feed_forward_output = self.feed_forward(attention_output)\n",
    "        layer_output = self.feed_forward_norm(attention_output + feed_forward_output)  # Skip connection\n",
    "\n",
    "        return layer_output\n",
    "\n",
    "class CamembertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([CamembertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "        return hidden_states\n",
    "    \n",
    "class CamembertModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = CamembertEmbeddings(config)  \n",
    "        self.encoder = CamembertEncoder(config)  \n",
    "        if config.head_type == \"MLM\":\n",
    "            self.head = CamembertLMHead(config)\n",
    "        else:\n",
    "            raise ValueError(f\"Head type {config.head_type} not supported\")\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded_input = self.embeddings(input_ids)\n",
    "        if attention_mask is not None:\n",
    "            # attention_mask = (1.0 - attention_mask) * -10000.0  \n",
    "            attention_mask = (1.0 - attention_mask) * -float('inf')\n",
    "\n",
    "        encoder_output = self.encoder(embedded_input, attention_mask)\n",
    "        logits = self.head(encoder_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class CamembertLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.gelu(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        logits = self.decoder(hidden_states)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CamembertEncoder(\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x CamembertLayer(\n",
      "      (attention): CamembertSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (feed_forward): CamembertFeedForward(\n",
      "        (dense_1): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dense_2): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dense_3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "CamembertEncoder                              --\n",
       "├─ModuleList: 1-1                             --\n",
       "│    └─CamembertLayer: 2-1                    --\n",
       "│    │    └─CamembertSelfAttention: 3-1       1,771,776\n",
       "│    │    └─LayerNorm: 3-2                    1,536\n",
       "│    │    └─CamembertFeedForward: 3-3         5,314,560\n",
       "│    │    └─LayerNorm: 3-4                    1,536\n",
       "│    └─CamembertLayer: 2-2                    --\n",
       "│    │    └─CamembertSelfAttention: 3-5       1,771,776\n",
       "│    │    └─LayerNorm: 3-6                    1,536\n",
       "│    │    └─CamembertFeedForward: 3-7         5,314,560\n",
       "│    │    └─LayerNorm: 3-8                    1,536\n",
       "│    └─CamembertLayer: 2-3                    --\n",
       "│    │    └─CamembertSelfAttention: 3-9       1,771,776\n",
       "│    │    └─LayerNorm: 3-10                   1,536\n",
       "│    │    └─CamembertFeedForward: 3-11        5,314,560\n",
       "│    │    └─LayerNorm: 3-12                   1,536\n",
       "│    └─CamembertLayer: 2-4                    --\n",
       "│    │    └─CamembertSelfAttention: 3-13      1,771,776\n",
       "│    │    └─LayerNorm: 3-14                   1,536\n",
       "│    │    └─CamembertFeedForward: 3-15        5,314,560\n",
       "│    │    └─LayerNorm: 3-16                   1,536\n",
       "│    └─CamembertLayer: 2-5                    --\n",
       "│    │    └─CamembertSelfAttention: 3-17      1,771,776\n",
       "│    │    └─LayerNorm: 3-18                   1,536\n",
       "│    │    └─CamembertFeedForward: 3-19        5,314,560\n",
       "│    │    └─LayerNorm: 3-20                   1,536\n",
       "│    └─CamembertLayer: 2-6                    --\n",
       "│    │    └─CamembertSelfAttention: 3-21      1,771,776\n",
       "│    │    └─LayerNorm: 3-22                   1,536\n",
       "│    │    └─CamembertFeedForward: 3-23        5,314,560\n",
       "│    │    └─LayerNorm: 3-24                   1,536\n",
       "│    └─CamembertLayer: 2-7                    --\n",
       "│    │    └─CamembertSelfAttention: 3-25      1,771,776\n",
       "│    │    └─LayerNorm: 3-26                   1,536\n",
       "│    │    └─CamembertFeedForward: 3-27        5,314,560\n",
       "│    │    └─LayerNorm: 3-28                   1,536\n",
       "│    └─CamembertLayer: 2-8                    --\n",
       "│    │    └─CamembertSelfAttention: 3-29      1,771,776\n",
       "│    │    └─LayerNorm: 3-30                   1,536\n",
       "│    │    └─CamembertFeedForward: 3-31        5,314,560\n",
       "│    │    └─LayerNorm: 3-32                   1,536\n",
       "│    └─CamembertLayer: 2-9                    --\n",
       "│    │    └─CamembertSelfAttention: 3-33      1,771,776\n",
       "│    │    └─LayerNorm: 3-34                   1,536\n",
       "│    │    └─CamembertFeedForward: 3-35        5,314,560\n",
       "│    │    └─LayerNorm: 3-36                   1,536\n",
       "│    └─CamembertLayer: 2-10                   --\n",
       "│    │    └─CamembertSelfAttention: 3-37      1,771,776\n",
       "│    │    └─LayerNorm: 3-38                   1,536\n",
       "│    │    └─CamembertFeedForward: 3-39        5,314,560\n",
       "│    │    └─LayerNorm: 3-40                   1,536\n",
       "│    └─CamembertLayer: 2-11                   --\n",
       "│    │    └─CamembertSelfAttention: 3-41      1,771,776\n",
       "│    │    └─LayerNorm: 3-42                   1,536\n",
       "│    │    └─CamembertFeedForward: 3-43        5,314,560\n",
       "│    │    └─LayerNorm: 3-44                   1,536\n",
       "│    └─CamembertLayer: 2-12                   --\n",
       "│    │    └─CamembertSelfAttention: 3-45      1,771,776\n",
       "│    │    └─LayerNorm: 3-46                   1,536\n",
       "│    │    └─CamembertFeedForward: 3-47        5,314,560\n",
       "│    │    └─LayerNorm: 3-48                   1,536\n",
       "======================================================================\n",
       "Total params: 85,072,896\n",
       "Trainable params: 85,072,896\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = CamembertEncoder(config)\n",
    "print(encoder)\n",
    "summary(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "CamembertLMHead                          --\n",
       "├─Linear: 1-1                            590,592\n",
       "├─LayerNorm: 1-2                         1,536\n",
       "├─Linear: 1-3                            23,471,418\n",
       "=================================================================\n",
       "Total params: 24,063,546\n",
       "Trainable params: 24,063,546\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = CamembertLMHead(config)\n",
    "summary(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 30522])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "CamembertModel                                     --\n",
       "├─CamembertEmbeddings: 1-1                         --\n",
       "│    └─Embedding: 2-1                              23,440,896\n",
       "│    └─Embedding: 2-2                              393,216\n",
       "│    └─Embedding: 2-3                              1,536\n",
       "│    └─LayerNorm: 2-4                              1,536\n",
       "│    └─Dropout: 2-5                                --\n",
       "├─CamembertEncoder: 1-2                            --\n",
       "│    └─ModuleList: 2-6                             --\n",
       "│    │    └─CamembertLayer: 3-1                    7,089,408\n",
       "│    │    └─CamembertLayer: 3-2                    7,089,408\n",
       "│    │    └─CamembertLayer: 3-3                    7,089,408\n",
       "│    │    └─CamembertLayer: 3-4                    7,089,408\n",
       "│    │    └─CamembertLayer: 3-5                    7,089,408\n",
       "│    │    └─CamembertLayer: 3-6                    7,089,408\n",
       "│    │    └─CamembertLayer: 3-7                    7,089,408\n",
       "│    │    └─CamembertLayer: 3-8                    7,089,408\n",
       "│    │    └─CamembertLayer: 3-9                    7,089,408\n",
       "│    │    └─CamembertLayer: 3-10                   7,089,408\n",
       "│    │    └─CamembertLayer: 3-11                   7,089,408\n",
       "│    │    └─CamembertLayer: 3-12                   7,089,408\n",
       "├─CamembertLMHead: 1-3                             --\n",
       "│    └─Linear: 2-7                                 590,592\n",
       "│    └─LayerNorm: 2-8                              1,536\n",
       "│    └─Linear: 2-9                                 23,471,418\n",
       "===========================================================================\n",
       "Total params: 132,973,626\n",
       "Trainable params: 132,973,626\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config_model import CamembertConfig\n",
    "\n",
    "\n",
    "config = CamembertConfig()\n",
    "\n",
    "# Create model\n",
    "model = CamembertModel(config)\n",
    "\n",
    "# Sample inputs\n",
    "input_ids = torch.randint(0, config.vocab_size, (2, 10))  # Batch size of 2, sequence length of 10\n",
    "attention_mask = torch.ones(2, 10)  # No masking\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_ids, attention_mask)\n",
    "print(output.shape)  # Should be (2, 10, 768)\n",
    "summary(model)  # Output: (batch_size, sequence_length, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "CamembertLMHead                          --\n",
       "├─Linear: 1-1                            590,592\n",
       "├─LayerNorm: 1-2                         1,536\n",
       "├─Linear: 1-3                            23,471,418\n",
       "=================================================================\n",
       "Total params: 24,063,546\n",
       "Trainable params: 24,063,546\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Head = CamembertLMHead(config)\n",
    "\n",
    "summary(Head)  # Output: (batch_size, sequence_length, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salhi\\.conda\\envs\\nlp\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salhi\\.conda\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CamembertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Model initialized\n",
      "CamembertForMaskedLM(\n",
      "  (roberta): CamembertModel(\n",
      "    (embeddings): CamembertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(512, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): CamembertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x CamembertLayer(\n",
      "          (attention): CamembertAttention(\n",
      "            (self): CamembertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): CamembertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): CamembertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): CamembertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): CamembertLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "CamembertForMaskedLM                                         --\n",
       "├─CamembertModel: 1-1                                        --\n",
       "│    └─CamembertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                                   23,440,896\n",
       "│    │    └─Embedding: 3-2                                   393,216\n",
       "│    │    └─Embedding: 3-3                                   1,536\n",
       "│    │    └─LayerNorm: 3-4                                   1,536\n",
       "│    │    └─Dropout: 3-5                                     --\n",
       "│    └─CamembertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                                  85,054,464\n",
       "├─CamembertLMHead: 1-2                                       --\n",
       "│    └─Linear: 2-3                                           590,592\n",
       "│    └─LayerNorm: 2-4                                        1,536\n",
       "│    └─Linear: 2-5                                           23,471,418\n",
       "=====================================================================================\n",
       "Total params: 132,955,194\n",
       "Trainable params: 132,955,194\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # model summary\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CamembertForMaskedLM, CamembertTokenizer, TrainingArguments, AdamW, Trainer , CamembertConfig\n",
    "from datasets import load_from_disk\n",
    "from dataset import OscarDataset\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "# === Model === #\n",
    "#config = CamembertConfig(\n",
    " #    vocab_size=tokenizer.vocab_size)  # Adjust to match your tokenizer's vocab size\n",
    "#     hidden_size=768,                 # Hidden size (RoBERTa_BASE)\n",
    "#     num_hidden_layers=12,            # Number of transformer layers\n",
    "#     num_attention_heads=12,          # Number of attention heads\n",
    "#     intermediate_size=3072,          # FFN inner hidden size\n",
    "#     hidden_dropout_prob=0.1,         # Dropout probability\n",
    "#     attention_probs_dropout_prob=0.1, # Attention dropout probability\n",
    "#     max_position_embeddings=514,     # Maximum sequence length + special tokens\n",
    "#     type_vocab_size=1,               # No token type embeddings\n",
    "#     initializer_range=0.02           # Standard deviation for weight initialization\n",
    "# )\n",
    "\n",
    "config = CamembertConfig()\n",
    "print(config)\n",
    "# Initialize a randomly weighted CamembertForMaskedLM model\n",
    "model = CamembertForMaskedLM(config) \n",
    "#model.to(\"cuda\")\n",
    "\n",
    "print(\"Model initialized\")\n",
    "print(model)\n",
    "summary(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CamembertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.position_ids = torch.arange(config.max_position_embeddings).unsqueeze(0)  # Shape (1, max_position_embeddings)\n",
    "        self.token_type_ids = torch.zeros_like(self.position_ids, dtype=torch.long)  # Shape (1, max_position_embeddings)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        input_shape = input_ids.size()\n",
    "        batch_size, seq_length = input_shape\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length].to(input_ids.device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = self.token_type_ids[:, :seq_length].expand(batch_size, seq_length).to(input_ids.device)\n",
    "\n",
    "        inputs_embeds = self.word_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        token_type_embeds = self.token_type_embeddings(token_type_ids)\n",
    "        embeddings = inputs_embeds + position_embeds + token_type_embeds\n",
    "\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "CamembertEmbeddings                      --\n",
       "├─Embedding: 1-1                         23,440,896\n",
       "├─Embedding: 1-2                         393,216\n",
       "├─Embedding: 1-3                         1,536\n",
       "├─LayerNorm: 1-4                         1,536\n",
       "├─Dropout: 1-5                           --\n",
       "=================================================================\n",
       "Total params: 23,837,184\n",
       "Trainable params: 23,837,184\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CamembertConfig\n",
    "class Config:\n",
    "    vocab_size = 30522  # Match this with the pretrained tokenizer's vocab size\n",
    "    hidden_size = 768\n",
    "    max_position_embeddings = 512\n",
    "    type_vocab_size = 2\n",
    "    hidden_dropout_prob = 0.1\n",
    "    layer_norm_eps = 1e-12\n",
    "    pad_token_id = 1\n",
    "\n",
    "\n",
    "# Initialize configuration and embeddings\n",
    "config = CamembertConfig()\n",
    "embeddings = CamembertEmbeddings(config)\n",
    "\n",
    "# Sample inputs\n",
    "input_ids = torch.randint(0, config.vocab_size, (2, 10))  # Batch of 2, sequence length of 10\n",
    "output = embeddings(input_ids)\n",
    "summary(embeddings)  # Output: (batch_size, sequence_length, hidden_size)\n",
    "  # Output: (batch_size, sequence_length, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class CamembertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        batch_size, seq_length, hidden_size = x.size()\n",
    "        x = x.view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size)\n",
    "        return x.permute(0, 2, 1, 3)  # [batch, num_heads, seq_len, head_size]\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Linear projections\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores /= math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # Apply attention mask (broadcast to match attention_scores shape)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, seq_len]\n",
    "            attention_scores += attention_mask\n",
    "\n",
    "        # Normalize attention scores to probabilities\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Weighted sum of value vectors\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        context_layer = context_layer.view(hidden_states.size(0), -1, self.all_head_size)\n",
    "\n",
    "        return context_layer\n",
    "\n",
    "class CamembertFeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.activation = nn.ReLU()  # Simpler activation for beginners\n",
    "        self.dense_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CamembertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = CamembertSelfAttention(config)\n",
    "        self.attention_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.feed_forward = CamembertFeedForward(config)\n",
    "        self.feed_forward_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Self-Attention with skip connection\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        attention_output = self.attention_norm(hidden_states + attention_output)  # Skip connection\n",
    "\n",
    "        # Feed-Forward with skip connection\n",
    "        feed_forward_output = self.feed_forward(attention_output)\n",
    "        layer_output = self.feed_forward_norm(attention_output + feed_forward_output)  # Skip connection\n",
    "\n",
    "        return layer_output\n",
    "\n",
    "class CamembertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([CamembertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "        return hidden_states\n",
    "    \n",
    "class CamembertModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = CamembertEmbeddings(config)  # Ensure it includes word, position, and type embeddings\n",
    "        self.encoder = CamembertEncoder(config)  # Ensure num_hidden_layers matches config\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Embedding layer\n",
    "        embedded_input = self.embeddings(input_ids)\n",
    "\n",
    "        # Attention mask preparation\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = (1.0 - attention_mask) * -10000.0  # Convert to large negative values for masked positions\n",
    "\n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(embedded_input, attention_mask)\n",
    "        return encoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "CamembertModel                                     --\n",
       "├─CamembertEmbeddings: 1-1                         --\n",
       "│    └─Embedding: 2-1                              23,440,896\n",
       "│    └─Embedding: 2-2                              393,216\n",
       "│    └─Embedding: 2-3                              1,536\n",
       "│    └─LayerNorm: 2-4                              1,536\n",
       "│    └─Dropout: 2-5                                --\n",
       "├─CamembertEncoder: 1-2                            --\n",
       "│    └─ModuleList: 2-6                             --\n",
       "│    │    └─CamembertLayer: 3-1                    6,497,280\n",
       "│    │    └─CamembertLayer: 3-2                    6,497,280\n",
       "│    │    └─CamembertLayer: 3-3                    6,497,280\n",
       "│    │    └─CamembertLayer: 3-4                    6,497,280\n",
       "│    │    └─CamembertLayer: 3-5                    6,497,280\n",
       "│    │    └─CamembertLayer: 3-6                    6,497,280\n",
       "│    │    └─CamembertLayer: 3-7                    6,497,280\n",
       "│    │    └─CamembertLayer: 3-8                    6,497,280\n",
       "│    │    └─CamembertLayer: 3-9                    6,497,280\n",
       "│    │    └─CamembertLayer: 3-10                   6,497,280\n",
       "│    │    └─CamembertLayer: 3-11                   6,497,280\n",
       "│    │    └─CamembertLayer: 3-12                   6,497,280\n",
       "===========================================================================\n",
       "Total params: 101,804,544\n",
       "Trainable params: 101,804,544\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a config with essential parameters\n",
    "class Config:\n",
    "    hidden_size = 768\n",
    "    num_attention_heads = 12\n",
    "    intermediate_size = 3072\n",
    "    hidden_dropout_prob = 0.1\n",
    "    attention_probs_dropout_prob = 0.1\n",
    "    num_hidden_layers = 12\n",
    "    layer_norm_eps = 1e-12\n",
    "    vocab_size = 30522\n",
    "    max_position_embeddings = 512\n",
    "\n",
    "config = CamembertConfig()\n",
    "\n",
    "# Create model\n",
    "model = CamembertModel(config)\n",
    "\n",
    "# Sample inputs\n",
    "input_ids = torch.randint(0, config.vocab_size, (2, 10))  # Batch size of 2, sequence length of 10\n",
    "attention_mask = torch.ones(2, 10)  # No masking\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_ids, attention_mask)\n",
    "print(output.shape)  # Should be (2, 10, 768)\n",
    "summary(model)  # Output: (batch_size, sequence_length, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Ajouter 'src' au chemin pour les imports\n",
    "project_root = os.path.abspath(r\"C:\\Users\\Napster\\Desktop\\M2_ISI\\MLA\\CamemBERT\\MLA-CamemBERT\\src\")\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from dataset import OscarDataset\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Napster\\Desktop\\M2_ISI\\MLA\\CamemBERT\\MLA-CamemBERT\\src\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Ajouter le dossier 'src' au chemin de recherche\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "print(current_dir)\n",
    "sys.path.append(current_dir)\n",
    "\n",
    "# Maintenant, importez\n",
    "from dataset import OscarDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Vis \n",
    "- longueur, et affichage des données\n",
    "- examiner un ou plusieurs echantillons\n",
    "- lister les colonnes\n",
    "- statistiques sur le contenu\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Napster\\anaconda3\\envs\\bert\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.activations import ACT2FN, gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oscar_data(Dataset):\n",
    "    def __init__(self, dattaset):\n",
    "        self.dataset = Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  728299, 32987690, 16414791, 31676307, 31131676, 46987146,\n",
       "       36948958, 45292408, 15175217, 16347027, 49728785, 34049189,\n",
       "        6045237, 30666585, 36534021], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ids = np.random.randint(0, 52037098, 15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résumé : Classe BertTokenizer\n",
    "\n",
    "La classe **`BertTokenizer`** de la bibliothèque `transformers` est utilisée pour transformer des textes en tokens compatibles avec le modèle **BERT**. Elle gère les étapes suivantes :\n",
    "1. **Tokenisation** : Découper le texte en sous-mots (`WordPiece`).\n",
    "2. **Ajout de tokens spéciaux** : Ajoute `[CLS]`, `[SEP]`, et éventuellement du padding (`[PAD]`).\n",
    "3. **Troncature** : Coupe les textes trop longs pour respecter la `max_length`.\n",
    "4. **Conversion** : Transforme les tokens en `input_ids` (entiers correspondant au vocabulaire de BERT).\n",
    "\n",
    "---\n",
    "\n",
    "## Principales Méthodes :\n",
    "- **`from_pretrained(model_name)`** : Charge un tokenizer pré-entraîné. Exemple : `\"bert-base-uncased\"`.\n",
    "- **`__call__`** : Méthode principale pour tokeniser du texte. Prend en compte plusieurs options comme :\n",
    "  - `padding=\"max_length\"` : Ajoute du padding jusqu'à une longueur fixée.\n",
    "  - `truncation=True` : Tronque les textes trop longs.\n",
    "  - `max_length=N` : Spécifie la longueur maximale.\n",
    "  - `return_tensors=\"pt\"` : Retourne les données sous forme de tenseurs PyTorch.\n",
    "- **`convert_ids_to_tokens()`** : Convertit les `input_ids` en tokens lisibles.\n",
    "- **`convert_tokens_to_ids()`** : Convertit les tokens en IDs.\n",
    "\n",
    "---\n",
    "\n",
    "## Exemple : Tokenisation Simple\n",
    "\n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Exemple avec une phrase\n",
    "example = \"Bonjour, comment allez-vous aujourd'hui ?\"\n",
    "tokenized = tokenizer(example, padding=\"max_length\", truncation=True, max_length=10)\n",
    "\n",
    "print(\"Input IDs :\", tokenized[\"input_ids\"])\n",
    "print(\"Tokens :\", tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résumé : Classe `datasets.arrow_dataset.Dataset`\n",
    "\n",
    "La classe `Dataset` de la bibliothèque `datasets` (Hugging Face) est utilisée pour gérer efficacement de grands ensembles de données, souvent stockés dans des fichiers `.arrow`.\n",
    "\n",
    "---\n",
    "\n",
    "## Principales caractéristiques :\n",
    "- **Lecture et accès** :\n",
    "  - Les datasets peuvent être chargés directement depuis des sources publiques comme Hugging Face Hub ou localement.\n",
    "  - Les données sont stockées de manière optimisée pour la vitesse et l'efficacité mémoire.\n",
    "\n",
    "- **Fonctionnalités** :\n",
    "  - Compatible avec des millions d'exemples.\n",
    "  - Permet des opérations de transformation (map, filter, shuffle).\n",
    "  - Supporte le streaming pour éviter de charger tout le dataset en mémoire.\n",
    "  - Facilement convertible en formats compatibles avec les frameworks (Pandas, PyTorch, TensorFlow).\n",
    "\n",
    "---\n",
    "\n",
    "## Accès aux données :\n",
    "\n",
    "### 1. **Afficher les premières lignes** :\n",
    "```python\n",
    "# Afficher le premier exemple\n",
    "print(dataset[0])\n",
    "\n",
    "# Afficher les deux premiers exemples\n",
    "print(dataset[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille totale du dataset\n",
    "print(f\"Nombre d'exemples : {len(dataset)}\")\n",
    "\n",
    "# Colonnes disponibles\n",
    "print(f\"Colonnes : {dataset.column_names}\")\n",
    "\n",
    "# Description d'une colonne\n",
    "print(f\"Premier exemple : {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "melange des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "print(shuffled_dataset[0])  # Exemple aléatoire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converit en pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transformer le dataset en DataLoader PyTorch\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)  # Exemple d'une batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples : 10\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "data_arrow_path = r\"C:\\Users\\Napster\\Desktop\\M2_ISI\\MLA\\CamemBERT\\MLA-CamemBERT\\data\\oscar.Arrow\"\n",
    "# Load the Dataset :\n",
    "dataset_path = data_arrow_path  # Remplacez par le chemin réel\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "# Show dataset info :\n",
    "print(f\"Nombre d'exemples : {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gardez l’œil sur toutes les images publiées sur le web en lien avec votre marque. Une analyse poussée des images diffusées par les internautes mais aussi des contenus visuels provenant de vos concurrents.\\nDes applications multiples qui vont de la simple analyse des contenus publiés jusqu’à la surveillance des contrefaçons. ImageTracker scanne ainsi les visuels provenant des réseaux sociaux Pinterest, Twitter et Instagram.\\nVisibility Index\\t\\nSocial Networks Analytics\\t\\nOPINION TRACKER®, une solution\\t\\nGroupe 361 est une agence de communication globale.\\nLa vision 361 est qu'aujourd'hui, plus que jamais, les marques doivent évoluer dans des environnements complexes où les marchés sont devenus des conversations.\\t\\nPlan du site\\nMentions légales\\nContactez-nous\\nLigne directe : 01 76 21 68 30\\nEtre contacté\\nVous souhaitez en savoir plus sur notre solution OPINION TRACKER® ? Complétez le formulaire ci-dessous, un de nos experts prendra contact avec vous rapidement.\\nNom* Prénom\\nSociété Fonction\\nEmail professionnel* Téléphone*\\nSujet* - Préciser -Demande d’infos généralesAvoir des précisions sur les fonctionnalitésEchanger sur un projet à venirProposition de consultationAutre :\\nRecevoir notre plaquette\\nRenseignez les champs ci-dessous pour recevoir notre plaquette.\\nNom* Prenom\\nEmail professionnel* Téléphone\\nSociété* Fonction\\nDemander une démo\\nRenseignez les champs ci-dessous, nous vous recontacterons rapidement.\\nNom* Prénom\\nEmail professionnel* Téléphone*\\nSociété* Fonction*\\nDate de lancement de votre projet* - Préciser -Entre 1 et 3 moisEntre 3 et 6 moisEntre 6 mois et 1 anPlus d’1 anPas de date définie\\nContactez-nous\\nVous souhaitez en savoir plus sur notre solution OPINION TRACKER® ? Complétez le formulaire ci-dessous, un de nos experts prendra contact avec vous rapidement.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)\n",
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "class oscar_data(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_lenght = 512, return_tokens = False):\n",
    "        \"\"\"\n",
    "        :param hf_dataset: Dataset au format Arrow (Hugging Face Dataset)\n",
    "        :param tokenizer: Tokenizer (ex. CamembertTokenizer)\n",
    "        :param max_length: Longueur maximale des séquences après tokenisation\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_lenght = max_lenght\n",
    "        self.return_tokens = return_tokens\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < len(dataset), \"Index out of range\"\n",
    "        # get the text\n",
    "        text = dataset[idx]['text']\n",
    "        # tokenize it\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        if self.return_tokens :\n",
    "            return{\n",
    "                \"input_ids\": tokens[\"input_ids\"].squeeze(0),  # Tensor 1D\n",
    "                \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),  # Tensor 1D\n",
    "                \"tokens\" : self.tokenizer.tokenize(text)\n",
    "            }        \n",
    "        else :\n",
    "            return{\n",
    "                \"input_ids\": tokens[\"input_ids\"].squeeze(0),  # Tensor 1D\n",
    "                \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),  # Tensor 1D\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Gardez', '▁l', '’', 'œil', '▁sur', '▁toutes', '▁les', '▁images', '▁publiées', '▁sur', '▁le', '▁web', '▁en', '▁lien', '▁avec', '▁votre', '▁marque', '.', '▁Une', '▁analyse', '▁poussée', '▁des', '▁images', '▁diffusée', 's', '▁par', '▁les', '▁internautes', '▁mais', '▁aussi', '▁des', '▁contenus', '▁visuels', '▁provenant', '▁de', '▁vos', '▁concurrents', '.', '▁Des', '▁applications', '▁multiples', '▁qui', '▁vont', '▁de', '▁la', '▁simple', '▁analyse', '▁des', '▁contenus', '▁publiés', '▁jusqu', '’', 'à', '▁la', '▁surveillance', '▁des', '▁contrefaçon', 's', '.', '▁Image', 'Tra', 'cker', '▁scan', 'ne', '▁ainsi', '▁les', '▁visuels', '▁provenant', '▁des', '▁réseaux', '▁sociaux', '▁Pinterest', ',', '▁Twitter', '▁et', '▁Instagram', '.', '▁Vis', 'ibili', 'ty', '▁Index', '▁Social', '▁Network', 's', '▁Analytics', '▁OP', 'IN', 'ION', '▁T', 'RAC', 'KER', '®', ',', '▁une', '▁solution', '▁Groupe', '▁3', '61', '▁est', '▁une', '▁agence', '▁de', '▁communication', '▁globale', '.', '▁La', '▁vision', '▁3', '61', '▁est', '▁qu', \"'\", 'aujourd', \"'\", 'hui', ',', '▁plus', '▁que', '▁jamais', ',', '▁les', '▁marques', '▁doivent', '▁évoluer', '▁dans', '▁des', '▁environnement', 's', '▁complexes', '▁où', '▁les', '▁marchés', '▁sont', '▁devenus', '▁des', '▁conversations', '.', '▁Plan', '▁du', '▁site', '▁Mention', 's', '▁légales', '▁Contactez', '-', 'nous', '▁Ligne', '▁directe', '▁:', '▁01', '▁76', '▁21', '▁68', '▁30', '▁Etre', '▁contacté', '▁Vous', '▁souhaitez', '▁en', '▁savoir', '▁plus', '▁sur', '▁notre', '▁solution', '▁OP', 'IN', 'ION', '▁T', 'RAC', 'KER', '®', '▁?', '▁Comp', 'lé', 'tez', '▁le', '▁formulaire', '▁ci', '-', 'dessous', ',', '▁un', '▁de', '▁nos', '▁experts', '▁prendra', '▁contact', '▁avec', '▁vous', '▁rapidement', '.', '▁Nom', '*', '▁Prénom', '▁Société', '▁Fonction', '▁Email', '▁professionnel', '*', '▁Téléphone', '*', '▁Sujet', '*', '▁-', '▁Précis', 'er', '▁-', 'De', 'mand', 'e', '▁d', '’', 'infos', '▁générales', 'A', 'voir', '▁des', '▁précisions', '▁sur', '▁les', '▁fonctionnalités', 'E', 'change', 'r', '▁sur', '▁un', '▁projet', '▁à', '▁venir', 'Pro', 'position', '▁de', '▁consultation', 'Autre', '▁:', '▁Re', 'ce', 'voir', '▁notre', '▁plaquette', '▁Renseignez', '▁les', '▁champs', '▁ci', '-', 'dessous', '▁pour', '▁recevoir', '▁notre', '▁plaquette', '.', '▁Nom', '*', '▁Pre', 'nom', '▁Email', '▁professionnel', '*', '▁Téléphone', '▁Société', '*', '▁Fonction', '▁Demande', 'r', '▁une', '▁démo', '▁Renseignez', '▁les', '▁champs', '▁ci', '-', 'dessous', ',', '▁nous', '▁vous', '▁recontacter', 'ons', '▁rapidement', '.', '▁Nom', '*', '▁Prénom', '▁Email', '▁professionnel', '*', '▁Téléphone', '*', '▁Société', '*', '▁Fonction', '*', '▁Date', '▁de', '▁lancement', '▁de', '▁votre', '▁projet', '*', '▁-', '▁Précis', 'er', '▁-', 'Entre', '▁1', '▁et', '▁3', '▁mois', 'Entre', '▁3', '▁et', '▁6', '▁mois', 'Entre', '▁6', '▁mois', '▁et', '▁1', '▁an', 'Plus', '▁d', '’', '1', '▁an', 'Pas', '▁de', '▁date', '▁définie', '▁Contactez', '-', 'nous', '▁Vous', '▁souhaitez', '▁en', '▁savoir', '▁plus', '▁sur', '▁notre', '▁solution', '▁OP', 'IN', 'ION', '▁T', 'RAC', 'KER', '®', '▁?', '▁Comp', 'lé', 'tez', '▁le', '▁formulaire', '▁ci', '-', 'dessous', ',', '▁un', '▁de', '▁nos', '▁experts', '▁prendra', '▁contact', '▁avec', '▁vous', '▁rapidement', '.']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Users\\Napster\\Desktop\\M2_ISI\\MLA\\CamemBERT\\MLA-CamemBERT\\data\\oscar.Arrow\"\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "train_dataset = oscar_data(dataset, tokenizer, return_tokens = True)\n",
    "\n",
    "# Exemple\n",
    "print(train_dataset[0]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CamemBERT Tokenizer\n",
    "\n",
    "The **CamemBERT** tokenizer works similarly to BERT's but includes specific adaptations for French and RoBERTa's architecture. It generates two key outputs: **IDs** and the **Attention Mask**.\n",
    "\n",
    "## 1. IDs\n",
    "### What are they?\n",
    "- Integer representations of each word or subword (token) in a sentence, based on the model's vocabulary.\n",
    "\n",
    "### Why?\n",
    "- Models cannot process raw text directly. IDs convert text into a numerical format the model can understand.\n",
    "\n",
    "### How?\n",
    "1. **Tokenization**:  \n",
    "   Example: `\"Bonjour, comment ça va?\"` → `[\"Bonjour\", \",\", \"comment\", \"ça\", \"va\", \"?\"]`\n",
    "2. **Conversion to IDs**:  \n",
    "   Example: `{\"Bonjour\": 101, \",\": 4, \"comment\": 202, \"ça\": 303, \"va\": 404, \"?\": 5}`\n",
    "3. **Adding special tokens**:  \n",
    "   `[CLS]` and `[SEP]` mark the beginning and end of the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Attention Mask\n",
    "### What is it?\n",
    "- A binary vector (1 or 0) that indicates which parts of the input the model should process.\n",
    "\n",
    "### Why?\n",
    "- CamemBERT uses fixed-length sequences. Shorter sentences are padded with **padding tokens** (0).  \n",
    "- The mask ensures the model ignores these padding tokens during computation.\n",
    "\n",
    "### Example:\n",
    "- **Input Sentence**: `\"Comment ça va?\"`  \n",
    "- **Max length** = 8 tokens  \n",
    "  - Tokens: `[\"[CLS]\", \"Comment\", \"ça\", \"va\", \"?\", \"[SEP]\", 0, 0]`  \n",
    "  - **IDs**: `[101, 202, 303, 404, 5, 102, 0, 0]`  \n",
    "  - **Attention Mask**: `[1, 1, 1, 1, 1, 1, 0, 0]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer\n",
    "\n",
    "# Charger le tokenizer CamemBERT\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Phrase d'exemple\n",
    "text = \"Bonjour, comment ça va ?\"\n",
    "text = dataset[0]['text']\n",
    "\n",
    "# Tokenisation\n",
    "tokens = tokenizer(\n",
    "    text,\n",
    "    padding=\"max_length\",       # Remplir jusqu'à la longueur max si nécessaire\n",
    "    truncation=True,            # Tronquer si la phrase dépasse la longueur max\n",
    "    #max_length=20,              # Longueur maximale (vous pouvez ajuster)\n",
    "    return_tensors=\"pt\",        # Retourner des tensors PyTorch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gardez l’œil sur toutes les images publiées sur le web en lien avec votre marque. Une analyse poussée des images diffusées par les internautes mais aussi des contenus visuels provenant de vos concurrents.\n",
      "Des applications multiples qui vont de la simple analyse des contenus publiés jusqu’à la surveillance des contrefaçons. ImageTracker scanne ainsi les visuels provenant des réseaux sociaux Pinterest, Twitter et Instagram.\n",
      "Visibility Index\t\n",
      "Social Networks Analytics\t\n",
      "OPINION TRACKER®, une solution\t\n",
      "Groupe 361 est une agence de communication globale.\n",
      "La vision 361 est qu'aujourd'hui, plus que jamais, les marques doivent évoluer dans des environnements complexes où les marchés sont devenus des conversations.\t\n",
      "Plan du site\n",
      "Mentions légales\n",
      "Contactez-nous\n",
      "Ligne directe : 01 76 21 68 30\n",
      "Etre contacté\n",
      "Vous souhaitez en savoir plus sur notre solution OPINION TRACKER® ? Complétez le formulaire ci-dessous, un de nos experts prendra contact avec vous rapidement.\n",
      "Nom* Prénom\n",
      "Société Fonction\n",
      "Email professionnel* Téléphone*\n",
      "Sujet* - Préciser -Demande d’infos généralesAvoir des précisions sur les fonctionnalitésEchanger sur un projet à venirProposition de consultationAutre :\n",
      "Recevoir notre plaquette\n",
      "Renseignez les champs ci-dessous pour recevoir notre plaquette.\n",
      "Nom* Prenom\n",
      "Email professionnel* Téléphone\n",
      "Société* Fonction\n",
      "Demander une démo\n",
      "Renseignez les champs ci-dessous, nous vous recontacterons rapidement.\n",
      "Nom* Prénom\n",
      "Email professionnel* Téléphone*\n",
      "Société* Fonction*\n",
      "Date de lancement de votre projet* - Préciser -Entre 1 et 3 moisEntre 3 et 6 moisEntre 6 mois et 1 anPlus d’1 anPas de date définie\n",
      "Contactez-nous\n",
      "Vous souhaitez en savoir plus sur notre solution OPINION TRACKER® ? Complétez le formulaire ci-dessous, un de nos experts prendra contact avec vous rapidement.\n",
      "Tokens : ['▁Gardez', '▁l', '’', 'œil', '▁sur', '▁toutes', '▁les', '▁images', '▁publiées', '▁sur', '▁le', '▁web', '▁en', '▁lien', '▁avec', '▁votre', '▁marque', '.', '▁Une', '▁analyse', '▁poussée', '▁des', '▁images', '▁diffusée', 's', '▁par', '▁les', '▁internautes', '▁mais', '▁aussi', '▁des', '▁contenus', '▁visuels', '▁provenant', '▁de', '▁vos', '▁concurrents', '.', '▁Des', '▁applications', '▁multiples', '▁qui', '▁vont', '▁de', '▁la', '▁simple', '▁analyse', '▁des', '▁contenus', '▁publiés', '▁jusqu', '’', 'à', '▁la', '▁surveillance', '▁des', '▁contrefaçon', 's', '.', '▁Image', 'Tra', 'cker', '▁scan', 'ne', '▁ainsi', '▁les', '▁visuels', '▁provenant', '▁des', '▁réseaux', '▁sociaux', '▁Pinterest', ',', '▁Twitter', '▁et', '▁Instagram', '.', '▁Vis', 'ibili', 'ty', '▁Index', '▁Social', '▁Network', 's', '▁Analytics', '▁OP', 'IN', 'ION', '▁T', 'RAC', 'KER', '®', ',', '▁une', '▁solution', '▁Groupe', '▁3', '61', '▁est', '▁une', '▁agence', '▁de', '▁communication', '▁globale', '.', '▁La', '▁vision', '▁3', '61', '▁est', '▁qu', \"'\", 'aujourd', \"'\", 'hui', ',', '▁plus', '▁que', '▁jamais', ',', '▁les', '▁marques', '▁doivent', '▁évoluer', '▁dans', '▁des', '▁environnement', 's', '▁complexes', '▁où', '▁les', '▁marchés', '▁sont', '▁devenus', '▁des', '▁conversations', '.', '▁Plan', '▁du', '▁site', '▁Mention', 's', '▁légales', '▁Contactez', '-', 'nous', '▁Ligne', '▁directe', '▁:', '▁01', '▁76', '▁21', '▁68', '▁30', '▁Etre', '▁contacté', '▁Vous', '▁souhaitez', '▁en', '▁savoir', '▁plus', '▁sur', '▁notre', '▁solution', '▁OP', 'IN', 'ION', '▁T', 'RAC', 'KER', '®', '▁?', '▁Comp', 'lé', 'tez', '▁le', '▁formulaire', '▁ci', '-', 'dessous', ',', '▁un', '▁de', '▁nos', '▁experts', '▁prendra', '▁contact', '▁avec', '▁vous', '▁rapidement', '.', '▁Nom', '*', '▁Prénom', '▁Société', '▁Fonction', '▁Email', '▁professionnel', '*', '▁Téléphone', '*', '▁Sujet', '*', '▁-', '▁Précis', 'er', '▁-', 'De', 'mand', 'e', '▁d', '’', 'infos', '▁générales', 'A', 'voir', '▁des', '▁précisions', '▁sur', '▁les', '▁fonctionnalités', 'E', 'change', 'r', '▁sur', '▁un', '▁projet', '▁à', '▁venir', 'Pro', 'position', '▁de', '▁consultation', 'Autre', '▁:', '▁Re', 'ce', 'voir', '▁notre', '▁plaquette', '▁Renseignez', '▁les', '▁champs', '▁ci', '-', 'dessous', '▁pour', '▁recevoir', '▁notre', '▁plaquette', '.', '▁Nom', '*', '▁Pre', 'nom', '▁Email', '▁professionnel', '*', '▁Téléphone', '▁Société', '*', '▁Fonction', '▁Demande', 'r', '▁une', '▁démo', '▁Renseignez', '▁les', '▁champs', '▁ci', '-', 'dessous', ',', '▁nous', '▁vous', '▁recontacter', 'ons', '▁rapidement', '.', '▁Nom', '*', '▁Prénom', '▁Email', '▁professionnel', '*', '▁Téléphone', '*', '▁Société', '*', '▁Fonction', '*', '▁Date', '▁de', '▁lancement', '▁de', '▁votre', '▁projet', '*', '▁-', '▁Précis', 'er', '▁-', 'Entre', '▁1', '▁et', '▁3', '▁mois', 'Entre', '▁3', '▁et', '▁6', '▁mois', 'Entre', '▁6', '▁mois', '▁et', '▁1', '▁an', 'Plus', '▁d', '’', '1', '▁an', 'Pas', '▁de', '▁date', '▁définie', '▁Contactez', '-', 'nous', '▁Vous', '▁souhaitez', '▁en', '▁savoir', '▁plus', '▁sur', '▁notre', '▁solution', '▁OP', 'IN', 'ION', '▁T', 'RAC', 'KER', '®', '▁?', '▁Comp', 'lé', 'tez', '▁le', '▁formulaire', '▁ci', '-', 'dessous', ',', '▁un', '▁de', '▁nos', '▁experts', '▁prendra', '▁contact', '▁avec', '▁vous', '▁rapidement', '.']\n",
      "Input IDs : tensor([[    5, 23213,    17,    12,  4255,    32,   208,    19,  1004, 12358,\n",
      "            32,    16,   939,    22,   818,    42,    75,   587,     9,   180,\n",
      "          2646, 10929,    20,  1004,  9143,    10,    37,    19,  5548,    65,\n",
      "            99,    20,  4019, 11453,  6030,     8,   140,  8416,     9,   363,\n",
      "          2592,  2523,    31,   774,     8,    13,   445,  2646,    20,  4019,\n",
      "          9241,   257,    12,   169,    13,  3848,    20, 21611,    10,     9,\n",
      "         10265, 15321,  7740, 19771,   324,   163,    19, 11453,  6030,    20,\n",
      "          1517,  1148, 14413,     7,  4651,    14,  8712,     9, 13836, 19945,\n",
      "          1933,  7996, 12553, 20071,    10, 26559, 15378,  2376, 11462,   309,\n",
      "         20270, 26904,     3,     7,    28,   932,  2772,   135,  5648,    30,\n",
      "            28,  2985,     8,  1006,  4141,     9,    61,  1588,   135,  5648,\n",
      "            30,    46,    11,  3462,    11,   265,     7,    40,    27,   283,\n",
      "             7,    19,  1977,   750,  5120,    29,    20,  1898,    10,  5577,\n",
      "           147,    19,  3020,    56,  8451,    20, 14232,     9,  1664,    25,\n",
      "           132, 14181,    10,  8070,  7610,    26,  1474,  8329,  3886,    43,\n",
      "          3455,  7976,   727,  6630,   417, 13945, 10919,   158,  1340,    22,\n",
      "           319,    40,    32,   127,   932, 15378,  2376, 11462,   309, 20270,\n",
      "         26904,     3,   106,  4231,  1357,  4731,    16,  2738,   642,    26,\n",
      "          1568,     7,    23,     8,   166,  3510,  7020,  1002,    42,    39,\n",
      "           736,     9,  5357,  1363, 26080,  2157,  8219, 25790,  1166,  1363,\n",
      "         13049,  1363,  6232,  1363,    67, 21313,   108,    67,  4217,  4428,\n",
      "            35,    18,    12, 12149,  5335,   243,  1419,    20, 14277,    32,\n",
      "            19,  4241,   399,  8910,    81,    32,    23,   327,    15,   894,\n",
      "          5114,  6937,     8,  4032, 16024,    43,   568,   291,  1419,   127,\n",
      "         25670, 30572,    19,  4044,   642,    26,  1568,    24,  1653,   127,\n",
      "         25670,     9,  5357,  1363,  8666,  4514, 25790,  1166,  1363, 13049,\n",
      "          2157,  1363,  8219, 12057,    81,    28,  9346, 30572,    19,  4044,\n",
      "           642,    26,  1568,     7,    63,    39, 31148,   273,   736,     9,\n",
      "          5357,  1363, 26080, 25790,  1166,  1363, 13049,  1363,  2157,  1363,\n",
      "          8219,  1363,  5661,     8,  3864,     8,    75,   327,  1363,    67,\n",
      "         21313,   108,    67, 23793,   124,    14,   135,   250, 23793,   135,\n",
      "            14,   260,   250, 23793,   260,   250,    14,   124,   674,  5076,\n",
      "            18,    12,   367,   674,  9382,     8,   749, 12022,  7610,    26,\n",
      "          1474,   158,  1340,    22,   319,    40,    32,   127,   932, 15378,\n",
      "          2376, 11462,   309, 20270, 26904,     3,   106,  4231,  1357,  4731,\n",
      "            16,  2738,   642,    26,  1568,     7,    23,     8,   166,  3510,\n",
      "          7020,  1002,    42,    39,   736,     9,     6,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "Attention Mask : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Tokens :\", tokenizer.tokenize(text))\n",
    "print(\"Input IDs :\", tokens[\"input_ids\"])\n",
    "print(\"Attention Mask :\", tokens[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple d'utilisation du dataloader :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "tensor([    5, 23213,    17,    12,  4255,    32,   208,    19,  1004, 12358,\n",
      "           32,    16,   939,    22,   818,    42,    75,   587,     9,   180,\n",
      "         2646, 10929,    20,  1004,  9143,    10,    37,    19,  5548,    65,\n",
      "           99,    20,  4019, 11453,  6030,     8,   140,  8416,     9,   363,\n",
      "         2592,  2523,    31,   774,     8,    13,   445,  2646,    20,  4019,\n",
      "         9241,   257,    12,   169,    13,  3848,    20, 21611,    10,     9,\n",
      "        10265, 15321,  7740, 19771,   324,   163,    19, 11453,  6030,    20,\n",
      "         1517,  1148, 14413,     7,  4651,    14,  8712,     9, 13836, 19945,\n",
      "         1933,  7996, 12553, 20071,    10, 26559, 15378,  2376, 11462,   309,\n",
      "        20270, 26904,     3,     7,    28,   932,  2772,   135,  5648,    30,\n",
      "           28,  2985,     8,  1006,  4141,     9,    61,  1588,   135,  5648,\n",
      "           30,    46,    11,  3462,    11,   265,     7,    40,    27,   283,\n",
      "            7,    19,  1977,   750,  5120,    29,    20,  1898,    10,  5577,\n",
      "          147,    19,  3020,    56,  8451,    20, 14232,     9,  1664,    25,\n",
      "          132, 14181,    10,  8070,  7610,    26,  1474,  8329,  3886,    43,\n",
      "         3455,  7976,   727,  6630,   417, 13945, 10919,   158,  1340,    22,\n",
      "          319,    40,    32,   127,   932, 15378,  2376, 11462,   309, 20270,\n",
      "        26904,     3,   106,  4231,  1357,  4731,    16,  2738,   642,    26,\n",
      "         1568,     7,    23,     8,   166,  3510,  7020,  1002,    42,    39,\n",
      "          736,     9,  5357,  1363, 26080,  2157,  8219, 25790,  1166,  1363,\n",
      "        13049,  1363,  6232,  1363,    67, 21313,   108,    67,  4217,  4428,\n",
      "           35,    18,    12, 12149,  5335,   243,  1419,    20, 14277,    32,\n",
      "           19,  4241,   399,  8910,    81,    32,    23,   327,    15,   894,\n",
      "         5114,  6937,     8,  4032, 16024,    43,   568,   291,  1419,   127,\n",
      "        25670, 30572,    19,  4044,   642,    26,  1568,    24,  1653,   127,\n",
      "        25670,     9,  5357,  1363,  8666,  4514, 25790,  1166,  1363, 13049,\n",
      "         2157,  1363,  8219, 12057,    81,    28,  9346, 30572,    19,  4044,\n",
      "          642,    26,  1568,     7,    63,    39, 31148,   273,   736,     9,\n",
      "         5357,  1363, 26080, 25790,  1166,  1363, 13049,  1363,  2157,  1363,\n",
      "         8219,  1363,  5661,     8,  3864,     8,    75,   327,  1363,    67,\n",
      "        21313,   108,    67, 23793,   124,    14,   135,   250, 23793,   135,\n",
      "           14,   260,   250, 23793,   260,   250,    14,   124,   674,  5076,\n",
      "           18,    12,   367,   674,  9382,     8,   749, 12022,  7610,    26,\n",
      "         1474,   158,  1340,    22,   319,    40,    32,   127,   932, 15378,\n",
      "         2376, 11462,   309, 20270, 26904,     3,   106,  4231,  1357,  4731,\n",
      "           16,  2738,   642,    26,  1568,     7,    23,     8,   166,  3510,\n",
      "         7020,  1002,    42,    39,   736,     9,     6,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "from dataset import OscarDataset\n",
    "\n",
    "# Charger le dataset Hugging Face sauvegardé\n",
    "data_path = r'C:\\Users\\Napster\\Desktop\\M2_ISI\\MLA\\CamemBERT\\MLA-CamemBERT\\data\\oscar.Arrow'\n",
    "hf_dataset = load_from_disk(data_path)\n",
    "\n",
    "# Initialiser le tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Créer le Dataset PyTorch\n",
    "dataset = OscarDataset(hf_dataset, tokenizer, max_length=512)\n",
    "\n",
    "# Créer le DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "# Boucle pour vérifier les données\n",
    "for batch in dataloader:\n",
    "    print(batch[\"input_ids\"].shape)  # Shape : (batch_size, max_length)\n",
    "    print(batch[\"attention_mask\"].shape)  # Shape : (batch_size, max_length)\n",
    "    print(batch['input_ids'][0])\n",
    "    print(batch['attention_mask'][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving Import Issues in Jupyter Notebook for CamemBERT Project\n",
    "\n",
    "## Problem\n",
    "When working on the CamemBERT project, we faced challenges importing:\n",
    "1. **Custom classes** (e.g., `OscarDataset`) from the `src` directory.\n",
    "2. **Data files** (e.g., `dataset.arrow`) stored in the `data` directory.\n",
    "\n",
    "The issues arose because:\n",
    "- Jupyter notebooks don't automatically recognize the project's folder structure.\n",
    "- The default `sys.path` doesn't include the `src` directory or other custom paths.\n",
    "\n",
    "## Solution\n",
    "We solved the problem by:\n",
    "1. **Adding the `src` directory to `sys.path`** for importing Python modules.\n",
    "2. **Using relative paths** to access data files in the `data` directory.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Adding `src` to `sys.path`\n",
    "Since the notebook was located in `src/notebooks`, we added the `src` directory to Python's `sys.path` to allow imports like `from dataset import OscarDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CamemBERT/\n",
    "# ├── data/\n",
    "# │   └── dataset.arrow  # The dataset file\n",
    "# ├── src/\n",
    "# │   ├── dataset.py      # Contains the OscarDataset class\n",
    "# │   ├── train.py        # Training script\n",
    "# │   ├── notebooks/\n",
    "# │   │   └── your_notebook.ipynb  # Jupyter notebook for experiments\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

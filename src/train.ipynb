{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertConfig, CamembertForMaskedLM, AdamW, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling, CamembertTokenizer\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "files = [\"path_to_your_oscar_french_text_file.txt\"]\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(\n",
    "    files=files,\n",
    "    vocab_size=32000,  \n",
    "    min_frequency=2,\n",
    "    special_tokens=[\n",
    "        \"<s>\",  \n",
    "        \"<pad>\", \n",
    "        \"</s>\",  \n",
    "        \"<unk>\",  \n",
    "        \"<mask>\"  \n",
    "    ]\n",
    ")\n",
    "\n",
    "tokenizer.save_model(\"./oscar_fr_vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import CamembertForMaskedLM, CamembertTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_from_disk\n",
    "from dataset import OscarDataset\n",
    "\n",
    "# === Initialize Tokenizer and Dataset === #\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/mini_oscar\"  # Replace with your dataset path\n",
    "hf_dataset = load_from_disk(dataset_path)\n",
    "oscar_dataset = OscarDataset(hf_dataset, tokenizer)\n",
    "# === Convert Dataset into Hugging Face Format === #\n",
    "# Hugging Face Trainer expects datasets in dictionary format with labels included\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([item[\"masked_input_ids\"] for item in batch]),\n",
    "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
    "        \"labels\": torch.stack([item[\"labels\"] for item in batch]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salhi\\.conda\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CamembertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Model initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "CamembertForMaskedLM                                         --\n",
       "├─CamembertModel: 1-1                                        --\n",
       "│    └─CamembertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                                   23,440,896\n",
       "│    │    └─Embedding: 3-2                                   393,216\n",
       "│    │    └─Embedding: 3-3                                   1,536\n",
       "│    │    └─LayerNorm: 3-4                                   1,536\n",
       "│    │    └─Dropout: 3-5                                     --\n",
       "│    └─CamembertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                                  85,054,464\n",
       "├─CamembertLMHead: 1-2                                       --\n",
       "│    └─Linear: 2-3                                           590,592\n",
       "│    └─LayerNorm: 2-4                                        1,536\n",
       "│    └─Linear: 2-5                                           23,471,418\n",
       "=====================================================================================\n",
       "Total params: 132,955,194\n",
       "Trainable params: 132,955,194\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # model summary\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CamembertForMaskedLM, CamembertTokenizer, TrainingArguments, Trainer , CamembertConfig\n",
    "from datasets import load_from_disk\n",
    "from dataset import OscarDataset\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "# === Model === #\n",
    "# config = CamembertConfig(\n",
    "#     vocab_size=tokenizer.vocab_size,  # Adjust to match your tokenizer's vocab size\n",
    "#     hidden_size=768,                 # Hidden size (RoBERTa_BASE)\n",
    "#     num_hidden_layers=12,            # Number of transformer layers\n",
    "#     num_attention_heads=12,          # Number of attention heads\n",
    "#     intermediate_size=3072,          # FFN inner hidden size\n",
    "#     hidden_dropout_prob=0.1,         # Dropout probability\n",
    "#     attention_probs_dropout_prob=0.1, # Attention dropout probability\n",
    "#     max_position_embeddings=514,     # Maximum sequence length + special tokens\n",
    "#     type_vocab_size=1,               # No token type embeddings\n",
    "#     initializer_range=0.02           # Standard deviation for weight initialization\n",
    "# )\n",
    "\n",
    "config = CamembertConfig()\n",
    "print(config)\n",
    "# Initialize a randomly weighted CamembertForMaskedLM model\n",
    "model = CamembertForMaskedLM(config) \n",
    "# model.to(\"cuda\")\n",
    "\n",
    "print(\"Model initialized\")\n",
    "\n",
    "summary(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training Arguments === #\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./camembert_mlm\",  # Directory to save the model and checkpoints\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # 3 epochs as specified\n",
    "    per_device_train_batch_size=8,  # Batch size\n",
    "    gradient_accumulation_steps=16,  # Effective batch size = 8 * 16\n",
    "    learning_rate=6e-4,  # Peak learning rate for base\n",
    "    weight_decay=0.01,  # Weight decay\n",
    "    max_steps=500000,  # Train for 500k steps\n",
    "    warmup_steps=24000,  # 24k warmup steps for base\n",
    "    save_steps=5000,  # Save model every 5000 steps\n",
    "    logging_steps=500,  # Log training loss every 500 steps\n",
    "    save_total_limit=2,  # Keep only the 2 most recent checkpoints\n",
    "    lr_scheduler_type=\"linear\",  # Linear learning rate decay\n",
    "    evaluation_strategy=\"no\",  # No validation dataset (can be added if needed)\n",
    "    fp16=True,  # Mixed precision training for faster performance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Trainer === #\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=oscar_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# === Train === #\n",
    "trainer.train()\n",
    "\n",
    "# === Save the Final Model === #\n",
    "trainer.save_model(\"./camembert_mlm\")\n",
    "tokenizer.save_pretrained(\"./camembert_mlm\")\n",
    "print(\"Training complete. Model saved to ./camembert_mlm.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
